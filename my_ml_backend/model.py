from typing import List, Dict, Optional
import json
import os
import time
import datetime
from pathlib import Path
from openai import OpenAI
from label_studio_ml.model import LabelStudioMLBase
from label_studio_ml.response import ModelResponse

# å¯¼å…¥å¤„ç†é…ç½®
try:
    from processing_config import get_processing_config
    processing_config = get_processing_config()
except ImportError:
    print("âš ï¸ å¤„ç†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    processing_config = None


# ==================== å‘½åå®ä½“é…ç½® ====================
# ä»é…ç½®æ–‡ä»¶å¯¼å…¥å®ä½“é…ç½®
try:
    from entity_config import get_entity_config, get_entity_labels, get_all_categories, get_entities_by_category
    NER_ENTITY_CONFIG = get_entity_config()
    ENTITY_LABELS = get_entity_labels()
    print(f"âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½äº† {len(ENTITY_LABELS)} ç§å®ä½“ç±»å‹")
    print(f"ğŸ“‹ åŒ…å«ç±»åˆ«: {', '.join(get_all_categories())}")
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    print("âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    NER_ENTITY_CONFIG = {
        "PER": {"description": "äººå", "examples": ["å¼ ä¸‰", "æå››"], "invalid_patterns": [r'å‘ç”Ÿ', r'èµ·ç«']},
        "LOC": {"description": "åœ°å", "examples": ["åŒ—äº¬", "ä¸Šæµ·"], "invalid_patterns": []},
        "ORG": {"description": "ç»„ç»‡", "examples": ["å…¬å¸", "å­¦æ ¡"], "invalid_patterns": []},
        "TIME": {"description": "æ—¶é—´", "examples": ["ä»Šå¤©", "æ˜å¤©"], "valid_patterns": [r'\d+å¹´', r'\d+æœˆ']},
        "EVENT": {"description": "äº‹ä»¶", "examples": ["ä¼šè®®", "æ´»åŠ¨", "ç«ç¾","èµ·ç«","æ‰‘ç­"], "invalid_patterns": []},
        "QUANTITY": {"description": "æ•°é‡", "examples": ["100ä¸ª", "50ä¸‡"], "valid_patterns": [r'\d+']},
    }
    ENTITY_LABELS = list(NER_ENTITY_CONFIG.keys())

# ç”Ÿæˆå®ä½“ç±»å‹è¯´æ˜æ–‡æœ¬
def get_entity_types_description():
    """ç”Ÿæˆå®ä½“ç±»å‹çš„è¯´æ˜æ–‡æœ¬"""
    descriptions = []
    for label, config in NER_ENTITY_CONFIG.items():
        descriptions.append(f"{label}({config['description']})")
    return "ã€".join(descriptions)

# ç”ŸæˆJSONæ ¼å¼ç¤ºä¾‹
def get_json_format_example():
    """ç”ŸæˆJSONæ ¼å¼ç¤ºä¾‹"""
    return """{{
  "entities": [
    {{
      "text": "å®ä½“æ–‡æœ¬",
      "start": èµ·å§‹ä½ç½®,
      "end": ç»“æŸä½ç½®,
      "label": "å®ä½“ç±»å‹"
    }}
  ]
}}"""

# æ ‡ç­¾éªŒè¯å’Œæ˜ å°„å‡½æ•°
def validate_and_map_label(original_label: str) -> str:
    """éªŒè¯å’Œæ˜ å°„æ ‡ç­¾åç§°ï¼Œç¡®ä¿ä¸é…ç½®æ–‡ä»¶ä¸€è‡´ï¼Œè¿”å›é…ç½®æ–‡ä»¶ä¸­çš„é”®å"""
    if not original_label:
        return None
    
    # æ¸…ç†æ ‡ç­¾ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ï¼‰
    clean_label = original_label.strip()
    
    # 1. ç›´æ¥åŒ¹é…é”®å
    if clean_label in ENTITY_LABELS:
        return clean_label
    
    # 2. åŒ¹é…descriptionï¼ˆAIå¯èƒ½è¿”å›descriptionæ ¼å¼çš„æ ‡ç­¾ï¼‰
    for label_key, config in NER_ENTITY_CONFIG.items():
        if config['description'] == clean_label:
            print(f"   ğŸ”„ æè¿°åŒ¹é…: '{clean_label}' -> '{label_key}'")
            return label_key
    
    # 3. å¸¸è§çš„æ ‡ç­¾æ˜ å°„ï¼ˆå¤„ç†AIå¯èƒ½è¿”å›çš„å˜ä½“ï¼‰
    # åŸºäºentity_config.pyä¸­çš„å®é™…æ ‡ç­¾åç§°è¿›è¡Œæ˜ å°„
    label_mapping = {
        # æ–‡æ¡£ç±»æ˜ å°„
        "æ–‡æ¡£ç¼–å·": "æ–‡å·",
        "æ–‡æ¡£å·": "æ–‡å·", 
        "ç¼–å·": "æ–‡å·",
        "ç‰ˆæœ¬": "ç‰ˆæœ¬å·",
        "ä¿®è®¢": "ç‰ˆæœ¬å·",
        "ä¿®æ­£": "ç‰ˆæœ¬å·",
        "é™„å½•": "é™„ä»¶",
        
        # æœºæ„ç»„ç»‡æ˜ å°„
        "ç»„ç»‡": "æœºæ„ç»„ç»‡",
        "æœºæ„": "æœºæ„ç»„ç»‡",
        "ç¾¤ä½“": "äººå‘˜",
        "äººå": "äººå‘˜",
        "èŒä½": "èŒä½èŒèƒ½",
        "èŒèƒ½": "èŒä½èŒèƒ½",
        
        # åœ°ç†ä½ç½®æ˜ å°„
        "åœ°ç‚¹": "åœ°ç†ä½ç½®",
        "ä½ç½®": "åœ°ç†ä½ç½®",
        "åœ°å": "åœ°ç†ä½ç½®",
        "åŒºåŸŸ": "åŒºåŸŸä¿¡æ¯",
        "è¡Œæ”¿åŒº": "è¡Œæ”¿åŒºåˆ’",
        "æ²³æ®µ": "æµåŸŸ",
        
        # æ—¶é—´æ˜ å°„
        "æ—¥æœŸ": "æ—¶é—´",
        "æ—¶æ®µ": "æ—¶é—´",
        "å‘å¸ƒæ—¶é—´": "å‘å¸ƒæ—¥æœŸ",
        "ç”Ÿæ•ˆæ—¶é—´": "ç”Ÿæ•ˆæ—¥æœŸ",
        
        # ç¾å®³æ˜ å°„
        "ç¾å®³": "ç¾å®³ç±»å‹",
        "äº‹ä»¶": "ç¾å®³äº‹ä»¶",
        "äº‹æ•…": "ç¾å®³äº‹ä»¶",
        "åæœ": "äº‹æ•…åæœ",
        "å†³å£": "æºƒå",
        
        # æ•°å€¼æ˜ å°„
        "æ•°å€¼": "æ•°å€¼æŒ‡æ ‡",
        "æŒ‡æ ‡": "æ•°å€¼æŒ‡æ ‡",
        "è­¦æˆ’çº¿": "é˜ˆå€¼",
        "ç­‰çº§": "é¢„è­¦çº§åˆ«",
        "çº§åˆ«": "é¢„è­¦çº§åˆ«",
        
        # ç›‘æµ‹é¢„è­¦æ˜ å°„
        "ç›‘æµ‹ç«™": "ç›‘æµ‹ç«™ç‚¹",
        "ç›‘æµ‹è®¾å¤‡": "ç›‘æµ‹ç«™ç‚¹",
        "é¢„è­¦": "é¢„è­¦ä¿¡æ¯",
        "é¢„æŠ¥": "é¢„è­¦ä¿¡æ¯",
        "æ¨¡å‹": "é¢„æµ‹æ¨¡å‹",
        "æ–¹æ³•": "é¢„æµ‹æ¨¡å‹",
        
        # åº”æ€¥å“åº”æ˜ å°„
        "å¯åŠ¨æ¡ä»¶": "è§¦å‘æ¡ä»¶",
        "åº”æ€¥ç­‰çº§": "å“åº”çº§åˆ«",
        "æŒ‡æŒ¥éƒ¨": "æŒ‡æŒ¥ä½“ç³»",
        "åº”å¯¹æªæ–½": "å¤„ç½®æªæ–½",
        "å‘½ä»¤": "å†³ç­–",
        
        # æ•‘æ´æ˜ å°„
        "é¿éš¾ç‚¹": "ç–æ•£è·¯çº¿",
        "å®‰ç½®": "æ•‘åŠ©æªæ–½",
        "æ•‘æ´é˜Ÿä¼": "æ•‘æ´åŠ›é‡",
        "è£…å¤‡": "ç‰©èµ„è£…å¤‡",
        "è¿è¾“": "ç‰©æµè¿è¾“",
        
        # åŸºç¡€è®¾æ–½æ˜ å°„
        "è®¾æ–½": "åŸºç¡€è®¾æ–½",
        "æŸå": "è®¾æ–½çŠ¶æ€",
        "åŠ å›º": "ç»´ä¿®åŠ å›º",
        
        # è´¢æ”¿æ˜ å°„
        "èµ„é‡‘": "èµ„é‡‘ä¿éšœ",
        "è´¢æ”¿": "èµ„é‡‘ä¿éšœ",
        "ä¿é™©": "ä¿é™©èµ”å¿",
        "èµ”å¿": "ä¿é™©èµ”å¿",
        "é‡‡è´­": "é‡‡è´­æ‹›æ ‡",
        "æ‹›æ ‡": "é‡‡è´­æ‹›æ ‡",
        "åˆåŒ": "é‡‡è´­æ‹›æ ‡",
        
        # è¯æ®æ˜ å°„
        "è®°å½•": "ç›‘æµ‹è®°å½•",
        "æŠ¥è¡¨": "ç›‘æµ‹è®°å½•",
        "ç…§ç‰‡": "è¯æ®ææ–™",
        "è§†é¢‘": "è¯æ®ææ–™",
        "è¯æ®": "è¯æ®ææ–™",
        "è¯äºº": "è¯äººè¯è¯",
        "è¯è¯": "è¯äººè¯è¯",
        
        # ç›‘ç®¡æ˜ å°„
        "æ£€æŸ¥": "æ£€æŸ¥éªŒæ”¶",
        "éªŒæ”¶": "æ£€æŸ¥éªŒæ”¶",
        "å¹´æ£€": "æ£€æŸ¥éªŒæ”¶",
        "éšæ‚£": "éšæ‚£æ¸…å•",
        "é—®é¢˜": "éšæ‚£æ¸…å•",
        "æ‰§æ³•": "ç›‘ç®¡æªæ–½",
        
        # åŸ¹è®­æ˜ å°„
        "æ¼”ç»ƒ": "æ¼”ç»ƒåŸ¹è®­",
        "åŸ¹è®­": "æ¼”ç»ƒåŸ¹è®­",
        "èƒ½åŠ›": "èƒ½åŠ›æ¸…å•",
        "èµ„æº": "èƒ½åŠ›æ¸…å•",
        "é¢„æ¡ˆ": "é¢„æ¡ˆæ¡ç›®",
        "ç« èŠ‚": "é¢„æ¡ˆæ¡ç›®",
        
        # ç¾åæ˜ å°„
        "æ¢å¤": "æ¢å¤é‡å»º",
        "é‡å»º": "æ¢å¤é‡å»º",
        "å–„å": "å–„åä¿éšœ",
        "å¿ƒç†": "å–„åä¿éšœ",
        "æ€»ç»“": "æ€»ç»“å»ºè®®",
        "å»ºè®®": "æ€»ç»“å»ºè®®",
        "æ•™è®­": "æ€»ç»“å»ºè®®",
        
        # é£é™©æ²»ç†æ˜ å°„
        "é£é™©": "é£é™©è¯„ä¼°",
        "è¯„ä¼°": "é£é™©è¯„ä¼°",
        "åŒºåˆ’": "é£é™©è¯„ä¼°",
        "æ ‡å‡†": "è®¾è®¡æ ‡å‡†",
        "è§„èŒƒ": "è®¾è®¡æ ‡å‡†",
        "æ²»ç†": "é•¿æœŸæ²»ç†",
        "é€‚åº”": "é•¿æœŸæ²»ç†",
        
        # ä¿¡æ¯ä¼ æ’­æ˜ å°„
        "è”ç³»äºº": "è”ç³»äººä¿¡æ¯",
        "æ¸ é“": "å‘å¸ƒæ¸ é“",
        "åª’ä½“": "åª’ä½“èˆ†æƒ…",
        "èˆ†æƒ…": "åª’ä½“èˆ†æƒ…",
        "æŠ¥é“": "åª’ä½“èˆ†æƒ…",
        
        # å…¶ä»–æ˜ å°„
        "ååŒ": "è·¨ç•ŒååŒ",
        "æµåŸŸ": "è·¨ç•ŒååŒ",
        "æ”¿ç­–": "æ”¿ç­–å˜æ›´",
        "å˜æ›´": "æ”¿ç­–å˜æ›´",
        "å†å²": "æ”¿ç­–å˜æ›´",
        "ç¾¤ä½“": "è„†å¼±ç¾¤ä½“",
        "èµ„äº§": "å…³é”®èµ„äº§",
        "ç»æµ": "å…³é”®èµ„äº§",
        "ç®—æ³•": "æ¨¡å‹ç®—æ³•",
        "æ•°æ®": "æ•°æ®æ¥æº",
        "å¼•ç”¨": "æ•°æ®æ¥æº",
        
        # å…³ç³»æ˜ å°„
        "ä½äº": "ä½äºå…³ç³»",
        "ä¸»ç®¡": "è´£ä»»å…³ç³»",
        "è´£ä»»": "è´£ä»»å…³ç³»",
        "è§¦å‘": "å› æœå…³ç³»",
        "å¯¼è‡´": "å› æœå…³ç³»",
        "å¼•ç”¨": "ä¾æ®å…³ç³»",
        "ä¾æ®": "ä¾æ®å…³ç³»",
        "åŒ…å«": "åŒ…å«å…³ç³»",
        "å±äº": "åŒ…å«å…³ç³»",
        "å½±å“": "å½±å“å…³ç³»",
        "å—å½±å“": "å½±å“å…³ç³»",
        "éš¶å±": "éš¶å±å…³ç³»",
        "ä¸Šä¸‹çº§": "éš¶å±å…³ç³»",
        "å‘èµ·": "å‘èµ·å…³ç³»",
        "ä¸‹è¾¾": "å‘èµ·å…³ç³»",
        "è°ƒé…": "è°ƒé…å…³ç³»",
        "æ”¯æ´": "è°ƒé…å…³ç³»",
        "æ£€æµ‹": "æ£€æµ‹å…³ç³»",
        "è§‚æµ‹": "æ£€æµ‹å…³ç³»",
        "å¿ä»˜": "è¡¥å¿å…³ç³»",
        "è¡¥å¿": "è¡¥å¿å…³ç³»",
        "æ•´æ”¹": "æ•´æ”¹å…³ç³»",
        "å¤„ç†": "æ•´æ”¹å…³ç³»"
    }
    
    # æ£€æŸ¥æ˜ å°„è¡¨
    if clean_label in label_mapping:
        mapped_label = label_mapping[clean_label]
        print(f"   ğŸ”„ æ ‡ç­¾æ˜ å°„: '{clean_label}' -> '{mapped_label}'")
        return mapped_label
    
    # 4. æ¨¡ç³ŠåŒ¹é…ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
    for valid_label in ENTITY_LABELS:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
        if clean_label in valid_label or valid_label in clean_label:
            print(f"   ğŸ” æ¨¡ç³ŠåŒ¹é…: '{clean_label}' -> '{valid_label}'")
            return valid_label
    
    # 5. å¦‚æœéƒ½æ— æ³•åŒ¹é…ï¼Œè¿”å›None
    print(f"   âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ ‡ç­¾: '{clean_label}'")
    return None

def get_valid_label_list():
    """è·å–æ‰€æœ‰æœ‰æ•ˆçš„æ ‡ç­¾åˆ—è¡¨ç”¨äºæç¤º"""
    try:
        from entity_config import get_entity_labels
        return get_entity_labels()
    except ImportError:
        return list(NER_ENTITY_CONFIG.keys())


class NewModel(LabelStudioMLBase):
    """Custom ML Backend model
    """
    
    def setup(self):
        """Configure any parameters of your model here
        """
        self.set("model_version", "0.0.1")
        
        # é­”å¡”ç¤¾åŒºAPIé…ç½®
        self.api_key = os.getenv('MODELSCOPE_API_KEY', 'ms-2c045fb7-f463-45bf-b0f9-a36d50b0400e')
        self.api_base_url = os.getenv('MODELSCOPE_API_URL', 'https://api-inference.modelscope.cn/v1')
        # æ¨èçš„æ¨¡å‹é€‰æ‹©ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:
        # 1. Qwen/Qwen3-235B-A22B-Instruct-2507 - æœ€é€‚åˆç»“æ„åŒ–è¾“å‡º
        # 2. Qwen/Qwen3-Coder-480B-A35B-Instruct - ä»£ç å’Œç»“æ„åŒ–æ•°æ®å¤„ç†
        # 3. Qwen/Qwen3-235B-A22B-Thinking-2507 - æ€ç»´é“¾æ¨¡å‹ï¼ˆè¾“å‡ºæ ¼å¼å¤æ‚ï¼‰
        self.model_name = "Qwen/Qwen3-235B-A22B-Instruct-2507"  # æ›´é€‚åˆNERä»»åŠ¡
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if self.api_key:
            try:
                self.client = OpenAI(
                    base_url=self.api_base_url,
                    api_key=self.api_key
                )
                print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.model_name}")
            except Exception as e:
                print(f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
        else:
            print(f"âš ï¸ è¯·è®¾ç½®MODELSCOPE_API_KEYç¯å¢ƒå˜é‡")
            self.client = None
        
        # æ£€æŸ¥APIè¿æ¥
        self._check_api_connection()
        
        # æ˜¾ç¤ºå½“å‰é…ç½®çš„å®ä½“æ ‡ç­¾
        self._show_entity_config()
        
    def _check_api_connection(self):
        """æ£€æŸ¥é­”å¡”ç¤¾åŒºAPIè¿æ¥"""
        if not self.client:
            print(f"âŒ å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1000,
                temperature=0.1
            )
            print(f"âœ… APIè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ APIè¿æ¥å¤±è´¥: {str(e)[:100]}")
    
    def _show_entity_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®çš„å®ä½“æ ‡ç­¾"""
        print(f"\nğŸ“‹ å½“å‰æ”¯æŒçš„å‘½åå®ä½“ç±»å‹:")
        print("="*60)
        
        try:
            from entity_config import get_all_categories, get_entities_by_category
            categories = get_all_categories()
            
            for category in categories:
                entities = get_entities_by_category(category)
                if entities:
                    print(f"\nğŸ“‚ {category}ç±» ({len(entities)}ä¸ªå®ä½“):")
                    for i, (label, config) in enumerate(entities.items(), 1):
                        print(f"  {i:2d}. {label} - {config['description']}")
                        if config['examples']:
                            examples = "ã€".join(config['examples'][:2])
                            print(f"      ç¤ºä¾‹: {examples}")
                            
        except ImportError:
            # å¤‡ç”¨æ˜¾ç¤ºæ–¹å¼
            for i, (label, config) in enumerate(NER_ENTITY_CONFIG.items(), 1):
                print(f"  {i}. {label} - {config['description']}")
                if config['examples']:
                    examples = "ã€".join(config['examples'][:3])
                    print(f"     ç¤ºä¾‹: {examples}")
        
        print(f"\nğŸ’¡ æ€»è®¡: {len(ENTITY_LABELS)} ç§å®ä½“ç±»å‹")
        print(f"ğŸ”§ å¦‚éœ€ä¿®æ”¹å®ä½“ç±»å‹ï¼Œè¯·ç¼–è¾‘entity_config.pyæ–‡ä»¶")
        print("="*60)
    
    def set_task_completed_callback(self, callback_func):
        """è®¾ç½®ä»»åŠ¡å®Œæˆå›è°ƒå‡½æ•°
        
        Args:
            callback_func: å›è°ƒå‡½æ•°ï¼Œæ¥å—å‚æ•° (current_task_index, total_tasks, prediction_result)
        """
        self._task_completed_callback = callback_func
        print("âœ… å·²è®¾ç½®ä»»åŠ¡å®Œæˆå›è°ƒå‡½æ•°")
    
    def clear_task_completed_callback(self):
        """æ¸…é™¤ä»»åŠ¡å®Œæˆå›è°ƒå‡½æ•°"""
        if hasattr(self, '_task_completed_callback'):
            delattr(self, '_task_completed_callback')
            print("âœ… å·²æ¸…é™¤ä»»åŠ¡å®Œæˆå›è°ƒå‡½æ•°")


    def predict(self, tasks: List[Dict], context: Optional[Dict] = None, **kwargs) -> ModelResponse:
        """ å‘½åå®ä½“è¯†åˆ«é¢„æµ‹ï¼ˆæ”¯æŒæ‰¹é‡å¯¼å‡ºæ¨¡å¼ï¼‰
            :param tasks: Label Studio tasks in JSON format
            :param context: Label Studio context in JSON format
            :return: ModelResponse with predictions
        """
        # ä½¿ç”¨é…ç½®åŒ–çš„å‚æ•°
        if processing_config:
            MAX_BATCH_SIZE = processing_config.MAX_BATCH_SIZE
            MAX_PROCESSING_TIME = processing_config.MAX_PROCESSING_TIME
            print(f"ğŸ“‹ ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°")
            if processing_config.ENABLE_DETAILED_LOGGING:
                print(processing_config.get_config_summary())
        else:
            # å¤‡ç”¨é…ç½®
            MAX_BATCH_SIZE = int(os.getenv('MAX_BATCH_SIZE', '10'))
            MAX_PROCESSING_TIME = int(os.getenv('MAX_PROCESSING_TIME', '45'))
            print(f"ğŸ“‹ ä½¿ç”¨ç¯å¢ƒå˜é‡/é»˜è®¤å‚æ•°")
        
        total_tasks = len(tasks)
        predictions = []
        
        print(f"ğŸš€ å¼€å§‹å¤„ç† {total_tasks} ä¸ªä»»åŠ¡")
        print(f"âš™ï¸ é…ç½®: æœ€å¤§æ‰¹é‡={MAX_BATCH_SIZE}, æœ€å¤§æ—¶é—´={MAX_PROCESSING_TIME}ç§’")
        print("="*60)
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ‰¹é‡å¯¼å‡ºæ¨¡å¼
        export_mode = os.getenv('BATCH_EXPORT_MODE', 'false').lower() == 'true'
        export_threshold = int(os.getenv('BATCH_EXPORT_THRESHOLD', '3'))  # é»˜è®¤20ä¸ªä»»åŠ¡ä»¥ä¸Šå¯ç”¨å¯¼å‡ºæ¨¡å¼
        
        if total_tasks >= export_threshold or export_mode:
            print(f"ğŸ“ ä»»åŠ¡æ•°é‡({total_tasks})è¾¾åˆ°å¯¼å‡ºé˜ˆå€¼({export_threshold})ï¼Œå¯ç”¨æ‰¹é‡å¯¼å‡ºæ¨¡å¼")
            print(f"ğŸ’¡ å°†ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨å¯¼å…¥åˆ°Label Studioå‰ç«¯")
            return self._process_batch_export_mode(tasks)
        
        # å¦‚æœä»»åŠ¡æ•°é‡è¶…è¿‡é™åˆ¶ä½†ä¸åˆ°å¯¼å‡ºé˜ˆå€¼ï¼Œä½¿ç”¨åˆ†å—å¤„ç†
        elif total_tasks > MAX_BATCH_SIZE:
            print(f"ğŸ“¦ ä»»åŠ¡æ•°é‡({total_tasks})è¶…è¿‡é™åˆ¶({MAX_BATCH_SIZE})ï¼Œå¯ç”¨åˆ†å—å¤„ç†")
            return self._process_tasks_in_chunks(tasks, MAX_BATCH_SIZE, MAX_PROCESSING_TIME)
        
        # å°æ‰¹é‡å¤„ç†ï¼šç›´æ¥å¤„ç†æ‰€æœ‰ä»»åŠ¡
        start_time = time.time()
        print(f"ğŸ”„ å°æ‰¹é‡å¤„ç†æ¨¡å¼: {total_tasks} ä¸ªä»»åŠ¡")
        print("="*60)
        
        for i, task in enumerate(tasks):
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            elapsed_time = time.time() - start_time
            if elapsed_time > MAX_PROCESSING_TIME:
                print(f"â±ï¸ å¤„ç†æ—¶é—´è¶…è¿‡é™åˆ¶({MAX_PROCESSING_TIME}ç§’)ï¼Œåœæ­¢å¤„ç†")
                print(f"ğŸ“Š å·²å¤„ç†: {i}/{total_tasks} ä¸ªä»»åŠ¡")
                break
            
            print(f"\nğŸ”„ æ­£åœ¨å¤„ç†ä»»åŠ¡ {i+1}/{total_tasks}...")
            print(f"â±ï¸ å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’, å‰©ä½™æ—¶é—´: {MAX_PROCESSING_TIME - elapsed_time:.1f}ç§’")
            
            # æ˜¾ç¤ºä»»åŠ¡å†…å®¹é¢„è§ˆ
            task_data = task.get('data', {})
            text_content = ""
            for key in ['text', 'content', 'prompt', 'question', 'description', 'query']:
                if key in task_data and isinstance(task_data[key], str):
                    text_content = task_data[key]
                    break
            
            if text_content:
                preview = text_content[:50] + "..." if len(text_content) > 50 else text_content
                print(f"   ğŸ“ æ–‡æœ¬é¢„è§ˆ: {preview}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            task_start_time = time.time()
            
            try:
                prediction = self._process_single_task(task)
                task_end_time = time.time()
                task_duration = task_end_time - task_start_time
                
                if prediction:
                    predictions.append(prediction)
                    entities_count = len(prediction.get('result', []))
                    print(f"âœ… ä»»åŠ¡ {i+1} å¤„ç†æˆåŠŸ (è€—æ—¶: {task_duration:.2f}ç§’, å®ä½“æ•°: {entities_count})")
                else:
                    prediction = {
                        "model_version": self.get("model_version"),
                        "score": 0.0,
                        "result": []
                    }
                    predictions.append(prediction)
                    print(f"âš ï¸ ä»»åŠ¡ {i+1} å¤„ç†å®Œæˆä½†æ— ç»“æœ (è€—æ—¶: {task_duration:.2f}ç§’)")
                    
            except Exception as e:
                task_end_time = time.time()
                task_duration = task_end_time - task_start_time
                print(f"âŒ ä»»åŠ¡ {i+1} å¤„ç†å¤±è´¥ (è€—æ—¶: {task_duration:.2f}ç§’): {e}")
                prediction = {
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": []
                }
                predictions.append(prediction)
            
            # å¯¹äºæ‰¹é‡ä»»åŠ¡ï¼Œè°ƒç”¨å›è°ƒå‡½æ•°
            if hasattr(self, '_task_completed_callback') and callable(self._task_completed_callback):
                try:
                    self._task_completed_callback(i+1, total_tasks, prediction)
                except Exception as e:
                    print(f"âš ï¸ å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
            
            # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
            import sys
            sys.stdout.flush()
        
        # å¤„ç†å®Œæˆåçš„æ€»ç»“
        end_time = time.time()
        total_duration = end_time - start_time
        processed_count = len(predictions)
        
        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ")
        print("="*60)
        print("ğŸ“Š å¤„ç†æ€»ç»“:")
        print(f"   å¤„ç†ä»»åŠ¡: {processed_count}/{total_tasks} ä¸ª")
        print(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"   å¹³å‡è€—æ—¶: {total_duration/processed_count:.2f}ç§’/ä»»åŠ¡" if processed_count > 0 else "   å¹³å‡è€—æ—¶: N/A")
        successful_tasks = sum(1 for p in predictions if p.get('result'))
        print(f"   æˆåŠŸä»»åŠ¡: {successful_tasks}/{processed_count} ä¸ª")
        print(f"   æ€»å®ä½“æ•°: {sum(len(p.get('result', [])) for p in predictions)} ä¸ª")
        print("="*60)
        
        return ModelResponse(predictions=predictions)
    
    def _process_tasks_in_chunks(self, tasks: List[Dict], max_batch_size: int, max_processing_time: int) -> ModelResponse:
        """åˆ†å—å¤„ç†å¤§æ‰¹é‡ä»»åŠ¡ï¼Œé¿å…è¶…æ—¶"""
        total_tasks = len(tasks)
        all_predictions = []
        
        # è®¡ç®—åˆ†å—æ•°é‡
        total_chunks = (total_tasks + max_batch_size - 1) // max_batch_size
        
        print(f"ğŸ“¦ åˆ†å—å¤„ç†é…ç½®:")
        print(f"   æ€»ä»»åŠ¡æ•°: {total_tasks}")
        print(f"   æ¯å—å¤§å°: {max_batch_size}")
        print(f"   æ€»å—æ•°: {total_chunks}")
        print(f"   æ¯å—æœ€å¤§æ—¶é—´: {max_processing_time}ç§’")
        print("="*60)
        
        start_time = time.time()
        
        for chunk_idx in range(total_chunks):
            chunk_start = chunk_idx * max_batch_size
            chunk_end = min(chunk_start + max_batch_size, total_tasks)
            chunk_tasks = tasks[chunk_start:chunk_end]
            
            print(f"\nğŸ“‹ å¤„ç†ç¬¬ {chunk_idx + 1}/{total_chunks} å—")
            print(f"   ä»»åŠ¡èŒƒå›´: {chunk_start + 1}-{chunk_end}")
            print(f"   å—å¤§å°: {len(chunk_tasks)}")
            
            # å¤„ç†å½“å‰å—
            chunk_start_time = time.time()
            chunk_predictions = []
            
            for i, task in enumerate(chunk_tasks):
                task_index = chunk_start + i + 1
                
                # æ£€æŸ¥æ€»æ—¶é—´é™åˆ¶
                elapsed_total_time = time.time() - start_time
                if elapsed_total_time > max_processing_time * total_chunks:
                    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´è¶…è¿‡é™åˆ¶ï¼Œåœæ­¢å¤„ç†")
                    print(f"ğŸ“Š å·²å¤„ç†: {len(all_predictions)}/{total_tasks} ä¸ªä»»åŠ¡")
                    return ModelResponse(predictions=all_predictions)
                
                print(f"\nğŸ”„ å¤„ç†ä»»åŠ¡ {task_index}/{total_tasks} (å—å†…: {i+1}/{len(chunk_tasks)})")
                
                try:
                    prediction = self._process_single_task(task)
                    
                    if prediction:
                        chunk_predictions.append(prediction)
                        entities_count = len(prediction.get('result', []))
                        print(f"âœ… ä»»åŠ¡ {task_index} å¤„ç†æˆåŠŸ (å®ä½“æ•°: {entities_count})")
                    else:
                        prediction = {
                            "model_version": self.get("model_version"),
                            "score": 0.0,
                            "result": []
                        }
                        chunk_predictions.append(prediction)
                        print(f"âš ï¸ ä»»åŠ¡ {task_index} å¤„ç†å®Œæˆä½†æ— ç»“æœ")
                        
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡ {task_index} å¤„ç†å¤±è´¥: {e}")
                    prediction = {
                        "model_version": self.get("model_version"),
                        "score": 0.0,
                        "result": []
                    }
                    chunk_predictions.append(prediction)
                
                # è°ƒç”¨å›è°ƒå‡½æ•°
                if hasattr(self, '_task_completed_callback') and callable(self._task_completed_callback):
                    try:
                        self._task_completed_callback(task_index, total_tasks, prediction)
                    except Exception as e:
                        print(f"âš ï¸ å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
            
            # æ·»åŠ åˆ°æ€»ç»“æœä¸­
            all_predictions.extend(chunk_predictions)
            
            chunk_end_time = time.time()
            chunk_duration = chunk_end_time - chunk_start_time
            
            print(f"\nâœ… ç¬¬ {chunk_idx + 1} å—å¤„ç†å®Œæˆ")
            print(f"   è€—æ—¶: {chunk_duration:.2f}ç§’")
            print(f"   æˆåŠŸä»»åŠ¡: {sum(1 for p in chunk_predictions if p.get('result'))}/{len(chunk_predictions)}")
            print(f"   ç´¯è®¡å®Œæˆ: {len(all_predictions)}/{total_tasks}")
            
            # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
            import sys
            sys.stdout.flush()
        
        # æœ€ç»ˆæ€»ç»“
        end_time = time.time()
        total_duration = end_time - start_time
        successful_tasks = sum(1 for p in all_predictions if p.get('result'))
        
        print(f"\nğŸ‰ åˆ†å—å¤„ç†å…¨éƒ¨å®Œæˆ")
        print("="*60)
        print("ğŸ“Š æœ€ç»ˆæ€»ç»“:")
        print(f"   å¤„ç†ä»»åŠ¡: {len(all_predictions)}/{total_tasks}")
        print(f"   æˆåŠŸä»»åŠ¡: {successful_tasks}/{len(all_predictions)}")
        print(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"   å¹³å‡è€—æ—¶: {total_duration/len(all_predictions):.2f}ç§’/ä»»åŠ¡" if all_predictions else "   å¹³å‡è€—æ—¶: N/A")
        print(f"   æ€»å®ä½“æ•°: {sum(len(p.get('result', [])) for p in all_predictions)}")
        print("="*60)
        
        return ModelResponse(predictions=all_predictions)
    
    def _process_batch_export_mode(self, tasks: List[Dict]) -> ModelResponse:
        """æ‰¹é‡å¯¼å‡ºæ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰ä»»åŠ¡å¹¶ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶"""
        total_tasks = len(tasks)
        all_predictions = []
        export_data = {
            "annotations": [],  # æ”¹ä¸ºannotationsè€Œä¸æ˜¯predictions
            "metadata": {
                "processed_at": datetime.datetime.now().isoformat(),
                "total_tasks": total_tasks,
                "model_version": self.get("model_version"),
                "export_format": "label_studio_annotations"  # æ›´æ–°æ ¼å¼æ ‡è¯†
            }
        }
        
        # åˆ›å»ºå¯¼å‡ºç›®å½•
        export_dir = Path("exports")
        export_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"batch_annotations_{timestamp}.json"  # æ”¹ä¸ºannotations
        csv_filename = f"batch_annotations_{timestamp}.csv"   # æ”¹ä¸ºannotations
        log_filename = f"batch_processing_{timestamp}.log"
        
        json_filepath = export_dir / json_filename
        csv_filepath = export_dir / csv_filename
        log_filepath = export_dir / log_filename
        
        print(f"ğŸ“ æ‰¹é‡å¯¼å‡ºæ¨¡å¼å¯åŠ¨")
        print(f"   æ€»ä»»åŠ¡æ•°: {total_tasks}")
        print(f"   å¯¼å‡ºç›®å½•: {export_dir.absolute()}")
        print(f"   JSONæ–‡ä»¶: {json_filename}")
        print(f"   CSVæ–‡ä»¶: {csv_filename}")
        print(f"   æ—¥å¿—æ–‡ä»¶: {log_filename}")
        print("="*60)
        
        start_time = time.time()
        successful_count = 0
        failed_count = 0
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
        with open(log_filepath, 'w', encoding='utf-8') as log_file:
            def log_message(message):
                timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                log_entry = f"[{timestamp}] {message}\n"
                log_file.write(log_entry)
                log_file.flush()
                print(message)
            
            log_message(f"å¼€å§‹æ‰¹é‡å¤„ç† {total_tasks} ä¸ªä»»åŠ¡")
            log_message(f"æ¨¡å‹: {self.model_name}")
            log_message("="*60)
            
            for i, task in enumerate(tasks):
                task_start_time = time.time()
                task_id = task.get('id', f'task_{i+1}')
                
                log_message(f"\nğŸ”„ å¤„ç†ä»»åŠ¡ {i+1}/{total_tasks} (ID: {task_id})")
                
                # æ˜¾ç¤ºä»»åŠ¡å†…å®¹é¢„è§ˆ
                task_data = task.get('data', {})
                text_content = ""
                for key in ['text', 'content', 'prompt', 'question', 'description', 'query']:
                    if key in task_data and isinstance(task_data[key], str):
                        text_content = task_data[key]
                        break
                
                if text_content:
                    preview = text_content[:100] + "..." if len(text_content) > 100 else text_content
                    log_message(f"   ğŸ“ æ–‡æœ¬é¢„è§ˆ: {preview}")
                
                try:
                    prediction = self._process_single_task(task)
                    task_end_time = time.time()
                    task_duration = task_end_time - task_start_time
                    
                    if prediction:
                        all_predictions.append(prediction)
                        entities_count = len(prediction.get('result', []))
                        
                        # ä¸ºå¯¼å‡ºæ ¼å¼æ·»åŠ ä»»åŠ¡ä¿¡æ¯ï¼ˆè½¬æ¢ä¸ºæ ‡æ³¨æ ¼å¼ï¼‰
                        annotation = {
                            "id": len(export_data["annotations"]) + 1,  # æ ‡æ³¨ID
                            "task": task_id,
                            "result": prediction.get('result', []),  # ç›´æ¥ä½¿ç”¨resultï¼Œä¸åŒ…è£…åœ¨predictionä¸­
                            "created_username": "ML-Backend",
                            "created_ago": "now",
                            "completed_by": 1,  # ç³»ç»Ÿç”¨æˆ·ID
                            "was_cancelled": False,
                            "ground_truth": False,
                            "created_at": datetime.datetime.now().isoformat(),
                            "updated_at": datetime.datetime.now().isoformat(),
                            "lead_time": task_duration,
                            "task_data": task_data,
                            "entities_count": entities_count,
                            "model_version": self.get("model_version")
                        }
                        export_data["annotations"].append(annotation)
                        
                        successful_count += 1
                        log_message(f"âœ… ä»»åŠ¡ {i+1} å¤„ç†æˆåŠŸ (è€—æ—¶: {task_duration:.2f}ç§’, å®ä½“æ•°: {entities_count})")
                    else:
                        prediction = {
                            "model_version": self.get("model_version"),
                            "score": 0.0,
                            "result": []
                        }
                        all_predictions.append(prediction)
                        
                        # åˆ›å»ºç©ºçš„æ ‡æ³¨ç»“æœ
                        annotation = {
                            "id": len(export_data["annotations"]) + 1,
                            "task": task_id,
                            "result": [],  # ç©ºç»“æœ
                            "created_username": "ML-Backend",
                            "created_ago": "now",
                            "completed_by": 1,
                            "was_cancelled": False,
                            "ground_truth": False,
                            "created_at": datetime.datetime.now().isoformat(),
                            "updated_at": datetime.datetime.now().isoformat(),
                            "lead_time": task_duration,
                            "task_data": task_data,
                            "entities_count": 0,
                            "model_version": self.get("model_version"),
                            "status": "no_result"
                        }
                        export_data["annotations"].append(annotation)
                        
                        failed_count += 1
                        log_message(f"âš ï¸ ä»»åŠ¡ {i+1} å¤„ç†å®Œæˆä½†æ— ç»“æœ (è€—æ—¶: {task_duration:.2f}ç§’)")
                        
                except Exception as e:
                    task_end_time = time.time()
                    task_duration = task_end_time - task_start_time
                    
                    prediction = {
                        "model_version": self.get("model_version"),
                        "score": 0.0,
                        "result": []
                    }
                    all_predictions.append(prediction)
                    
                    # åˆ›å»ºé”™è¯¯çš„æ ‡æ³¨ç»“æœ
                    annotation = {
                        "id": len(export_data["annotations"]) + 1,
                        "task": task_id,
                        "result": [],  # ç©ºç»“æœ
                        "created_username": "ML-Backend",
                        "created_ago": "now",
                        "completed_by": 1,
                        "was_cancelled": False,
                        "ground_truth": False,
                        "created_at": datetime.datetime.now().isoformat(),
                        "updated_at": datetime.datetime.now().isoformat(),
                        "lead_time": task_duration,
                        "task_data": task_data,
                        "entities_count": 0,
                        "model_version": self.get("model_version"),
                        "status": "error",
                        "error_message": str(e)
                    }
                    export_data["annotations"].append(annotation)
                    
                    failed_count += 1
                    log_message(f"âŒ ä»»åŠ¡ {i+1} å¤„ç†å¤±è´¥ (è€—æ—¶: {task_duration:.2f}ç§’): {e}")
                
                # æ¯10ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
                if (i + 1) % 10 == 0:
                    log_message(f"ğŸ“Š ä¸­é—´è¿›åº¦: {i+1}/{total_tasks}, æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count}")
        
        # ä¿å­˜å¯¼å‡ºæ–‡ä»¶
        end_time = time.time()
        total_duration = end_time - start_time
        
        # æ›´æ–°å…ƒæ•°æ®
        export_data["metadata"].update({
            "processing_duration": total_duration,
            "successful_tasks": successful_count,
            "failed_tasks": failed_count,
            "total_entities": sum(ann.get("entities_count", 0) for ann in export_data["annotations"])
        })
        
        # ä¿å­˜JSONæ–‡ä»¶
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ–‡ä»¶
        self._save_csv_export(csv_filepath, export_data)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ")
        print("="*60)
        print("ğŸ“Š å¤„ç†æ€»ç»“:")
        print(f"   æ€»ä»»åŠ¡æ•°: {total_tasks}")
        print(f"   æˆåŠŸä»»åŠ¡: {successful_count}")
        print(f"   å¤±è´¥ä»»åŠ¡: {failed_count}")
        print(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"   å¹³å‡è€—æ—¶: {total_duration/total_tasks:.2f}ç§’/ä»»åŠ¡")
        print(f"   æ€»å®ä½“æ•°: {export_data['metadata']['total_entities']}")
        print("\nğŸ“ å¯¼å‡ºæ–‡ä»¶:")
        print(f"   JSONæ–‡ä»¶: {json_filepath.absolute()}")
        print(f"   CSVæ–‡ä»¶: {csv_filepath.absolute()}")
        print(f"   æ—¥å¿—æ–‡ä»¶: {log_filepath.absolute()}")
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   1. ä¸‹è½½ç”Ÿæˆçš„JSONæ–‡ä»¶")
        print("   2. åœ¨Label Studioå‰ç«¯é€‰æ‹©'Import'")
        print("   3. é€‰æ‹©'Annotations'å¯¼å…¥ç±»å‹")
        print("   4. ä¸Šä¼ JSONæ–‡ä»¶ä»¥å¯¼å…¥æ‰€æœ‰æ ‡æ³¨ç»“æœ")
        print("="*60)
        
        # è¿”å›ç®€åŒ–çš„å“åº”ï¼ˆé¿å…å‰ç«¯å¤„ç†å¤§é‡æ•°æ®ï¼‰
        summary_response = [{
            "model_version": self.get("model_version"),
            "score": 1.0,
            "result": [{
                "from_name": "prediction",
                "to_name": "text", 
                "type": "textarea",
                "value": {
                    "text": [f"æ‰¹é‡æ ‡æ³¨å®Œæˆï¼æˆåŠŸ: {successful_count}/{total_tasks}\n"
                           f"å¯¼å‡ºæ–‡ä»¶: {json_filename}\n"
                           f"è¯·ä¸‹è½½æ–‡ä»¶å¹¶ä½œä¸ºæ ‡æ³¨ç»“æœå¯¼å…¥åˆ°Label Studioå‰ç«¯"]
                }
            }]
        }]
        
        return ModelResponse(predictions=summary_response)
    
    def _save_csv_export(self, csv_filepath: Path, export_data: dict):
        """ä¿å­˜CSVæ ¼å¼çš„å¯¼å‡ºæ–‡ä»¶"""
        import csv
        
        with open(csv_filepath, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['annotation_id', 'task_id', 'text_content', 'entities_count', 'entities', 'processing_time', 'status']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for ann in export_data["annotations"]:
                task_data = ann.get("task_data", {})
                text_content = ""
                for key in ['text', 'content', 'prompt', 'question', 'description', 'query']:
                    if key in task_data and isinstance(task_data[key], str):
                        text_content = task_data[key]
                        break
                
                # æå–å®ä½“ä¿¡æ¯
                entities_info = []
                result = ann.get("result", [])
                for entity in result:
                    if entity.get("type") == "labels":
                        value = entity.get("value", {})
                        entities_info.append({
                            "text": value.get("text", ""),
                            "label": value.get("labels", []),
                            "start": value.get("start", 0),
                            "end": value.get("end", 0)
                        })
                
                writer.writerow({
                    'annotation_id': ann.get("id", ""),
                    'task_id': ann.get("task", ""),
                    'text_content': text_content[:200] + "..." if len(text_content) > 200 else text_content,
                    'entities_count': ann.get("entities_count", 0),
                    'entities': json.dumps(entities_info, ensure_ascii=False),
                    'processing_time': f"{ann.get('lead_time', 0):.2f}s",
                    'status': ann.get("status", "success")
                })
    
    def _process_single_task(self, task: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task_data = task.get('data', {})
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = ""
        text_keys = ['text', 'content', 'prompt', 'question', 'description', 'query']
        
        for key, value in task_data.items():
            if isinstance(value, str) and key in text_keys:
                text_content = value
                break
        
        if not text_content:
            return None
        
        # æ„å»ºNERæç¤ºè¯ï¼ˆä½¿ç”¨é…ç½®åŒ–çš„å®ä½“æ ‡ç­¾ï¼‰
        json_format = get_json_format_example()
        
        # æŒ‰ç±»åˆ«ç»„ç»‡å®ä½“ç±»å‹è¯´æ˜
        try:
            from entity_config import get_all_categories, get_entities_by_category
            categories = get_all_categories()
            categorized_examples = ""
            
            for category in categories:
                entities = get_entities_by_category(category)
                if entities:
                    categorized_examples += f"\nğŸ“‚ {category}ç±»:\n"
                    for label_key, config in list(entities.items())[:5]:  # æ¯ç±»æœ€å¤šæ˜¾ç¤º5ä¸ªå®ä½“ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
                        examples = "ã€".join(config['examples'][:2])  # æ¯ä¸ªå®ä½“æ˜¾ç¤º2ä¸ªç¤ºä¾‹
                        description = config['description']
                        categorized_examples += f"   â€¢ {description}: {examples}\n"
            
            # ç”Ÿæˆç®€åŒ–çš„å®ä½“åˆ—è¡¨ï¼ˆä½¿ç”¨descriptionï¼‰
            entity_descriptions = []
            for label_key in ENTITY_LABELS[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªï¼Œé¿å…è¿‡é•¿
                if label_key in NER_ENTITY_CONFIG:
                    entity_descriptions.append(NER_ENTITY_CONFIG[label_key]['description'])
                else:
                    entity_descriptions.append(label_key)
            
            entity_labels_list = "ã€".join(entity_descriptions)
            if len(ENTITY_LABELS) > 20:
                entity_labels_list += f"ç­‰{len(ENTITY_LABELS)}ç§å®ä½“ç±»å‹"
                
        except ImportError:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸæ¥çš„æ ¼å¼
            categorized_examples = ""
            for label, config in NER_ENTITY_CONFIG.items():
                examples = "ã€".join(config['examples'][:2])
                categorized_examples += f"   {label}({config['description']}): {examples}\n"
            entity_labels_list = "ã€".join(ENTITY_LABELS)
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Œè¯†åˆ«å‡ºæ–‡æœ¬ä¸­å­˜åœ¨çš„å®ä½“ã€‚

ğŸ“ æ–‡æœ¬å†…å®¹ï¼š
{text_content}

ğŸ¯ æ”¯æŒçš„å®ä½“ç±»å‹åŠç¤ºä¾‹ï¼š{categorized_examples}

âš ï¸ é‡è¦è¯´æ˜ï¼š
1. åªè¯†åˆ«æ–‡æœ¬ä¸­çœŸå®å­˜åœ¨çš„å®ä½“ï¼Œä¸è¦ç¼–é€ 
2. å‡†ç¡®æ ‡æ³¨å®ä½“çš„èµ·å§‹å’Œç»“æŸä½ç½®
3. æ¯ä¸ªå®ä½“å¿…é¡»é€‰æ‹©ä¸‹é¢åˆ—å‡ºçš„æ ‡ç­¾ç±»å‹ä¹‹ä¸€
4. æ ‡ç­¾åç§°å¿…é¡»å®Œå…¨åŒ¹é…ï¼Œä¸èƒ½ä½¿ç”¨è¿‘ä¼¼æˆ–ç®€åŒ–çš„åç§°
5. å¦‚æœä¸ç¡®å®šå®ä½“ç±»å‹ï¼Œé€‰æ‹©æœ€ç›¸è¿‘çš„ç±»åˆ«

ğŸ“‹ è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{json_format}

ğŸ·ï¸ æ‰€æœ‰æœ‰æ•ˆçš„æ ‡ç­¾ç±»å‹ï¼ˆå¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹æ ‡ç­¾ä¹‹ä¸€ï¼‰ï¼š
{entity_labels_list}

æ³¨æ„ï¼šæ ‡ç­¾åç§°å¿…é¡»ä¸ä¸Šé¢åˆ—å‡ºçš„å®Œå…¨ä¸€è‡´ï¼Œä¸æ¥å—ä»»ä½•å˜ä½“æˆ–ç®€åŒ–å½¢å¼ï¼"""
        
        # è°ƒç”¨API
        api_response = self._call_modelscope_api(prompt)
        
        if api_response:
            return self._format_prediction(api_response, task)
        
        return None
    
    def _call_modelscope_api(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨é­”å¡”ç¤¾åŒºAPI"""
        if not self.client:
            print("âŒ OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        
        print(f"ğŸ“¤ å‘é€APIè¯·æ±‚...")
        print(f"   æ¨¡å‹: {self.model_name}")
        print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant specialized in Named Entity Recognition. Always respond with valid JSON format."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.1,
                top_p=0.9,
                stream=False
            )
            
            print(f"ğŸ“¥ æ”¶åˆ°APIå“åº”")
            
            if response.choices and len(response.choices) > 0:
                content = response.choices[0].message.content
                print(f"âœ… å“åº”å†…å®¹é•¿åº¦: {len(content) if content else 0} å­—ç¬¦")
                if content:
                    print(f"ğŸ“‹ å“åº”å†…å®¹é¢„è§ˆ: {content[:300]}{'...' if len(content) > 300 else ''}")
                return content
            else:
                print("âŒ APIå“åº”ä¸­æ²¡æœ‰choices")
                return None
                
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {str(e)}")
            print(f"   å®Œæ•´é”™è¯¯ä¿¡æ¯: {repr(e)}")
            return None
    
    def _format_prediction(self, api_response: str, task: Dict) -> Dict:
        """æ ¼å¼åŒ–é¢„æµ‹ç»“æœä¸ºLabel Studioæ ¼å¼"""
        print(f"\nğŸ”„ æ ¼å¼åŒ–é¢„æµ‹ç»“æœ:")
        print(f"   APIå“åº”é•¿åº¦: {len(api_response)} å­—ç¬¦")
        print(f"   APIå“åº”å†…å®¹: {api_response[:200]}{'...' if len(api_response) > 200 else ''}")
        
        prediction = {
            "model_version": self.get("model_version"),
            "score": 0.95,
            "result": []
        }
        
        # å°è¯•è§£æNERç»“æœ
        ner_results = self._parse_ner_response(api_response, task)
        if ner_results:
            prediction["result"] = ner_results
            print(f"âœ… NERè§£ææˆåŠŸï¼Œè¯†åˆ«åˆ° {len(ner_results)} ä¸ªå®ä½“")
            for i, result in enumerate(ner_results):
                entity = result.get('value', {})
                text = entity.get('text', '')
                labels = entity.get('labels', [])
                start = entity.get('start', 0)
                end = entity.get('end', 0)
                print(f"   å®ä½“ {i+1}: [{text}] -> {labels} ({start}-{end})")
            return prediction
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šè¿”å›åŸå§‹æ–‡æœ¬
        print("âš ï¸ NERè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬æ ¼å¼")
        prediction["result"].append({
            "from_name": "prediction",
            "to_name": "text",
            "type": "textarea",
            "value": {
                "text": [api_response]
            }
        })
        
        return prediction
    
    def _parse_ner_response(self, api_response: str, task: Dict) -> Optional[List[Dict]]:
        """è§£æAIè¿”å›çš„å‘½åå®ä½“è¯†åˆ«JSONç»“æœ"""
        print(f"\nğŸ” å¼€å§‹è§£æNERå“åº”...")
        
        if not api_response or not api_response.strip():
            print("âŒ APIå“åº”ä¸ºç©º")
            return None
        
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                print("ğŸ”§ å°è¯•ç›´æ¥JSONè§£æ...")
                ner_data = json.loads(api_response.strip())
                print("âœ… ç›´æ¥JSONè§£ææˆåŠŸ")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ ç›´æ¥JSONè§£æå¤±è´¥: {e}")
                # å°è¯•æå–JSONéƒ¨åˆ†
                import re
                print("ğŸ”§ å°è¯•æå–JSONç‰‡æ®µ...")
                
                # å¤šç§JSONæå–ç­–ç•¥
                patterns = [
                    r'\{[^{}]*"entities"[^{}]*:.*?\}',  # æœ€ä¸¥æ ¼çš„entitiesåŒ¹é…
                    r'\{.*?"entities".*?\}',            # å®½æ¾çš„entitiesåŒ¹é…
                    r'\{.*\}',                          # æœ€å®½æ¾çš„JSONåŒ¹é…
                ]
                
                ner_data = None
                for i, pattern in enumerate(patterns):
                    json_match = re.search(pattern, api_response, re.DOTALL)
                    if json_match:
                        try:
                            ner_data = json.loads(json_match.group())
                            print(f"âœ… JSONæå–æˆåŠŸ (ç­–ç•¥ {i+1})")
                            break
                        except json.JSONDecodeError:
                            print(f"âš ï¸ JSONæå–ç­–ç•¥ {i+1} å¤±è´¥")
                            continue
                
                if not ner_data:
                    print("âŒ æ‰€æœ‰JSONæå–ç­–ç•¥éƒ½å¤±è´¥")
                    print(f"ğŸ“„ åŸå§‹å“åº”å†…å®¹: {api_response}")
                    return None
            
            # æ£€æŸ¥entitieså­—æ®µ
            if 'entities' not in ner_data or not isinstance(ner_data['entities'], list):
                return None
            
            entities = ner_data['entities']
            
            # è·å–åŸå§‹æ–‡æœ¬
            task_data = task.get('data', {})
            original_text = ""
            for key in ['text', 'content', 'prompt', 'question', 'description', 'query']:
                if key in task_data and isinstance(task_data[key], str):
                    original_text = task_data[key]
                    break
            
            if not original_text:
                return None
            
            print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {original_text}")
            print(f"ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦: {len(original_text)} å­—ç¬¦")
            
            # è½¬æ¢ä¸ºLabel Studioæ ¼å¼
            results = []
            for i, entity in enumerate(entities):
                # éªŒè¯å¿…éœ€å­—æ®µ
                if not all(key in entity for key in ['text', 'start', 'end', 'label']):
                    print(f"   âš ï¸ å®ä½“ {i+1} ç¼ºå°‘å¿…éœ€å­—æ®µï¼Œè·³è¿‡")
                    continue
                
                start = entity['start']
                end = entity['end']
                text = entity['text']
                original_label = entity['label']
                
                print(f"\nğŸ” å¤„ç†å®ä½“ {i+1}: {entity}")
                
                # éªŒè¯å’Œæ˜ å°„æ ‡ç­¾
                validated_label_key = validate_and_map_label(original_label)
                if not validated_label_key:
                    print(f"   âŒ å®ä½“ {i+1} æ ‡ç­¾æ— æ•ˆ: '{original_label}'ï¼Œè·³è¿‡")
                    continue
                
                # è·å–æ ‡ç­¾çš„descriptionä½œä¸ºæœ€ç»ˆè¿”å›å€¼
                if validated_label_key in NER_ENTITY_CONFIG:
                    label = NER_ENTITY_CONFIG[validated_label_key]['description']
                    print(f"   ğŸ“ æ ‡ç­¾æ˜ å°„: '{original_label}' -> '{validated_label_key}' -> '{label}'")
                else:
                    label = validated_label_key
                    print(f"   ğŸ“ æ ‡ç­¾å·²ä¿®æ­£: '{original_label}' -> '{label}'")
                
                # éªŒè¯ä½ç½®ä¿¡æ¯åŸºæœ¬åˆç†æ€§
                if not isinstance(start, int) or not isinstance(end, int) or start < 0:
                    print(f"   âŒ å®ä½“ {i+1} ä½ç½®ä¿¡æ¯æ— æ•ˆ (start={start}, end={end})ï¼Œè·³è¿‡")
                    continue
                
                print(f"   ğŸ“‹ AIæä¾›çš„æ–‡æœ¬: '{text}'")
                print(f"   ğŸ“ åŸå§‹ä½ç½®: {start}-{end}")
                
                # å…ˆå°è¯•ä¿®æ­£ä½ç½®ï¼Œå†è¿›è¡ŒèŒƒå›´æ£€æŸ¥
                corrected_start, corrected_end, corrected_text = self._correct_entity_position(
                    original_text, text, start, end
                )
                
                # æ£€æŸ¥ä¿®æ­£åçš„ä½ç½®æ˜¯å¦åˆç†
                if corrected_start is None or corrected_end is None or corrected_text is None:
                    print(f"   âŒ å®ä½“ {i+1} ä½ç½®ä¿®æ­£å¤±è´¥ï¼Œè·³è¿‡")
                    continue
                
                # éªŒè¯ä¿®æ­£åçš„ä½ç½®ä¸è¶…å‡ºæ–‡æœ¬é•¿åº¦
                if corrected_end > len(original_text) or corrected_start < 0:
                    print(f"   âŒ å®ä½“ {i+1} ä¿®æ­£åä½ç½®è¶…å‡ºæ–‡æœ¬é•¿åº¦ (start={corrected_start}, end={corrected_end}, text_len={len(original_text)})ï¼Œè·³è¿‡")
                    continue
                
                print(f"   ğŸ“‹ ä¿®æ­£åçš„æ–‡æœ¬: '{corrected_text}'")
                print(f"   ğŸ“ ä¿®æ­£åä½ç½®: {corrected_start}-{corrected_end}")
                
                if corrected_text:
                    # éªŒè¯ä¿®æ­£åçš„å®ä½“æ˜¯å¦åˆç†ï¼ˆé•¿åº¦ä¸èƒ½å¤ªçŸ­ï¼Œä¸èƒ½åªæ˜¯æ ‡ç‚¹ç¬¦å·ï¼‰
                    # ä½¿ç”¨validated_label_keyè¿›è¡ŒéªŒè¯ï¼ˆé…ç½®æ–‡ä»¶ä¸­çš„é”®åï¼‰
                    if self._is_valid_entity(corrected_text, validated_label_key):
                        result = {
                            "from_name": "label",
                            "to_name": "text",
                            "type": "labels",
                            "value": {
                                "start": corrected_start,
                                "end": corrected_end,
                                "text": corrected_text,
                                "labels": [label]
                            }
                        }
                        
                        results.append(result)
                        print(f"   âœ… å®ä½“ {i+1} å·²æ·»åŠ : '{corrected_text}' -> {label} ({corrected_start}-{corrected_end})")
                    else:
                        print(f"   âŒ å®ä½“ {i+1} éªŒè¯å¤±è´¥: '{corrected_text}' ä¸æ˜¯æœ‰æ•ˆçš„ {label} å®ä½“")
                else:
                    print(f"   âŒ å®ä½“ {i+1} æ— æ³•ä¿®æ­£ä½ç½®ï¼Œè·³è¿‡")
            
            print(f"\nğŸ“Š æœ€ç»ˆæœ‰æ•ˆå®ä½“æ•°é‡: {len(results)}")
            return results if results else None
            
        except Exception as e:
            print(f"âŒ è§£æNERç»“æœå¼‚å¸¸: {e}")
            return None
    
    def _correct_entity_position(self, original_text: str, entity_text: str, start: int, end: int) -> tuple:
        """ä¿®æ­£å®ä½“ä½ç½®"""
        # é¦–å…ˆæ£€æŸ¥åŸå§‹ä½ç½®æ˜¯å¦æ­£ç¡®
        if start < len(original_text) and end <= len(original_text):
            extracted = original_text[start:end]
            if extracted == entity_text:
                return start, end, entity_text
        
        # æ¸…ç†å®ä½“æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        clean_entity = entity_text.strip()
        if not clean_entity:
            return None, None, None
        
        # åœ¨åŸæ–‡ä¸­æœç´¢å®ä½“æ–‡æœ¬
        try:
            # å°è¯•ç²¾ç¡®åŒ¹é…
            exact_start = original_text.find(clean_entity)
            if exact_start != -1:
                exact_end = exact_start + len(clean_entity)
                print(f"   ğŸ”§ ç²¾ç¡®åŒ¹é…ä¿®æ­£: '{clean_entity}' ({exact_start}-{exact_end})")
                return exact_start, exact_end, clean_entity
            
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
            import re
            clean_text_for_search = re.sub(r'[^\w\u4e00-\u9fff]', '', clean_entity)
            if len(clean_text_for_search) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦æ‰è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                for i in range(len(original_text) - len(clean_text_for_search) + 1):
                    slice_text = original_text[i:i + len(clean_text_for_search)]
                    clean_slice = re.sub(r'[^\w\u4e00-\u9fff]', '', slice_text)
                    if clean_slice == clean_text_for_search:
                        print(f"   ğŸ”§ æ¨¡ç³ŠåŒ¹é…ä¿®æ­£: '{slice_text}' ({i}-{i + len(clean_text_for_search)})")
                        return i, i + len(clean_text_for_search), slice_text
            
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
            if len(clean_entity) >= 3:
                core_part = clean_entity[:min(len(clean_entity), 5)]  # å–å‰å‡ ä¸ªå­—ç¬¦ä½œä¸ºæ ¸å¿ƒ
                core_start = original_text.find(core_part)
                if core_start != -1:
                    # å°è¯•æ‰©å±•åŒ¹é…
                    extended_end = min(core_start + len(clean_entity) + 2, len(original_text))
                    extended_text = original_text[core_start:extended_end]
                    print(f"   ğŸ”§ éƒ¨åˆ†åŒ¹é…ä¿®æ­£: '{extended_text}' ({core_start}-{extended_end})")
                    return core_start, extended_end, extended_text
            
        except Exception as e:
            print(f"   âŒ ä½ç½®ä¿®æ­£å¤±è´¥: {e}")
        
        return None, None, None
    
    def _is_valid_entity(self, text: str, label: str) -> bool:
        """éªŒè¯å®ä½“æ˜¯å¦åˆç†ï¼ˆä½¿ç”¨é…ç½®åŒ–çš„éªŒè¯è§„åˆ™ï¼‰"""
        if not text or len(text.strip()) < 1:
            return False
        
        # å»é™¤é¦–å°¾æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        clean_text = text.strip()
        
        # ä¸èƒ½åªæ˜¯æ ‡ç‚¹ç¬¦å·
        import re
        if re.match(r'^[^\w\u4e00-\u9fff]+$', clean_text):
            return False
        
        # é•¿åº¦éªŒè¯
        if len(clean_text) < 1:
            return False
        
        # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦åœ¨é…ç½®ä¸­
        if label not in NER_ENTITY_CONFIG:
            return True  # å¦‚æœä¸åœ¨é…ç½®ä¸­ï¼Œé»˜è®¤é€šè¿‡
        
        config = NER_ENTITY_CONFIG[label]
        
        # æ£€æŸ¥æ— æ•ˆæ¨¡å¼ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if 'invalid_patterns' in config:
            for pattern in config['invalid_patterns']:
                if re.search(pattern, clean_text):
                    return False
        
        # æ£€æŸ¥æœ‰æ•ˆæ¨¡å¼ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if 'valid_patterns' in config:
            valid_patterns = config['valid_patterns']
            has_valid_pattern = any(re.search(pattern, clean_text) for pattern in valid_patterns)
            if not has_valid_pattern and len(clean_text) < 4:
                return False
        
        return True
    
    def _extract_choice(self, response: str, choices: List[str]) -> Optional[str]:
        """ä»å“åº”ä¸­æå–æœ€åŒ¹é…çš„é€‰æ‹©"""
        response_lower = response.lower()
        for choice in choices:
            if choice.lower() in response_lower:
                return choice
        return choices[0] if choices else None
    
    def fit(self, event, data, **kwargs):
        """
        è®­ç»ƒ/æ›´æ–°æ¨¡å‹
        :param event: äº‹ä»¶ç±»å‹ ('ANNOTATION_CREATED', 'ANNOTATION_UPDATED', 'START_TRAINING')
        :param data: äº‹ä»¶æ•°æ®
        """
        # æ›´æ–°ç¼“å­˜æ•°æ®
        old_data = self.get('my_data')
        self.set('my_data', 'updated_data')
        self.set('model_version', 'updated_version')
        print(f"âœ… æ¨¡å‹å·²æ›´æ–° (äº‹ä»¶: {event})")

