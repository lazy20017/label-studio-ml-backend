from typing import List, Dict, Optional
import json
import os
import time
from openai import OpenAI
from label_studio_ml.model import LabelStudioMLBase
from label_studio_ml.response import ModelResponse

# å¯åŠ¨å‘½ä»¤  label-studio-ml start my_ml_backend

# ==================== å‘½åå®ä½“é…ç½® ====================
# ä»é…ç½®æ–‡ä»¶å¯¼å…¥å®ä½“é…ç½®
try:
    from entity_config import get_entity_config, get_entity_labels, get_all_categories, get_entities_by_category
    NER_ENTITY_CONFIG = get_entity_config()
    ENTITY_LABELS = get_entity_labels()
    print(f"âœ… åŠ è½½äº† {len(ENTITY_LABELS)} ç§å®ä½“ç±»å‹")
except ImportError:
    print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé€€å‡ºç¨‹åº")
    exit()


# ç”ŸæˆJSONæ ¼å¼ç¤ºä¾‹
def get_json_format_example():
    """ç”ŸæˆJSONæ ¼å¼ç¤ºä¾‹"""
    return """{{
  "entities": [
    {{
      "text": "å®ä½“æ–‡æœ¬",
      "start": èµ·å§‹ä½ç½®,
      "end": ç»“æŸä½ç½®,
      "label": "å®ä½“ç±»å‹"
    }}
  ]
}}"""

def validate_label(label: str) -> str:
    """éªŒè¯æ ‡ç­¾æ˜¯å¦åœ¨æœ‰æ•ˆæ ‡ç­¾åˆ—è¡¨ä¸­"""
    if not label:
        return None
    
    clean_label = label.strip()
    
    # ç›´æ¥åŒ¹é…æ ‡ç­¾åç§°
    if clean_label in ENTITY_LABELS:
        return clean_label
    
    return None



class NewModel(LabelStudioMLBase):
    """Custom ML Backend model
    """
    
    def setup(self):
        """Configure any parameters of your model here
        """
        self.set("model_version", "0.0.1")
        
        # é­”å¡”ç¤¾åŒºAPIé…ç½®
        self.api_key = os.getenv('MODELSCOPE_API_KEY', 'ms-2c045fb7-f463-45bf-b0f9-a36d50b0400e')
        self.api_base_url = os.getenv('MODELSCOPE_API_URL', 'https://api-inference.modelscope.cn/v1')
        
        # ğŸ”„ å¤šæ¨¡å‹é…ç½® - æŒ‰ä¼˜å…ˆçº§æ’åº
        self.available_models = [
                'Qwen/Qwen3-Coder-480B-A35B-Instruct', # ä¸»åŠ›æ¨¡å‹ - æœ€æ–°Qwen3
                'ZhipuAI/GLM-4.5', # å¤‡ç”¨æ¨¡å‹1 - å¤§å‚æ•°é‡
                'deepseek-ai/DeepSeek-V3.1', # å¤‡ç”¨æ¨¡å‹2 - ä¸­ç­‰å‚æ•°é‡
                'Qwen/Qwen3-235B-A22B-Instruct-2507', # å¤‡ç”¨æ¨¡å‹2 - ä¸­ç­‰å‚æ•°é‡
                'deepseek-ai/DeepSeek-R1-0528', # å¤‡ç”¨æ¨¡å‹5 - å¹³è¡¡æ€§èƒ½
                'deepseek-ai/DeepSeek-R1-0528' # å¤‡ç”¨æ¨¡å‹6 - å¹³è¡¡æ€§èƒ½
        ]
        
        # ğŸ¯ æ¨¡å‹åˆ‡æ¢æ§åˆ¶
        self.current_model_index = 0  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹ç´¢å¼•
        self.model_consecutive_failures = 0  # å½“å‰æ¨¡å‹è¿ç»­å¤±è´¥æ¬¡æ•°
        self.max_model_failures = 3  # æ¨¡å‹è¿ç»­å¤±è´¥é˜ˆå€¼
        self.model_failure_history = {}  # è®°å½•æ¯ä¸ªæ¨¡å‹çš„å¤±è´¥å†å²
        
        # åŠ¨æ€è·å–å½“å‰æ¨¡å‹åç§°
        self.model_name = self.available_models[self.current_model_index]
        
        # å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œåªåœ¨éœ€è¦æ—¶è¿æ¥
        self.client = None
        self._api_initialized = False
        
        print("âœ… MLåç«¯åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ¯ ä¸»åŠ›æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“‹ å¤‡ç”¨æ¨¡å‹: {len(self.available_models)-1} ä¸ª")
        
    def _switch_to_next_model(self):
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹"""
        # è®°å½•å½“å‰æ¨¡å‹çš„å¤±è´¥
        current_model = self.available_models[self.current_model_index]
        if current_model not in self.model_failure_history:
            self.model_failure_history[current_model] = 0
        self.model_failure_history[current_model] += self.model_consecutive_failures
        
        # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
        old_model = self.model_name
        old_index = self.current_model_index
        
        self.current_model_index = (self.current_model_index + 1) % len(self.available_models)
        self.model_name = self.available_models[self.current_model_index]
        self.model_consecutive_failures = 0  # é‡ç½®æ–°æ¨¡å‹çš„å¤±è´¥è®¡æ•°
        
        # é‡ç½®APIè¿æ¥ï¼ˆå¼ºåˆ¶é‡æ–°è¿æ¥æ–°æ¨¡å‹ï¼‰
        self._api_initialized = False
        self.client = None
        
        print(f"ğŸ”„ æ¨¡å‹åˆ‡æ¢: {old_model} â†’ {self.model_name}")
        print(f"ğŸ“Š åˆ‡æ¢åŸå› : è¿ç»­å¤±è´¥ {self.max_model_failures} æ¬¡")
        
        # å¦‚æœå›åˆ°äº†ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œè¯´æ˜æ‰€æœ‰æ¨¡å‹éƒ½è¯•è¿‡äº†
        if self.current_model_index == 0 and old_index != 0:
            print("âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½å·²å°è¯•ï¼Œå›åˆ°ä¸»åŠ›æ¨¡å‹")
            
        return True
    
    def _handle_model_failure(self):
        """å¤„ç†æ¨¡å‹å¤±è´¥ï¼Œå†³å®šæ˜¯å¦åˆ‡æ¢æ¨¡å‹"""
        self.model_consecutive_failures += 1
        current_model = self.available_models[self.current_model_index]
        
        print(f"âŒ æ¨¡å‹ {current_model} è¿ç»­å¤±è´¥: {self.model_consecutive_failures}/{self.max_model_failures}")
        
        # å¦‚æœè¾¾åˆ°å¤±è´¥é˜ˆå€¼ï¼Œåˆ‡æ¢æ¨¡å‹
        if self.model_consecutive_failures >= self.max_model_failures:
            if len(self.available_models) > 1:  # åªæœ‰åœ¨æœ‰å¤šä¸ªæ¨¡å‹æ—¶æ‰åˆ‡æ¢
                self._switch_to_next_model()
                return True  # è¡¨ç¤ºå·²åˆ‡æ¢æ¨¡å‹
            else:
                print("âš ï¸ åªæœ‰ä¸€ä¸ªæ¨¡å‹å¯ç”¨ï¼Œæ— æ³•åˆ‡æ¢")
                return False
        
        return False  # æ²¡æœ‰åˆ‡æ¢æ¨¡å‹
    
    def _handle_model_success(self):
        """å¤„ç†æ¨¡å‹æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°"""
        if self.model_consecutive_failures > 0:
            print(f"âœ… æ¨¡å‹ {self.model_name} æ¢å¤æ­£å¸¸")
            self.model_consecutive_failures = 0
        
    def _ensure_api_connection(self):
        """ç¡®ä¿APIè¿æ¥å·²åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._api_initialized and self.client:
            return True
        
        if not self.api_key:
            print("âš ï¸ è¯·è®¾ç½®MODELSCOPE_API_KEYç¯å¢ƒå˜é‡")
            return False
        
        try:
            print("ğŸ”„ æ­£åœ¨è¿æ¥å¤§æ¨¡å‹API...")
            self.client = OpenAI(
                base_url=self.api_base_url,
                api_key=self.api_key
            )
            
            # æµ‹è¯•è¿æ¥
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10,
                temperature=0.1
            )
            
            self._api_initialized = True
            print("âœ… å¤§æ¨¡å‹APIè¿æ¥æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ APIè¿æ¥å¤±è´¥: {str(e)[:100]}")
            self.client = None
            self._api_initialized = False
            return False
    


    def predict(self, tasks: List[Dict], context: Optional[Dict] = None, **kwargs) -> ModelResponse:
        """ å‘½åå®ä½“è¯†åˆ«é¢„æµ‹
            :param tasks: Label Studio tasks in JSON format
            :param context: Label Studio context in JSON format
            :return: ModelResponse with predictions
        """
        total_tasks = len(tasks)
        predictions = []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå®ä½“æ ‡æ³¨ä»»åŠ¡
        if not self._is_annotation_task(tasks):
            print("â„¹ï¸ éæ ‡æ³¨ä»»åŠ¡ï¼Œè·³è¿‡å¤§æ¨¡å‹è¿æ¥")
            # è¿”å›ç©ºé¢„æµ‹ç»“æœ
            empty_predictions = []
            for task in tasks:
                empty_prediction = {
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": []
                }
                empty_predictions.append(empty_prediction)
            return ModelResponse(predictions=empty_predictions)
        
        # åªåœ¨éœ€è¦è¿›è¡Œæ ‡æ³¨é¢„æµ‹æ—¶æ‰è¿æ¥å¤§æ¨¡å‹
        if not self._ensure_api_connection():
            print("âŒ æ— æ³•è¿æ¥å¤§æ¨¡å‹APIï¼Œè¿”å›ç©ºé¢„æµ‹ç»“æœ")
            empty_predictions = []
            for task in tasks:
                empty_prediction = {
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": [],
                    "error": "APIè¿æ¥å¤±è´¥"
                }
                empty_predictions.append(empty_prediction)
            return ModelResponse(predictions=empty_predictions)
        
        if total_tasks > 1:
            print(f"ğŸš€ å¼€å§‹å¤„ç† {total_tasks} ä¸ªæ ‡æ³¨ä»»åŠ¡")
        
        start_time = time.time()
        
        for i, task in enumerate(tasks):
            if total_tasks > 1:  # å¤šä»»åŠ¡æ—¶æ˜¾ç¤ºè¿›åº¦
                print(f"\nğŸ”„ å¤„ç†ä»»åŠ¡ {i+1}/{total_tasks}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            task_start_time = time.time()
            
            try:
                prediction = self._process_single_task(task)
                task_end_time = time.time()
                task_duration = task_end_time - task_start_time
                
                if prediction and prediction.get('result') and len(prediction.get('result', [])) > 0:
                    # æˆåŠŸè¯†åˆ«åˆ°å®ä½“
                    predictions.append(prediction)
                    entities_count = len(prediction.get('result', []))
                    if total_tasks > 1:
                        print(f"âœ… ä»»åŠ¡ {i+1} æˆåŠŸ (è€—æ—¶: {task_duration:.1f}s, å®ä½“: {entities_count})")
                else:
                    # æœªè¯†åˆ«åˆ°å®ä½“æˆ–å¤„ç†å¤±è´¥
                    failed_prediction = {
                        "model_version": self.get("model_version"),
                        "score": 0.0,
                        "result": [],
                        "error": "æœªè¯†åˆ«åˆ°ä»»ä½•å®ä½“",
                        "status": "failed"
                    }
                    predictions.append(failed_prediction)
                    if total_tasks > 1:
                        print(f"âŒ ä»»åŠ¡ {i+1} å¤±è´¥ - æ— å®ä½“ (è€—æ—¶: {task_duration:.1f}s)")
                    
            except Exception as e:
                task_end_time = time.time()
                task_duration = task_end_time - task_start_time
                if total_tasks > 1:
                    print(f"âŒ ä»»åŠ¡ {i+1} å¼‚å¸¸ (è€—æ—¶: {task_duration:.1f}s): {str(e)[:50]}")
                failed_prediction = {
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": [],
                    "error": f"å¤„ç†å¼‚å¸¸: {str(e)}",
                    "status": "failed"
                }
                predictions.append(failed_prediction)
            
        
        # å¤„ç†å®Œæˆåçš„æ€»ç»“
        end_time = time.time()
        total_duration = end_time - start_time
        processed_count = len(predictions)
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡
        successful_tasks = sum(1 for p in predictions if p.get('result') and len(p.get('result', [])) > 0)
        failed_tasks = processed_count - successful_tasks
        total_entities = sum(len(p.get('result', [])) for p in predictions)
        
        if total_tasks > 1:
            print(f"\nğŸ“Š å¤„ç†å®Œæˆ: {successful_tasks}/{processed_count} æˆåŠŸ, æ€»å®ä½“: {total_entities}, è€—æ—¶: {total_duration:.1f}s")
            if failed_tasks > 0:
                print(f"âš ï¸ {failed_tasks} ä¸ªä»»åŠ¡å¤„ç†å¤±è´¥")
            
            # æ˜¾ç¤ºæ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
            self._print_model_statistics()
        
        return ModelResponse(predictions=predictions)
    
    def _is_annotation_task(self, tasks: List[Dict]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦è¿›è¡Œå®ä½“æ ‡æ³¨çš„ä»»åŠ¡"""
        if not tasks:
            return False
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åŒ…å«éœ€è¦æ ‡æ³¨çš„æ–‡æœ¬å†…å®¹
        for task in tasks:
            task_data = task.get('data', {})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹éœ€è¦æ ‡æ³¨
            text_keys = ['text', 'content', 'prompt', 'question', 'description', 'query']
            has_text_content = False
            
            for key, value in task_data.items():
                if isinstance(value, str) and key in text_keys and value.strip():
                    has_text_content = True
                    break
            
            if has_text_content:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ ‡æ³¨ç»“æœï¼ˆå¦‚æœæœ‰å®Œæ•´æ ‡æ³¨åˆ™å¯èƒ½æ˜¯æŸ¥çœ‹ä»»åŠ¡ï¼‰
                annotations = task.get('annotations', [])
                if annotations:
                    # å¦‚æœæœ‰æ ‡æ³¨ä½†æ˜¯ç©ºçš„ï¼Œè¯´æ˜éœ€è¦é¢„æµ‹
                    for annotation in annotations:
                        result = annotation.get('result', [])
                        if not result:  # ç©ºæ ‡æ³¨ï¼Œéœ€è¦é¢„æµ‹
                            return True
                else:
                    # æ²¡æœ‰æ ‡æ³¨ï¼Œéœ€è¦é¢„æµ‹
                    return True
        
        return False
    
    def _process_single_task(self, task: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task_data = task.get('data', {})
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = ""
        text_keys = ['text', 'content', 'prompt', 'question', 'description', 'query']
        
        for key, value in task_data.items():
            if isinstance(value, str) and key in text_keys:
                text_content = value
                break
        
        if not text_content:
            return None
        
        # æ„å»ºNERæç¤ºè¯ï¼ˆä½¿ç”¨é…ç½®åŒ–çš„å®ä½“æ ‡ç­¾ï¼‰
        json_format = get_json_format_example()
        
        # æŒ‰ç±»åˆ«ç»„ç»‡å®ä½“ç±»å‹è¯´æ˜
        try:
            from entity_config import get_all_categories, get_entities_by_category
            categories = get_all_categories()
            categorized_examples = ""
            
            for category in categories:
                entities = get_entities_by_category(category)
                if entities:
                    # ç‰¹æ®Šå¤„ç†å…³ç³»æ ‡ç­¾ç±»åˆ«
                    if category == "å…³ç³»æ ‡ç­¾":
                        categorized_examples += f"\nğŸ”— {category}ç±»ï¼ˆè¯­ä¹‰å…³ç³»å®ä½“ï¼‰:\n"
                        for label_key, config in list(entities.items())[:8]:  # å…³ç³»æ ‡ç­¾æ˜¾ç¤ºæ›´å¤š
                            examples = "ã€".join(config['examples'][:3])  # å…³ç³»æ ‡ç­¾æ˜¾ç¤º3ä¸ªç¤ºä¾‹
                            description = config['description']
                            categorized_examples += f"   â€¢ {description}: {examples}\n"
                    else:
                        categorized_examples += f"\nğŸ“‚ {category}ç±»:\n"
                        for label_key, config in list(entities.items())[:5]:  # æ¯ç±»æœ€å¤šæ˜¾ç¤º5ä¸ªå®ä½“ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
                            examples = "ã€".join(config['examples'][:2])  # æ¯ä¸ªå®ä½“æ˜¾ç¤º2ä¸ªç¤ºä¾‹
                            description = config['description']
                            categorized_examples += f"   â€¢ {description}: {examples}\n"
            
            # ç”Ÿæˆç®€åŒ–çš„å®ä½“åˆ—è¡¨ï¼ˆä½¿ç”¨descriptionï¼‰
            entity_descriptions = []
            for label_key in ENTITY_LABELS[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªï¼Œé¿å…è¿‡é•¿
                if label_key in NER_ENTITY_CONFIG:
                    entity_descriptions.append(NER_ENTITY_CONFIG[label_key]['description'])
                else:
                    entity_descriptions.append(label_key)
            
            entity_labels_list = "ã€".join(entity_descriptions)
            if len(ENTITY_LABELS) > 20:
                entity_labels_list += f"ç­‰{len(ENTITY_LABELS)}ç§å®ä½“ç±»å‹"
                
        except ImportError:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸæ¥çš„æ ¼å¼
            categorized_examples = ""
            for label, config in NER_ENTITY_CONFIG.items():
                examples = "ã€".join(config['examples'][:2])
                categorized_examples += f"   {label}({config['description']}): {examples}\n"
            entity_labels_list = "ã€".join(ENTITY_LABELS)
        
        # ç”Ÿæˆä¸¥æ ¼çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆåªæ˜¾ç¤ºæ ‡ç­¾åç§°ï¼Œä¸æ˜¾ç¤ºæè¿°ï¼‰
        valid_labels_only = list(ENTITY_LABELS)
        labels_display = "ã€".join(valid_labels_only)
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Œè¯†åˆ«å‡ºæ–‡æœ¬ä¸­å­˜åœ¨çš„æ‰€æœ‰å®ä½“ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„å‘½åå®ä½“å’Œå…³ç³»è¡¨è¾¾ã€‚

ğŸ“ æ–‡æœ¬å†…å®¹ï¼š
{text_content}

ğŸ¯ æ”¯æŒçš„å®ä½“ç±»å‹åŠç¤ºä¾‹ï¼š{categorized_examples}

ğŸ”— å…³ç³»æ ‡ç­¾è¯´æ˜ï¼š
å…³ç³»æ ‡ç­¾ç”¨äºæ ‡æ³¨å®ä½“ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œé€šå¸¸æ˜¯åŠ¨è¯çŸ­è¯­æˆ–è¿æ¥è¯ã€‚è¿™äº›å…³ç³»è¯åŒæ ·ä½œä¸ºå®ä½“è¿›è¡Œæ ‡æ³¨ï¼š

ğŸ’¡ å…³ç³»æ ‡ç­¾æ ‡æ³¨åŸåˆ™ï¼š
1. æ ‡æ³¨å®Œæ•´çš„å…³ç³»è¡¨è¾¾ï¼Œè€Œä¸æ˜¯å•ä¸ªè¯æ±‡
2. åŒ…å«å…³ç³»åŠ¨è¯åŠå…¶å‰åçš„ç›¸å…³æˆåˆ†
3. å…³ç³»æ ‡ç­¾é€šå¸¸è¿æ¥ä¸¤ä¸ªæˆ–å¤šä¸ªå…¶ä»–å®ä½“
4. ç¤ºä¾‹ï¼š
   - "æ ¹æ®ã€Šé˜²æ´ªæ³•ã€‹ç¬¬åæ¡è§„å®š" â†’ "æ ¹æ®...è§„å®š"æ ‡æ³¨ä¸º"ä¾æ®å…³ç³»"
   - "æ°´åˆ©éƒ¨è´Ÿè´£å…¨å›½é˜²æ±›å·¥ä½œ" â†’ "è´Ÿè´£"æ ‡æ³¨ä¸º"è´£ä»»å…³ç³»"
   - "æ´ªæ°´å¯¼è‡´å†œç”°å—æŸ" â†’ "å¯¼è‡´"æ ‡æ³¨ä¸º"å› æœå…³ç³»"
   - "å„éƒ¨é—¨åè°ƒé…åˆæŠ¢é™©æ•‘ç¾" â†’ "åè°ƒé…åˆ"æ ‡æ³¨ä¸º"åè°ƒå…³ç³»"

âš ï¸ ä¸¥æ ¼è¦æ±‚ï¼š
1. è¯†åˆ«æ–‡æœ¬ä¸­çœŸå®å­˜åœ¨çš„æ‰€æœ‰å®ä½“ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿå®ä½“å’Œå…³ç³»
2. å‡†ç¡®æ ‡æ³¨å®ä½“çš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼ˆåŸºäºå­—ç¬¦ä½ç½®ï¼‰
3. æ ‡ç­¾åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä¸‹é¢åˆ—å‡ºçš„æ ‡ç­¾åç§°ï¼Œä¸€å­—ä¸å·®
4. ç¦æ­¢ä½¿ç”¨æè¿°æ€§æ–‡å­—ã€ç®€åŒ–åç§°æˆ–ä»»ä½•å˜ä½“å½¢å¼
5. å¦‚æœå®ä½“ç±»å‹ä¸åœ¨æ ‡ç­¾åˆ—è¡¨ä¸­ï¼Œåˆ™ä¸è¦æ ‡æ³¨è¯¥å®ä½“
6. å…³ç³»æ ‡ç­¾è¦åŒ…å«å®Œæ•´çš„å…³ç³»è¡¨è¾¾ï¼Œä¸è¦åªæ ‡æ³¨å•ä¸ªåŠ¨è¯

ğŸ” ç‰¹åˆ«å…³æ³¨ï¼š
- æ³•å¾‹æ¡æ¬¾ï¼šè¯†åˆ«"ç¬¬Xæ¡"ã€"ç¬¬Xç« "ã€"ç¬¬XèŠ‚"ç­‰æ³•å¾‹æ¡æ¬¾æ ¼å¼
- å…³ç³»è¡¨è¾¾ï¼šè¯†åˆ«è¡¨ç¤ºä¾æ®ã€è´£ä»»ã€ç®¡è¾–ã€å› æœç­‰å…³ç³»çš„åŠ¨è¯çŸ­è¯­
- æ—¶åºå…³ç³»ï¼šè¯†åˆ«è¡¨ç¤ºæ—¶é—´å…ˆåçš„å…³ç³»è¯æ±‡
- å½±å“å…³ç³»ï¼šè¯†åˆ«è¡¨ç¤ºå½±å“ã€æ³¢åŠçš„å…³ç³»è¡¨è¾¾

ğŸ“‹ è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{json_format}

ğŸ·ï¸ ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹æ ‡ç­¾åç§°ï¼ˆå¤åˆ¶ç²˜è´´ï¼Œä¸€å­—ä¸å·®ï¼‰ï¼š
{labels_display}

âŒ ç¦æ­¢äº‹é¡¹ï¼š
- ç¦æ­¢ä½¿ç”¨æè¿°æ€§æ–‡å­—ä½œä¸ºæ ‡ç­¾ï¼ˆå¦‚"æ”¿åºœéƒ¨é—¨"åº”ä½¿ç”¨"æ”¿åºœæœºæ„"ï¼‰
- ç¦æ­¢ä½¿ç”¨ç®€åŒ–å½¢å¼ï¼ˆå¦‚"æ¡æ¬¾"åº”ä½¿ç”¨"æ³•å¾‹æ¡æ¬¾"ï¼‰
- ç¦æ­¢ä½¿ç”¨è¿‘ä¼¼è¯æ±‡ï¼ˆå¦‚"æ³•è§„"åº”ä½¿ç”¨"æ³•å¾‹æ³•è§„"ï¼‰
- ç¦æ­¢è‡ªåˆ›æ ‡ç­¾åç§°
- ç¦æ­¢åªæ ‡æ³¨å…³ç³»åŠ¨è¯çš„å•ä¸ªè¯æ±‡ï¼Œè¦æ ‡æ³¨å®Œæ•´çš„å…³ç³»è¡¨è¾¾

âœ… æ­£ç¡®ç¤ºä¾‹ï¼š
å®ä½“æ ‡ç­¾ï¼š
- æ ‡ç­¾å¿…é¡»æ˜¯ï¼š"æ³•å¾‹æ¡æ¬¾"ï¼Œä¸èƒ½æ˜¯"æ¡æ¬¾"ã€"æ³•æ¡"ã€"æ¡æ–‡"
- æ ‡ç­¾å¿…é¡»æ˜¯ï¼š"æ”¿åºœæœºæ„"ï¼Œä¸èƒ½æ˜¯"æ”¿åºœéƒ¨é—¨"ã€"æœºæ„"
- æ ‡ç­¾å¿…é¡»æ˜¯ï¼š"æ³•å¾‹æ³•è§„"ï¼Œä¸èƒ½æ˜¯"æ³•å¾‹"ã€"æ³•è§„"ã€"æ¡ä¾‹"

å…³ç³»æ ‡ç­¾ï¼š
- "æ ¹æ®ã€Šé˜²æ´ªæ³•ã€‹è§„å®š" â†’ "ä¾æ®å…³ç³»"
- "æ°´åŠ¡å±€è´Ÿè´£æ²³é“ç®¡ç†" â†’ "è´£ä»»å…³ç³»"  
- "æ´ªæ°´é€ æˆæŸå¤±" â†’ "å› æœå…³ç³»"
- "å„éƒ¨é—¨åè°ƒé…åˆ" â†’ "åè°ƒå…³ç³»"
- "æ±›æœŸæœŸé—´æ‰§è¡Œé¢„æ¡ˆ" â†’ "æ‰§è¡Œå…³ç³»"

è¯·ç¡®ä¿æ¯ä¸ªæ ‡ç­¾éƒ½ä»ä¸Šé¢çš„åˆ—è¡¨ä¸­ç²¾ç¡®å¤åˆ¶ï¼Œå…³ç³»æ ‡ç­¾è¦æ ‡æ³¨å®Œæ•´çš„å…³ç³»è¡¨è¾¾ï¼"""
        
        # è°ƒç”¨API
        api_response = self._call_modelscope_api(prompt)
        
        if api_response and api_response.strip():
            return self._format_prediction(api_response, task)
        
        # APIè°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºå“åº”
        print("âŒ APIè°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºå“åº”")
        return None
    
    def _call_modelscope_api(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨é­”å¡”ç¤¾åŒºAPIï¼ˆæ”¯æŒè‡ªåŠ¨æ¨¡å‹åˆ‡æ¢ï¼‰"""
        max_retries_per_model = 2  # æ¯ä¸ªæ¨¡å‹æœ€å¤šé‡è¯•2æ¬¡å†åˆ‡æ¢
        
        for attempt in range(max_retries_per_model):
            # ç¡®ä¿APIè¿æ¥å¯ç”¨
            if not self._ensure_api_connection():
                self._handle_model_failure()
                if self._has_more_models_to_try():
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                return None
            
            try:
                print(f"ğŸ”„ è°ƒç”¨æ¨¡å‹: {self.model_name} (å°è¯• {attempt + 1}/{max_retries_per_model})")
                
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a specialized Named Entity Recognition assistant for legal texts. CRITICAL: You must extract both traditional entities AND relational expressions. Use EXACT label names from the provided list. Never use descriptions, abbreviations, or variations. For relation labels, extract complete phrases that express semantic relationships between entities. Always respond with valid JSON format containing only the specified labels."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.1,
                    top_p=0.9,
                    stream=False
                )
                
                if response.choices and len(response.choices) > 0:
                    content = response.choices[0].message.content
                    if content and content.strip():
                        # APIè°ƒç”¨æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                        self._handle_model_success()
                        return content
                    else:
                        print(f"âš ï¸ æ¨¡å‹ {self.model_name} è¿”å›ç©ºå†…å®¹")
                        # ç©ºå†…å®¹ä¹Ÿç®—å¤±è´¥
                        self._handle_model_failure()
                else:
                    print(f"âš ï¸ æ¨¡å‹ {self.model_name} å“åº”æ ¼å¼å¼‚å¸¸")
                    self._handle_model_failure()
                    
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {self.model_name} APIè°ƒç”¨å¼‚å¸¸: {str(e)[:100]}")
                self._handle_model_failure()
            
            # å¦‚æœå½“å‰æ¨¡å‹å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢
            if self.model_consecutive_failures >= self.max_model_failures:
                if self._has_more_models_to_try():
                    print(f"ğŸ”„ åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹...")
                    break  # è·³å‡ºé‡è¯•å¾ªç¯ï¼Œåˆ‡æ¢æ¨¡å‹
                else:
                    print("âŒ æ‰€æœ‰æ¨¡å‹éƒ½å·²å°è¯•å¤±è´¥")
                    return None
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›None
        return None
    
    def _has_more_models_to_try(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–æ¨¡å‹å¯ä»¥å°è¯•"""
        return len(self.available_models) > 1
        
    def _print_model_statistics(self):
        """æ‰“å°æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ¤– æ¨¡å‹ä½¿ç”¨æƒ…å†µ:")
        print(f"   å½“å‰æ¨¡å‹: {self.model_name}")
        print(f"   å½“å‰å¤±è´¥æ¬¡æ•°: {self.model_consecutive_failures}/{self.max_model_failures}")
        
        if self.model_failure_history:
            print(f"   æ¨¡å‹å¤±è´¥å†å²:")
            for model, failures in self.model_failure_history.items():
                if failures > 0:
                    model_short = model.split('/')[-1] if '/' in model else model
                    print(f"     â€¢ {model_short}: {failures} æ¬¡å¤±è´¥")
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
        print(f"   å¯ç”¨æ¨¡å‹: {len(self.available_models)} ä¸ª")
        for i, model in enumerate(self.available_models):
            status = "ğŸ¯" if i == self.current_model_index else "ğŸ’¤"
            model_short = model.split('/')[-1] if '/' in model else model
            print(f"     {status} {model_short}")
    
    def get_model_status(self) -> Dict:
        """è·å–æ¨¡å‹çŠ¶æ€ä¿¡æ¯ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
        return {
            "current_model": self.model_name,
            "current_model_index": self.current_model_index,
            "consecutive_failures": self.model_consecutive_failures,
            "max_failures": self.max_model_failures,
            "available_models": self.available_models,
            "failure_history": self.model_failure_history.copy()
        }
    
    def _format_prediction(self, api_response: str, task: Dict) -> Dict:
        """æ ¼å¼åŒ–é¢„æµ‹ç»“æœä¸ºLabel Studioæ ¼å¼"""
        
        prediction = {
            "model_version": self.get("model_version"),
            "score": 0.95,
            "result": []
        }
        
        # å°è¯•è§£æNERç»“æœ
        ner_results = self._parse_ner_response(api_response, task)
        if ner_results and len(ner_results) > 0:
            prediction["result"] = ner_results
            prediction["score"] = 0.95
            return prediction
        
        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½•å®ä½“ï¼Œè¿”å›å¤±è´¥ä¿¡æ¯
        prediction["score"] = 0.0
        prediction["result"] = []
        return None
    
    def _parse_ner_response(self, api_response: str, task: Dict) -> Optional[List[Dict]]:
        """è§£æAIè¿”å›çš„å‘½åå®ä½“è¯†åˆ«JSONç»“æœï¼Œå¹¶ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¡¥å……"""
        
        # è·å–åŸå§‹æ–‡æœ¬
        task_data = task.get('data', {})
        original_text = ""
        for key in ['text', 'content', 'prompt', 'question', 'description', 'query']:
            if key in task_data and isinstance(task_data[key], str):
                original_text = task_data[key]
                break
        
        if not original_text:
            return None
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        results = []
        ai_entities = []
        
        # ç¬¬ä¸€æ­¥ï¼šè§£æAIæ¨¡å‹çš„è¯†åˆ«ç»“æœ
        if api_response and api_response.strip():
            ai_entities = self._parse_ai_entities(api_response, original_text)
            if ai_entities:
                results.extend(ai_entities)
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¡¥å……è¯†åˆ«
        regex_entities = self._extract_regex_entities(original_text, ai_entities)
        if regex_entities:
            results.extend(regex_entities)
        
        # ç¬¬ä¸‰æ­¥ï¼šå»é‡å’Œæ’åº
        final_results = self._deduplicate_entities(results)
        
        return final_results if final_results else None
    
    def _parse_ai_entities(self, api_response: str, original_text: str) -> List[Dict]:
        """è§£æAIæ¨¡å‹è¿”å›çš„å®ä½“"""
        ai_results = []
        
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                ner_data = json.loads(api_response.strip())
            except json.JSONDecodeError as e:
                # å°è¯•æå–JSONéƒ¨åˆ†
                import re
                
                # å¤šç§JSONæå–ç­–ç•¥
                patterns = [
                    r'\{[^{}]*"entities"[^{}]*:.*?\}',  # æœ€ä¸¥æ ¼çš„entitiesåŒ¹é…
                    r'\{.*?"entities".*?\}',            # å®½æ¾çš„entitiesåŒ¹é…
                    r'\{.*\}',                          # æœ€å®½æ¾çš„JSONåŒ¹é…
                ]
                
                ner_data = None
                for pattern in patterns:
                    json_match = re.search(pattern, api_response, re.DOTALL)
                    if json_match:
                        try:
                            ner_data = json.loads(json_match.group())
                            break
                        except json.JSONDecodeError:
                            continue
                
                if not ner_data:
                    return ai_results
            
            # æ£€æŸ¥entitieså­—æ®µ
            if 'entities' not in ner_data or not isinstance(ner_data['entities'], list):
                return ai_results
            
            entities = ner_data['entities']
            
            # è½¬æ¢ä¸ºLabel Studioæ ¼å¼
            for entity in entities:
                # éªŒè¯å¿…éœ€å­—æ®µ
                if not all(key in entity for key in ['text', 'start', 'end', 'label']):
                    continue
                
                start = entity['start']
                end = entity['end']
                text = entity['text']
                original_label = entity['label']
                
                # ä¸¥æ ¼éªŒè¯æ ‡ç­¾
                validated_label = validate_label(original_label)
                if not validated_label:
                    continue
                
                # ä½¿ç”¨éªŒè¯é€šè¿‡çš„æ ‡ç­¾
                label = validated_label
                
                # éªŒè¯ä½ç½®ä¿¡æ¯åŸºæœ¬åˆç†æ€§
                if not isinstance(start, int) or not isinstance(end, int) or start < 0:
                    continue
                
                # å…ˆå°è¯•ä¿®æ­£ä½ç½®ï¼Œå†è¿›è¡ŒèŒƒå›´æ£€æŸ¥
                corrected_start, corrected_end, corrected_text = self._correct_entity_position(
                    original_text, text, start, end
                )
                
                # æ£€æŸ¥ä¿®æ­£åçš„ä½ç½®æ˜¯å¦åˆç†
                if corrected_start is None or corrected_end is None or corrected_text is None:
                    continue
                
                # éªŒè¯ä¿®æ­£åçš„ä½ç½®ä¸è¶…å‡ºæ–‡æœ¬é•¿åº¦
                if corrected_end > len(original_text) or corrected_start < 0:
                    continue
                
                if corrected_text:
                    # éªŒè¯ä¿®æ­£åçš„å®ä½“æ˜¯å¦åˆç†
                    if self._is_valid_entity(corrected_text, validated_label):
                        result = {
                            "from_name": "label",
                            "to_name": "text",
                            "type": "labels",
                            "value": {
                                "start": corrected_start,
                                "end": corrected_end,
                                "text": corrected_text,
                                "labels": [label]
                            },
                            "source": "ai"  # æ ‡è®°æ¥æºä¸ºAI
                        }
                        
                        ai_results.append(result)
            
            return ai_results
            
        except Exception:
            return ai_results
    
    def _extract_regex_entities(self, original_text: str, existing_entities: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¡¥å……è¯†åˆ«å®ä½“"""
        regex_results = []
        
        try:
            # è·å–å·²è¯†åˆ«å®ä½“çš„ä½ç½®èŒƒå›´ï¼Œé¿å…é‡å¤
            existing_ranges = set()
            for entity in existing_entities:
                value = entity.get('value', {})
                start = value.get('start', -1)
                end = value.get('end', -1)
                if start >= 0 and end > start:
                    # æ‰©å±•èŒƒå›´ä»¥é¿å…é‡å 
                    for pos in range(max(0, start-1), min(len(original_text), end+1)):
                        existing_ranges.add(pos)
            
            # ä»entity_configè·å–æ­£åˆ™æ¨¡å¼
            from entity_config import get_entity_config
            entity_config = get_entity_config()
            
            # éå†æ‰€æœ‰é…ç½®çš„å®ä½“ç±»å‹
            for label_key, config in entity_config.items():
                if 'patterns' not in config or not config['patterns']:
                    continue
                
                patterns = config['patterns']
                description = config['description']
                
                # å¯¹æ¯ä¸ªæ­£åˆ™æ¨¡å¼è¿›è¡ŒåŒ¹é…
                for pattern in patterns:
                    try:
                        import re
                        matches = re.finditer(pattern, original_text)
                        
                        for match in matches:
                            start = match.start()
                            end = match.end()
                            text = match.group()
                            
                            # æ£€æŸ¥æ˜¯å¦ä¸å·²è¯†åˆ«çš„å®ä½“é‡å 
                            overlapping = any(pos in existing_ranges for pos in range(start, end))
                            if overlapping:
                                continue
                            
                            # éªŒè¯å®ä½“æ˜¯å¦åˆç†
                            if self._is_valid_entity(text, label_key):
                                result = {
                                    "from_name": "label",
                                    "to_name": "text",
                                    "type": "labels",
                                    "value": {
                                        "start": start,
                                        "end": end,
                                        "text": text,
                                        "labels": [label_key]  # ä½¿ç”¨æ ‡ç­¾é”®åè€Œä¸æ˜¯description
                                    },
                                    "source": "regex"  # æ ‡è®°æ¥æºä¸ºæ­£åˆ™
                                }
                                
                                regex_results.append(result)
                                
                                # æ›´æ–°å·²è¯†åˆ«èŒƒå›´
                                for pos in range(start, end):
                                    existing_ranges.add(pos)
                                
                    except re.error:
                        continue
            
            return regex_results
            
        except Exception:
            return regex_results
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """å»é‡å’Œæ’åºå®ä½“"""
        
        # æŒ‰èµ·å§‹ä½ç½®æ’åº
        sorted_entities = sorted(entities, key=lambda x: x.get('value', {}).get('start', 0))
        
        # å»é‡é€»è¾‘ï¼šå¦‚æœä¸¤ä¸ªå®ä½“ä½ç½®é‡å è¶…è¿‡50%ï¼Œä¿ç•™ç½®ä¿¡åº¦é«˜çš„
        deduplicated = []
        
        for current in sorted_entities:
            current_value = current.get('value', {})
            current_start = current_value.get('start', 0)
            current_end = current_value.get('end', 0)
            current_text = current_value.get('text', '')
            current_source = current.get('source', 'unknown')
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æ·»åŠ çš„å®ä½“é‡å 
            should_add = True
            for i, existing in enumerate(deduplicated):
                existing_value = existing.get('value', {})
                existing_start = existing_value.get('start', 0)
                existing_end = existing_value.get('end', 0)
                existing_text = existing_value.get('text', '')
                existing_source = existing.get('source', 'unknown')
                
                # è®¡ç®—é‡å åº¦
                overlap_start = max(current_start, existing_start)
                overlap_end = min(current_end, existing_end)
                
                if overlap_start < overlap_end:  # æœ‰é‡å 
                    overlap_length = overlap_end - overlap_start
                    current_length = current_end - current_start
                    existing_length = existing_end - existing_start
                    
                    # è®¡ç®—é‡å æ¯”ä¾‹ï¼ˆç›¸å¯¹äºè¾ƒçŸ­çš„å®ä½“ï¼‰
                    min_length = min(current_length, existing_length)
                    overlap_ratio = overlap_length / min_length if min_length > 0 else 0
                    
                    if overlap_ratio > 0.5:  # é‡å è¶…è¿‡50%
                        
                        # ä¼˜å…ˆçº§ï¼šAI > æ­£åˆ™ï¼Œé•¿å®ä½“ > çŸ­å®ä½“
                        should_replace = False
                        if current_source == 'ai' and existing_source == 'regex':
                            should_replace = True
                        elif current_source == existing_source and current_length > existing_length:
                            should_replace = True
                        
                        if should_replace:
                            deduplicated[i] = current
                        
                        should_add = False
                        break
            
            if should_add:
                deduplicated.append(current)
        
        # æœ€ç»ˆæŒ‰ä½ç½®æ’åº
        final_results = sorted(deduplicated, key=lambda x: x.get('value', {}).get('start', 0))
        
        # ç§»é™¤sourceæ ‡è®°ï¼ˆLabel Studioä¸éœ€è¦ï¼‰
        for result in final_results:
            result.pop('source', None)
        
        return final_results
    
    def _correct_entity_position(self, original_text: str, entity_text: str, start: int, end: int) -> tuple:
        """ä¿®æ­£å®ä½“ä½ç½®"""
        # é¦–å…ˆæ£€æŸ¥åŸå§‹ä½ç½®æ˜¯å¦æ­£ç¡®
        if start < len(original_text) and end <= len(original_text):
            extracted = original_text[start:end]
            if extracted == entity_text:
                return start, end, entity_text
        
        # æ¸…ç†å®ä½“æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        clean_entity = entity_text.strip()
        if not clean_entity:
            return None, None, None
        
        # åœ¨åŸæ–‡ä¸­æœç´¢å®ä½“æ–‡æœ¬
        try:
            # å°è¯•ç²¾ç¡®åŒ¹é…
            exact_start = original_text.find(clean_entity)
            if exact_start != -1:
                exact_end = exact_start + len(clean_entity)
                return exact_start, exact_end, clean_entity
            
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
            import re
            clean_text_for_search = re.sub(r'[^\w\u4e00-\u9fff]', '', clean_entity)
            if len(clean_text_for_search) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦æ‰è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                for i in range(len(original_text) - len(clean_text_for_search) + 1):
                    slice_text = original_text[i:i + len(clean_text_for_search)]
                    clean_slice = re.sub(r'[^\w\u4e00-\u9fff]', '', slice_text)
                    if clean_slice == clean_text_for_search:
                        return i, i + len(clean_text_for_search), slice_text
            
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
            if len(clean_entity) >= 3:
                core_part = clean_entity[:min(len(clean_entity), 5)]  # å–å‰å‡ ä¸ªå­—ç¬¦ä½œä¸ºæ ¸å¿ƒ
                core_start = original_text.find(core_part)
                if core_start != -1:
                    # å°è¯•æ‰©å±•åŒ¹é…
                    extended_end = min(core_start + len(clean_entity) + 2, len(original_text))
                    extended_text = original_text[core_start:extended_end]
                    return core_start, extended_end, extended_text
            
        except Exception:
            pass
        
        return None, None, None
    
    def _is_valid_entity(self, text: str, label: str) -> bool:
        """ç®€åŒ–çš„å®ä½“éªŒè¯ï¼ˆåŸºç¡€è§„åˆ™éªŒè¯ï¼‰"""
        if not text or len(text.strip()) < 1:
            return False
        
        # å»é™¤é¦–å°¾æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        clean_text = text.strip()
        
        # ä¸èƒ½åªæ˜¯æ ‡ç‚¹ç¬¦å·
        import re
        if re.match(r'^[^\w\u4e00-\u9fff]+$', clean_text):
            return False
        
        # åŸºç¡€é•¿åº¦éªŒè¯
        if len(clean_text) < 1:
            return False
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦æœ‰æ•ˆ
        if label not in ENTITY_LABELS:
            return False
        
        # ç‰¹æ®ŠéªŒè¯ï¼šå…³ç³»æ ‡ç­¾
        if label.endswith("å…³ç³»"):
            # å…³ç³»æ ‡ç­¾åº”è¯¥åŒ…å«åŠ¨è¯æˆ–è¿æ¥è¯
            relation_keywords = ['æ ¹æ®', 'ä¾æ®', 'æŒ‰ç…§', 'è´Ÿè´£', 'ä¸»ç®¡', 'ç®¡è¾–', 'å¯¼è‡´', 'é€ æˆ', 'å¼•èµ·', 
                               'ä¹‹å‰', 'ä¹‹å', 'åŒæ—¶', 'åŒ…æ‹¬', 'åŒ…å«', 'å±äº', 'å½±å“', 'æ³¢åŠ', 'åè°ƒ', 
                               'é…åˆ', 'æ‰§è¡Œ', 'å®æ–½', 'è¡¥å¿', 'èµ”å¿']
            
            if not any(keyword in clean_text for keyword in relation_keywords):
                # å¯¹äºå…³ç³»æ ‡ç­¾ï¼Œæ”¾å®½éªŒè¯ï¼Œåªè¦ä¸æ˜¯çº¯æ ‡ç‚¹å°±æ¥å—
                pass
        
        return True
    
    
    def fit(self, event, data, **kwargs):
        """è®­ç»ƒ/æ›´æ–°æ¨¡å‹"""
        self.set('my_data', 'updated_data')
        self.set('model_version', 'updated_version')
        print(f"âœ… æ¨¡å‹å·²æ›´æ–° ({event})")

