from typing import List, Dict, Optional
import json
import os
import base64
import time
from openai import OpenAI
from label_studio_ml.model import LabelStudioMLBase
from label_studio_ml.response import ModelResponse


# ==================== å¤šæ¨¡æ€å›¾ç‰‡æè¿°é…ç½® ====================
# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
SUPPORTED_IMAGE_FORMATS = ['jpg', 'jpeg', 'png', 'gif', 'webp', 'bmp']

# Label Studio åª’ä½“ç›®å½•é…ç½®
# å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ LABEL_STUDIO_MEDIA_DIR è®¾ç½®
LABEL_STUDIO_MEDIA_DIR = os.getenv('LABEL_STUDIO_MEDIA_DIR', r'C:\Users\Administrator\AppData\Local\label-studio\label-studio\media')

# å›¾ç‰‡æè¿°ä»»åŠ¡é…ç½®
IMAGE_DESCRIPTION_CONFIG = {
    "task_type": "å›¾ç‰‡æè¿°æ–‡æœ¬æ ‡æ³¨",
    "model_type": "å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹", 
    "output_format": "è‡ªç„¶è¯­è¨€æ–‡æœ¬æè¿°",
    "language": "ä¸­æ–‡",
    "max_tokens": 1000,
    "temperature": 0.7,
    "features": [
        "ç‰©ä½“è¯†åˆ«",
        "åœºæ™¯ç†è§£", 
        "é¢œè‰²åˆ†æ",
        "åŠ¨ä½œæè¿°",
        "ç»†èŠ‚è§‚å¯Ÿ"
    ]
}

# ==================== ğŸŒ å…¨å±€çŠ¶æ€ç®¡ç† - API Keyå’Œæ¨¡å‹åˆ‡æ¢ ====================
# ä½¿ç”¨å…¨å±€å˜é‡ç»Ÿä¸€ç®¡ç†å½“å‰çŠ¶æ€ï¼Œé¿å…å¤æ‚çš„åˆ‡æ¢é€»è¾‘
api_key_list = [
    "ms-d200fd06-f07f-4be8-a6a8-9ebf76dd103a",  # åŸå§‹é»˜è®¤Key
    "ms-758c9c64-2498-467c-a0de-8b32a1370bc1",
    "ms-376c277c-8f18-4c42-9ba9-c4b0911fa9b0",
    "ms-78247b29-fd23-4ef9-a86a-0e792da83f3e",
    "ms-89acac3e-5ed3-4c06-ad67-8941aef812d1",
    'ms-b980f2d1-86e3-43bc-a72e-30c6849b3148',
    'ms-6a7bc978-f320-48bc-aa67-f9c2e6c9d5c6',
    'ms-7fa00741-856a-4134-80d2-f296b15c0e76',
    'ms-ca41cec5-48ca-4a9e-9fdf-ac348a638d11',
]

# ğŸ”‘ å…¨å±€API KeyçŠ¶æ€
GLOBAL_API_KEY_INDEX = 0
GLOBAL_CURRENT_API_KEY = api_key_list[GLOBAL_API_KEY_INDEX]

# ğŸ¤– å…¨å±€æ¨¡å‹çŠ¶æ€ - å¤šæ¨¡æ€æ¨¡å‹åˆ—è¡¨
available_models_global = [ 
"Qwen/Qwen2.5-VL-72B-Instruct",
"stepfun-ai/step3",
]

GLOBAL_MODEL_INDEX = 0
GLOBAL_CURRENT_MODEL = available_models_global[GLOBAL_MODEL_INDEX]

# ğŸ”„ ç®€åŒ–çš„åˆ‡æ¢å‡½æ•°
def switch_to_next_api_key():
    """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªAPI Key"""
    global GLOBAL_API_KEY_INDEX, GLOBAL_CURRENT_API_KEY, GLOBAL_MODEL_INDEX, GLOBAL_CURRENT_MODEL
    
    old_api_key = GLOBAL_CURRENT_API_KEY
    GLOBAL_API_KEY_INDEX = (GLOBAL_API_KEY_INDEX + 1) % len(api_key_list)
    GLOBAL_CURRENT_API_KEY = api_key_list[GLOBAL_API_KEY_INDEX]
    
    # åˆ‡æ¢API Keyæ—¶é‡ç½®åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹
    GLOBAL_MODEL_INDEX = 0
    GLOBAL_CURRENT_MODEL = available_models_global[GLOBAL_MODEL_INDEX]
    
    print(f"ğŸ”‘ API Keyåˆ‡æ¢: ***{old_api_key[-8:]} â†’ ***{GLOBAL_CURRENT_API_KEY[-8:]}")
    print(f"ğŸ”„ é‡ç½®åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹: {GLOBAL_CURRENT_MODEL}")
    return True

def switch_to_next_model():
    """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹"""
    global GLOBAL_MODEL_INDEX, GLOBAL_CURRENT_MODEL
    
    old_model = GLOBAL_CURRENT_MODEL
    GLOBAL_MODEL_INDEX = (GLOBAL_MODEL_INDEX + 1) % len(available_models_global)
    GLOBAL_CURRENT_MODEL = available_models_global[GLOBAL_MODEL_INDEX]
    
    print(f"ğŸ”„ æ¨¡å‹åˆ‡æ¢: {old_model.split('/')[-1]} â†’ {GLOBAL_CURRENT_MODEL.split('/')[-1]}")
    
    # å¦‚æœå›åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œè¯´æ˜æ‰€æœ‰æ¨¡å‹éƒ½è¯•è¿‡äº†ï¼Œåˆ‡æ¢API Key
    if GLOBAL_MODEL_INDEX == 0:
        print("âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½å·²å°è¯•ï¼Œåˆ‡æ¢API Key")
        switch_to_next_api_key()
        return True
    
    return True

def get_current_api_key():
    """è·å–å½“å‰API Key"""
    return GLOBAL_CURRENT_API_KEY

def get_current_model():
    """è·å–å½“å‰æ¨¡å‹"""
    return GLOBAL_CURRENT_MODEL

def reset_global_state():
    """é‡ç½®å…¨å±€çŠ¶æ€åˆ°åˆå§‹å€¼"""
    global GLOBAL_API_KEY_INDEX, GLOBAL_CURRENT_API_KEY, GLOBAL_MODEL_INDEX, GLOBAL_CURRENT_MODEL
    
    GLOBAL_API_KEY_INDEX = 0
    GLOBAL_CURRENT_API_KEY = api_key_list[0]
    GLOBAL_MODEL_INDEX = 0 
    GLOBAL_CURRENT_MODEL = available_models_global[0]
    
    print(f"ğŸ”„ å…¨å±€çŠ¶æ€å·²é‡ç½®: API Key ***{GLOBAL_CURRENT_API_KEY[-8:]}, æ¨¡å‹ {GLOBAL_CURRENT_MODEL.split('/')[-1]}")
    return True


class NewModel(LabelStudioMLBase):
    """Custom ML Backend model
    """
    
    def setup(self):
        """Configure any parameters of your model here
        """
        print(f"\nğŸš€ å›¾ç‰‡æè¿°ML Backendå¯åŠ¨ä¸­...")
        
        self.set("model_version", "2.0.0-å¤šè´¦å·åˆ‡æ¢ç‰ˆ")
        
        # ğŸŒ ä½¿ç”¨å…¨å±€çŠ¶æ€ç®¡ç† - ç®€åŒ–æ¶æ„
        self.api_base_url = os.getenv('MODELSCOPE_API_URL', 'https://api-inference.modelscope.cn/v1')
        
        # ğŸ¯ ç®€åŒ–çš„å¤±è´¥è®¡æ•°
        self.consecutive_failures = 0  # å½“å‰æ¨¡å‹çš„è¿ç»­å¤±è´¥æ¬¡æ•°
        self.max_failures_before_switch = 2  # è¿ç»­å¤±è´¥2æ¬¡ååˆ‡æ¢
        
        # å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œåªåœ¨éœ€è¦æ—¶è¿æ¥
        self.client = None
        self._api_initialized = False
        
        print("âœ… å¤šæ¨¡æ€å›¾ç‰‡æè¿°MLåç«¯åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ¯ å½“å‰æ¨¡å‹: {get_current_model().split('/')[-1]}")
        print(f"ğŸ”‘ å½“å‰API Key: ***{get_current_api_key()[-8:]}")
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {len(available_models_global)} ä¸ª")
        print(f"ğŸ”‘ å¯ç”¨API Key: {len(api_key_list)} ä¸ª")
        print(f"ğŸ”„ ç®€åŒ–åˆ‡æ¢: å¤±è´¥{self.max_failures_before_switch}æ¬¡åˆ‡æ¢æ¨¡å‹ï¼Œæ‰€æœ‰æ¨¡å‹å¤±è´¥åˆ‡æ¢API Key")
        print(f"â° è¶…æ—¶è®¾ç½®: 250ç§’ï¼ˆç»™å¤§æ¨¡å‹å……è¶³å¤„ç†æ—¶é—´ï¼‰")
        print(f"ğŸ–¼ï¸ ä¸“ä¸šé¢†åŸŸ: å¤šæ¨¡æ€å›¾ç‰‡æè¿° v2.0.0")
        print(f"ğŸš€ ç®€åŒ–ç­–ç•¥: ä½¿ç”¨å…¨å±€çŠ¶æ€ç»Ÿä¸€ç®¡ç†API Keyå’Œæ¨¡å‹åˆ‡æ¢")
    
    def reset_state(self):
        """ğŸ”„ é‡ç½®çŠ¶æ€åˆ°åˆå§‹çŠ¶æ€ï¼ˆä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰"""
        print("ğŸ”„ é‡ç½®çŠ¶æ€åˆ°åˆå§‹çŠ¶æ€...")
        reset_global_state()
        self.consecutive_failures = 0
        
        # é‡ç½®APIè¿æ¥
        self._api_initialized = False
        self.client = None
        
        print(f"âœ… çŠ¶æ€å·²é‡ç½®")
        return True
    
    def _handle_failure(self, reason: str = "æœªçŸ¥é”™è¯¯", force_switch: bool = False):
        """ğŸš¨ ç®€åŒ–çš„å¤±è´¥å¤„ç†é€»è¾‘"""
        self.consecutive_failures += 1
        current_model = get_current_model()
        
        print(f"âŒ æ¨¡å‹å¤±è´¥: {current_model.split('/')[-1]} - {reason} (è¿ç»­: {self.consecutive_failures}/{self.max_failures_before_switch})")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡æ¢
        should_switch = force_switch or (self.consecutive_failures >= self.max_failures_before_switch)
        
        if should_switch:
            print(f"ğŸ”„ {'å¼ºåˆ¶' if force_switch else 'è¾¾åˆ°é˜ˆå€¼'}åˆ‡æ¢æ¨¡å‹")
            switch_to_next_model()  # ä½¿ç”¨å…¨å±€å‡½æ•°åˆ‡æ¢
            self.consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
            
            # é‡ç½®APIè¿æ¥
            self._api_initialized = False
            self.client = None
            return True
        
        return False
    
    def _handle_success(self):
        """âœ… å¤„ç†æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°"""
        if self.consecutive_failures > 0:
            print(f"âœ… æ¨¡å‹æ¢å¤æ­£å¸¸")
            self.consecutive_failures = 0
    
    def _should_switch_immediately(self, error_str: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç«‹å³åˆ‡æ¢æ¨¡å‹ï¼ˆä¸é‡è¯•ï¼‰"""
        immediate_switch_patterns = [
            # APIé™æµé”™è¯¯ - ç«‹å³åˆ‡æ¢
            "429",
            "Too Many Requests", 
            "Request limit exceeded",
            "Rate limit exceeded",
            "Quota exceeded",
            "è¯·æ±‚è¶…é™",  # ä¸­æ–‡é”™è¯¯ä¿¡æ¯
            "è¯·æ±‚é™åˆ¶",
            "è¶…å‡ºé™åˆ¶",
            "è¾¾åˆ°é™åˆ¶",
            "Exceeded limit",
            "API rate limit",
            "Usage limit exceeded",
            "Daily limit exceeded",
            "Monthly limit exceeded",
            
            # è®¤è¯/æƒé™é”™è¯¯ - ç«‹å³åˆ‡æ¢
            "401", 
            "403",
            "Unauthorized",
            "Forbidden",
            "Invalid API key",
            "API key expired",
            
            # æ¨¡å‹ä¸å¯ç”¨é”™è¯¯ - ç«‹å³åˆ‡æ¢
            "404",
            "Model not found",
            "Model unavailable",
            "Service unavailable",
            "Model is overloaded",
            
            # æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ - ç«‹å³åˆ‡æ¢
            "500",
            "502", 
            "503",
            "504",
            "Internal server error",
            "Bad gateway",
            "Gateway timeout",
            
            # å‚æ•°é”™è¯¯ - ç«‹å³åˆ‡æ¢ï¼ˆæ¨¡å‹ä¸æ”¯æŒå½“å‰å‚æ•°ï¼‰
            "Invalid model",
            "Unsupported model",
            "Model does not exist",
            
            # ä¸¥é‡è¶…æ—¶é”™è¯¯ - ç«‹å³åˆ‡æ¢ï¼ˆé¿å…é•¿æ—¶é—´ç­‰å¾…ï¼‰
            "Connection timeout",  # è¿æ¥å±‚é¢çš„è¶…æ—¶
            "Read timeout",        # è¯»å–è¶…æ—¶
            "Gateway timeout"      # ç½‘å…³è¶…æ—¶
        ]
        
        error_lower = error_str.lower()
        for pattern in immediate_switch_patterns:
            if pattern.lower() in error_lower:
                return True
        
        return False
    
    def _get_error_type(self, error_str: str) -> str:
        """è·å–é”™è¯¯ç±»å‹æè¿°"""
        error_lower = error_str.lower()
        
        # ğŸš¨ é«˜ä¼˜å…ˆçº§é”™è¯¯ï¼ˆç«‹å³åˆ‡æ¢ï¼‰
        if any(x in error_lower for x in ["429", "too many requests", "rate limit", "quota exceeded", "request limit exceeded", "è¯·æ±‚è¶…é™", "è¯·æ±‚é™åˆ¶", "è¶…å‡ºé™åˆ¶", "è¾¾åˆ°é™åˆ¶", "exceeded limit", "usage limit", "daily limit", "monthly limit"]):
            return "APIé™æµ"
        elif any(x in error_lower for x in ["401", "403", "unauthorized", "forbidden", "api key"]):
            return "è®¤è¯å¤±è´¥"
        elif any(x in error_lower for x in ["404", "model not found", "model unavailable"]):
            return "æ¨¡å‹ä¸å­˜åœ¨"
        elif any(x in error_lower for x in ["500", "502", "503", "504", "internal server"]):
            return "æœåŠ¡å™¨é”™è¯¯"
        elif any(x in error_lower for x in ["invalid model", "unsupported model"]):
            return "æ¨¡å‹ä¸æ”¯æŒ"
        elif any(x in error_lower for x in ["connection timeout", "read timeout", "gateway timeout"]):
            return "ç½‘ç»œè¶…æ—¶"
        elif any(x in error_lower for x in ["timeout", "timed out"]):
            return "å¤„ç†è¶…æ—¶"
        
        # ğŸ”„ ä¸€èˆ¬é”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰  
        elif any(x in error_lower for x in ["connection", "network"]):
            return "ç½‘ç»œè¿æ¥"
        elif any(x in error_lower for x in ["json", "parse", "format"]):
            return "æ ¼å¼é”™è¯¯"
        elif "empty" in error_lower or "ç©º" in error_str:
            return "ç©ºå“åº”"
        else:
            return "æœªçŸ¥é”™è¯¯"
        
    def _ensure_api_connection(self):
        """ğŸ”Œ ç¡®ä¿APIè¿æ¥å·²åˆå§‹åŒ–ï¼ˆä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰"""
        if self._api_initialized and self.client:
            return True
        
        current_api_key = get_current_api_key()
        current_model = get_current_model()
        
        try:
            print(f"ğŸ”„ è¿æ¥API... (æ¨¡å‹: {current_model.split('/')[-1]}, Key: ***{current_api_key[-8:]})")
            self.client = OpenAI(
                base_url=self.api_base_url,
                api_key=current_api_key,
                max_retries=0,  # ç¦ç”¨å†…ç½®é‡è¯•
                timeout=250.0   # 250ç§’è¶…æ—¶
            )
            
            # ç®€å•æµ‹è¯•è¿æ¥
            response = self.client.chat.completions.create(
                model=current_model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5,
                temperature=0.1,
                timeout=250
            )
            
            self._api_initialized = True
            print(f"âœ… APIè¿æ¥æˆåŠŸ")
            return True
            
        except Exception as e:
            error_str = str(e)
            print(f"âŒ APIè¿æ¥å¤±è´¥: {error_str[:100]}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³åˆ‡æ¢
            if self._should_switch_immediately(error_str):
                error_type = self._get_error_type(error_str)
                print(f"ğŸ”„ æ£€æµ‹åˆ°éœ€è¦ç«‹å³åˆ‡æ¢çš„é”™è¯¯: {error_type}")
                self._handle_failure(f"è¿æ¥-{error_type}", force_switch=True)
            else:
                self._handle_failure(f"è¿æ¥-{self._get_error_type(error_str)}")
            
            # é‡ç½®è¿æ¥çŠ¶æ€
            self.client = None
            self._api_initialized = False
            return False
    
    def _convert_local_path_to_base64(self, file_path: str) -> Optional[str]:
        """å°†æœ¬åœ°æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºbase64æ ¼å¼çš„æ•°æ®URL"""
        
        # è·å–ç›®å½•ä¿¡æ¯
        current_dir = os.getcwd()
        parent_dir = os.path.dirname(current_dir)
        grandparent_dir = os.path.dirname(parent_dir)
        
        # å°è¯•è·¯å¾„è§£æ - ä¸“æ³¨äºLabel Studioåª’ä½“ç›®å½•
        possible_paths = []
        
        # 1. Label Studio å®é™…åª’ä½“ç›®å½• (Windows) - ä¸»è¦è·¯å¾„
        label_studio_media_dir = r'C:\Users\Administrator\AppData\Local\label-studio\label-studio\media'
        
        if os.path.exists(label_studio_media_dir):
            # å¤„ç†è·¯å¾„:ç§»é™¤å¼€å¤´çš„æ–œæ ï¼Œç›´æ¥ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            relative_path = file_path.lstrip('/')
            possible_paths.append(os.path.join(label_studio_media_dir, relative_path))
        else:
            print(f"   âŒ Label Studioåª’ä½“ç›®å½•ä¸å­˜åœ¨: {label_studio_media_dir}")
        
        # 2. å¤‡ç”¨è·¯å¾„ (ä»…å½“ä¸»è·¯å¾„ä¸å­˜åœ¨æ—¶)
        backup_media_dirs = [
            os.path.expanduser(r'~\AppData\Local\label-studio\label-studio\media'),
            r'C:\Users\%USERNAME%\AppData\Local\label-studio\label-studio\media',
        ]
        
        for backup_dir in backup_media_dirs:
            if os.path.exists(backup_dir):
                relative_path = file_path.lstrip('/')
                possible_paths.append(os.path.join(backup_dir, relative_path))
        
        # 3. é…ç½®çš„åª’ä½“ç›®å½• (å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡)
        if LABEL_STUDIO_MEDIA_DIR and os.path.exists(LABEL_STUDIO_MEDIA_DIR):
            relative_path = file_path.lstrip('/')
            possible_paths.append(os.path.join(LABEL_STUDIO_MEDIA_DIR, relative_path))
        
        # 4. åŸå§‹è·¯å¾„ (æœ€åå¤‡ç”¨)
        # åˆ é™¤è·¯å¾„ä¸­å¼€å§‹çš„/data/æ–‡ä»¶å¤¹
        file_path = file_path.replace('/data/', '')
        possible_paths.append(file_path)
        
        # æµ‹è¯•æ¯ä¸ªå¯èƒ½çš„è·¯å¾„
        for i, test_path in enumerate(possible_paths):
            test_path = test_path.replace('\data', '')
            
            if os.path.exists(test_path):
                file_path = test_path
                break
        else:
            print(f"\nâŒ æœªæ‰¾åˆ°Label Studioåª’ä½“æ–‡ä»¶!")
            return self._create_config_guidance_message()
        
        try:
            # è·å–æ–‡ä»¶æ‰©å±•åæ¥ç¡®å®šMIMEç±»å‹
            _, ext = os.path.splitext(file_path)
            ext = ext.lower().lstrip('.')
            
            mime_type_map = {
                'jpg': 'image/jpeg',
                'jpeg': 'image/jpeg', 
                'png': 'image/png',
                'gif': 'image/gif',
                'webp': 'image/webp',
                'bmp': 'image/bmp'
            }
            
            mime_type = mime_type_map.get(ext, 'image/jpeg')
            
            # è¯»å–æ–‡ä»¶å¹¶è½¬æ¢ä¸ºbase64
            with open(file_path, 'rb') as image_file:
                image_data = image_file.read()
                base64_data = base64.b64encode(image_data).decode('utf-8')
                
            # æ„å»ºdata URL
            data_url = f"data:{mime_type};base64,{base64_data}"
            
            return data_url
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return None
    
    def _create_config_guidance_message(self) -> str:
        """åˆ›å»ºé…ç½®æŒ‡å¼•æ¶ˆæ¯(å½“æ–‡ä»¶æœªæ‰¾åˆ°æ—¶çš„fallback)"""
        return """âš ï¸ é…ç½®é—®é¢˜:æ— æ³•è®¿é—®ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶

ğŸ”§ è§£å†³æ–¹æ¡ˆ:

1ï¸âƒ£ æ£€æŸ¥Label Studioé…ç½®
   - ç¡®ä¿å¯ç”¨äº†æœ¬åœ°æ–‡ä»¶æœåŠ¡
   - è®¾ç½®æ­£ç¡®çš„æ–‡ä»¶æ ¹ç›®å½•

2ï¸âƒ£ æ£€æŸ¥æ–‡ä»¶è·¯å¾„
   - ç¡®ä¿æ–‡ä»¶å·²æ­£ç¡®ä¸Šä¼ åˆ°Label Studio
   - æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æ­£ç¡®çš„åª’ä½“ç›®å½•ä¸­

3ï¸âƒ£ ä½¿ç”¨Base64ä¸Šä¼ 
   - ç›´æ¥ä¸Šä¼ base64ç¼–ç çš„å›¾ç‰‡
   - é¿å…æ–‡ä»¶è·¯å¾„ä¾èµ–é—®é¢˜

4ï¸âƒ£ é…ç½®ç¯å¢ƒå˜é‡
   - LABEL_STUDIO_MEDIA_DIR=your_media_path
   - é‡å¯ML BackendæœåŠ¡

è¯·è”ç³»ç®¡ç†å‘˜é…ç½®æ–‡ä»¶æœåŠ¡åé‡è¯•ã€‚"""
    
    def _format_config_guidance_prediction(self, guidance_message: str, task: Dict) -> Dict:
        """æ ¼å¼åŒ–é…ç½®æŒ‡å¼•æ¶ˆæ¯ä¸ºLabel Studioé¢„æµ‹æ ¼å¼"""
        
        # åŠ¨æ€è·å–å­—æ®µå
        from_name, to_name = self._get_field_names()
        
        prediction = {
            "model_version": self.get("model_version"),
            "score": 0.1,  # ä½åˆ†è¡¨ç¤ºè¿™æ˜¯é…ç½®é—®é¢˜
            "result": [{
                "from_name": from_name,
                "to_name": to_name, 
                "type": "textarea",
                "value": {
                    "text": [guidance_message]
                }
            }]
        }
        
        return prediction


    def predict(self, tasks: List[Dict], context: Optional[Dict] = None, **kwargs) -> ModelResponse:
        """ å›¾ç‰‡æè¿°æ–‡æœ¬æ ‡æ³¨é¢„æµ‹
            :param tasks: Label Studio tasks in JSON format (åŒ…å«å›¾ç‰‡æ•°æ®)
            :param context: Label Studio context in JSON format
            :return: ModelResponse with predictions (å›¾ç‰‡æè¿°æ–‡æœ¬)
        """
        
        predictions = []
        
        for i, task in enumerate(tasks):
            try:
                prediction = self._process_single_task(task)
                if prediction:
                    predictions.append(prediction)
                else:
                    predictions.append({
                        "model_version": self.get("model_version"),
                        "score": 0.0,
                        "result": []
                    })
            except Exception as e:
                print(f"âŒ ä»»åŠ¡ {i+1} å¤„ç†å¤±è´¥: {e}")
                predictions.append({
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": []
                })
        
        # è¾“å‡ºæœ€ç»ˆè¿”å›çš„JSONç»“æœ
        print("\n" + "="*60)
        print("ğŸ“¤ æœ€ç»ˆè¿”å›çš„JSONç»“æœ:")
        print("="*60)
        for i, prediction in enumerate(predictions):
            print(f"\n--- é¢„æµ‹ç»“æœ {i+1} ---")
            print(json.dumps(prediction, indent=2, ensure_ascii=False))
        print("\n" + "="*60)
        
        return ModelResponse(predictions=predictions)
    
    def _process_single_task(self, task: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªå›¾ç‰‡æè¿°ä»»åŠ¡"""
        
        task_data = task.get('data', {})
        
        # æå–å›¾ç‰‡å†…å®¹
        image_url = None
        image_data = None
        custom_prompt = ""
        
        # æŸ¥æ‰¾å›¾ç‰‡URL
        for key, value in task_data.items():
            if isinstance(value, str):
                # ä¼˜å…ˆæ£€æŸ¥captioningå­—æ®µ(æ‚¨çš„æ¨¡æ¿ä¸­çš„å›¾ç‰‡å­—æ®µ)
                if key in ['captioning', 'image', 'img', 'photo', 'picture', 'url']:
                    image_url = value
                    break
                elif value.startswith(('http://', 'https://', 'data:image/')):
                    image_url = value
                    break
                elif key in ['text', 'prompt', 'question', 'description']:
                    custom_prompt = value
        
        if not image_url:
            return None
        
        # å¤„ç†å›¾ç‰‡æ•°æ®
        if image_url.startswith('data:image/'):
            # Base64ç¼–ç çš„å›¾ç‰‡
            image_data = image_url
            
        elif image_url.startswith(('http://', 'https://')):
            # ç½‘ç»œURLå›¾ç‰‡
            image_data = image_url
            
        else:
            # æœ¬åœ°æ–‡ä»¶è·¯å¾„
            # è½¬æ¢æœ¬åœ°æ–‡ä»¶ä¸ºbase64
            image_data = self._convert_local_path_to_base64(image_url)
            
            if not image_data:
                return None
            
            # æ£€æŸ¥æ˜¯å¦è¿”å›çš„æ˜¯é…ç½®æŒ‡å¼•æ¶ˆæ¯
            if image_data.startswith("âš ï¸ é…ç½®é—®é¢˜"):
                return self._format_config_guidance_prediction(image_data, task)
        
        # æ„å»ºå›¾ç‰‡æè¿°æç¤ºè¯
        if custom_prompt:
            prompt = f"è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚æè¿°è¿™å¼ å›¾ç‰‡:{custom_prompt}"
        else:
            prompt = """ä½ æ˜¯æ´ªæ¶ç¾å®³åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹å›¾ç‰‡è¿›è¡Œåˆ†æï¼Œå¹¶æŒ‰ç…§ç”Ÿäº§çº§æ ‡å‡†è¾“å‡ºã€‚

è¦æ±‚:

1. **è‡ªä¸»è§†è§‰æ€ç»´é“¾(Visual Chain-of-Thought)**:
   - ä½¿ç”¨åˆ†æ­¥åˆ—è¡¨å½¢å¼ï¼Œæ¯ä¸€æ­¥åŒ…å«ä»¥ä¸‹å­—æ®µ:
     1. **reasoning_level**:æ¨ç†å±‚æ¬¡ï¼Œå¯é€‰å€¼:
        - "perception"(æ„ŸçŸ¥å±‚ï¼Œç›´æ¥ä»å›¾ç‰‡è·å–ä¿¡æ¯)  
        - "relationship"(å…³ç³»æ¨ç†å±‚ï¼Œåˆ†æå¯¹è±¡æˆ–å› ç´ é—´å…³ç³»)  
        - "semantic"(è¯­ä¹‰/å› æœæ¨ç†å±‚ï¼Œåˆ¤æ–­ç¾å®³ç­‰çº§ã€åŸå› å’Œæ½œåœ¨å½±å“)
     2. **reasoning / Why**:ä¸ºä»€ä¹ˆåšè¿™æ­¥ï¼Œè¯´æ˜è§‚å¯Ÿæˆ–æ¨ç†ç›®çš„ã€‚
     3. **observation / How**:æ€ä¹ˆåšï¼Œè¯´æ˜å…·ä½“è§‚å¯Ÿæˆ–åˆ†ææ–¹æ³•ã€‚
     4. **expected_outcome / What to obtain**:å¸Œæœ›é€šè¿‡è¿™æ­¥è·å¾—çš„ä¿¡æ¯æˆ–ç»“æœã€‚
     5. **inference / Conclusion**:æ ¹æ®è§‚å¯Ÿå’Œåˆ†æå¾—å‡ºçš„ç»“è®ºã€‚
     6. **step_type**(å¯é€‰):æ­¥éª¤ç±»å‹ï¼Œä¾‹å¦‚ "observation", "inference", "cause_analysis", "impact_estimation"ã€‚
     7. **confidence**(å¯é€‰):åˆ†æå¯ä¿¡åº¦ï¼Œä¾‹å¦‚ "é«˜", "ä¸­", "ä½"ã€‚
     8. **time_reference**(å¯é€‰):å½“å‰è§‚å¯Ÿ/è¿‡å»/é¢„æµ‹ã€‚
     9. **mapped_field**(å¯é€‰):å¯¹åº”ç»“æ„åŒ–å­—æ®µã€‚
   - æ­¥éª¤æŒ‰é€»è¾‘é¡ºåºç¼–å·ï¼Œç¬¬1æ­¥ã€ç¬¬2æ­¥ã€ç¬¬3æ­¥â€¦â€¦ã€‚
   - æ¨¡å‹è‡ªä¸»æ¨ç†ï¼Œä¸éœ€è¦æå‰æä¾›åˆ†ææ­¥éª¤ã€‚
   - ç¤ºä¾‹æ ¼å¼:
[
  {
    "step": 1,
    "reasoning_level": "perception",
    "reasoning": "éœ€è¦äº†è§£æ´ªæ°´ä¸¥é‡ç¨‹åº¦ï¼Œåˆ¤æ–­å±…æ°‘é£é™©ã€‚",
    "observation": "è§‚å¯Ÿåˆ°è¡—é“ç§¯æ°´ï¼Œæ°´æ·±åŠè†ï¼Œå¤šæ ‹å»ºç­‘åº•å±‚è¢«æ·¹ã€‚",
    "expected_outcome": "è·å–å—ç¾åŒºåŸŸåŠå½±å“èŒƒå›´ã€‚",
    "inference": "ä½æ´¼è¡—åŒºå—æ´ªæ°´ç›´æ¥å½±å“ï¼Œå±…æ°‘ç”Ÿæ´»å—é˜»ã€‚",
    "step_type": "observation",
    "confidence": "é«˜",
    "time_reference": "å½“å‰",
    "mapped_field": "affected_area"
  },
  {
    "step": 2,
    "reasoning_level": "relationship",
    "reasoning": "åˆ†æå»ºç­‘å—ç¾ä¸åœ°åŠ¿å…³ç³»ï¼Œåˆ¤æ–­æ´ªæ°´æ‰©æ•£è·¯å¾„ã€‚",
    "observation": "æ°´ä½é«˜çš„è¡—é“é‚»è¿‘ä½æ´¼å»ºç­‘ï¼Œéƒ¨åˆ†é“è·¯é˜»å¡ã€‚",
    "expected_outcome": "ç†è§£æ´ªæ°´ä¼ æ’­åŠå—ç¾é“¾æ¡ã€‚",
    "inference": "æ´ªæ°´ä¸»è¦å½±å“ä½æ´¼åŒºåŸŸï¼Œäº¤é€šå—é˜»ã€‚",
    "step_type": "impact_estimation",
    "confidence": "ä¸­",
    "time_reference": "å½“å‰",
    "mapped_field": "affected_area"
  },
  {
    "step": 3,
    "reasoning_level": "semantic",
    "reasoning": "åˆ¤æ–­ç¾å®³ç­‰çº§ã€åŸå› åŠæ½œåœ¨å½±å“ã€‚",
    "observation": "è¿ç»­å¼ºé™é›¨ï¼Œæ’æ°´ä¸ç•…ï¼Œä½æ´¼å»ºç­‘æ·¹æ°´ã€‚",
    "expected_outcome": "ç¡®å®šç¾å®³ç±»å‹ã€ç­‰çº§åŠåº”å¯¹æªæ–½ã€‚",
    "inference": "è¯¥åŒºåŸŸä¸­åº¦è‡³é‡åº¦æ´ªæ°´ï¼Œå±…æ°‘éœ€æ’¤ç¦»ï¼Œç»æµæŸå¤±å¯èƒ½å‘ç”Ÿã€‚",
    "step_type": "cause_analysis",
    "confidence": "ä¸­",
    "time_reference": "å½“å‰",
    "mapped_field": "disaster_level"
  }
]

2. **ç»“æ„åŒ–æ€»ç»“(Structured Summary)**:
   - æ ¸å¿ƒç»´åº¦(è§‚å¯Ÿåˆ°å°±å¡«ï¼Œæ— æ³•è§‚å¯Ÿæ ‡è®°â€œæœªè§‚å¯Ÿåˆ°â€):
     - disaster_type(ç¾å®³ç±»å‹)
     - affected_environment(æ‰¿ç¾ç¯å¢ƒ)
     - affected_area(å—ç¾èŒƒå›´)
     - disaster_level(ç¾å®³ç­‰çº§)
     - disaster_time(ç¾å®³æ—¶é—´)
     - disaster_location(ç¾å®³åœ°ç‚¹)
     - disaster_cause(ç¾å®³åŸå› )
     - disaster_consequence(ç¾å®³åæœ)
     - disaster_impact(ç¾å®³å½±å“)
     - response_measures(ç¾å®³åº”å¯¹æªæ–½)
     - other_details(å…¶ä»–å€¼å¾—æ³¨æ„çš„ç»†èŠ‚)
   - å¯é€‰æ‰©å±•ç»´åº¦(è§‚å¯Ÿåˆ°å°±å¡«ï¼Œæ— æ³•è§‚å¯Ÿæ ‡è®°â€œæœªè§‚å¯Ÿåˆ°â€):
     - hydrological_features(æ°´æ·±ã€æ°´æµé€Ÿåº¦ç­‰)
     - affected_population
     - infrastructure_damage
     - environmental_factors
     - warning_signals
     - socioeconomic_impact
     - disaster_trend
     - recoverability
     - anomalies_or_unusual_observations
     - weather_conditions

3. **æ€»ä½“æ–‡æœ¬æè¿°(overall_text_description)**:
   - ç»¼åˆè§†è§‰æ€ç»´é“¾å’Œç»“æ„åŒ–ä¿¡æ¯ç”Ÿæˆè‡ªç„¶è¯­è¨€æ€»ç»“ã€‚
   - è¯­è¨€ç®€æ˜ã€å®¢è§‚ã€ä¸“ä¸šï¼Œå¯ç›´æ¥ç”¨äºæŠ¥å‘Šã€ç›‘æµ‹æˆ–æ–°é—»ç¨¿ã€‚

4. **è¾“å‡ºè¦æ±‚**:
   - JSON æ ¼å¼ï¼ŒåŒ…å«å››éƒ¨åˆ†:
     1. "image_id":å›¾ç‰‡åç§°æˆ–ID
     2. "visual_cot":åˆ†æ­¥äº”ç»´æ€ç»´é“¾
     3. "structured_description":ç»“æ„åŒ–å­—æ®µ
     4. "overall_text_description":è‡ªç„¶è¯­è¨€æ€»ç»“
   - æ ¸å¿ƒç»´åº¦å’Œå¯é€‰æ‰©å±•ç»´åº¦ç»Ÿä¸€é‡‡ç”¨â€œè§‚å¯Ÿåˆ°å°±å¡«ï¼Œæœªè§‚å¯Ÿæ ‡è®°â€˜æœªè§‚å¯Ÿåˆ°â€™â€æ–¹å¼ã€‚"""
        
        # è°ƒç”¨å¤šæ¨¡æ€APIï¼ˆä½¿ç”¨æ™ºèƒ½åˆ‡æ¢ç‰ˆæœ¬ï¼‰
        api_response = self._call_multimodal_api_with_switching(prompt, image_data)
        
        if api_response:
            return self._format_description_prediction(api_response, task)
        else:
            return None
    
    def _call_multimodal_api_with_switching(self, prompt: str, image_data: str) -> Optional[str]:
        """ğŸš€ æ™ºèƒ½åˆ‡æ¢ç‰ˆæœ¬çš„å¤šæ¨¡æ€APIè°ƒç”¨ï¼ˆä½¿ç”¨å…¨å±€çŠ¶æ€ç®¡ç†ï¼‰"""
        max_total_attempts = len(available_models_global) * 2  # æ€»å…±å°è¯•æ¬¡æ•°
        
        for attempt in range(max_total_attempts):
            # ç¡®ä¿APIè¿æ¥å¯ç”¨
            if not self._ensure_api_connection():
                continue  # å·²ç»åœ¨è¿æ¥æ—¶å¤„ç†äº†åˆ‡æ¢ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•
            
            current_model = get_current_model()
            try:
                print(f"ğŸ”„ è°ƒç”¨å¤šæ¨¡æ€API (å°è¯• {attempt + 1}/{max_total_attempts})")
                print(f"   ğŸ“¡ æ¨¡å‹: {current_model.split('/')[-1]} | â° è¶…æ—¶: 250s | ğŸ’¾ æœ€å¤§token: 1000")
                
                start_time = time.time()
                
                # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
                system_message = "You are a helpful assistant specialized in image description. Please provide detailed, accurate descriptions in Chinese."
                
                messages = [
                    {
                        "role": "system", 
                        "content": system_message
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": image_data
                                }
                            }
                        ]
                    }
                ]
                
                response = self.client.chat.completions.create(
                    model=current_model,
                    messages=messages,
                    max_tokens=1000,
                    temperature=0.7,
                    top_p=0.9,
                    stream=False,
                        timeout=250
                    )
                
                end_time = time.time()
                api_duration = end_time - start_time
                
                if response.choices and len(response.choices) > 0:
                    choice = response.choices[0]
                    
                    if hasattr(choice, 'message'):
                        message = choice.message
                        content = getattr(message, 'content', None)
                        
                        if content and content.strip():
                            # æˆåŠŸ
                            self._handle_success()
                            print(f"   âœ… æˆåŠŸ (è€—æ—¶: {api_duration:.1f}s, é•¿åº¦: {len(content)})")
                            return content
                        else:
                            print(f"âš ï¸ è¿”å›ç©ºå†…å®¹")
                            self._handle_failure("ç©ºå“åº”")
                    else:
                        print(f"âš ï¸ æ— æ¶ˆæ¯å†…å®¹")
                        self._handle_failure("æ— æ¶ˆæ¯")
                else:
                    print(f"âš ï¸ æ— å“åº”choices")
                    self._handle_failure("æ— å“åº”")
                        
            except Exception as e:
                error_str = str(e)
                print(f"âŒ å¤šæ¨¡æ€APIå¼‚å¸¸: {error_str[:100]}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³åˆ‡æ¢
                if self._should_switch_immediately(error_str):
                    error_type = self._get_error_type(error_str)
                    print(f"ğŸ”„ ç«‹å³åˆ‡æ¢é”™è¯¯: {error_type}")
                    self._handle_failure(f"ç«‹å³åˆ‡æ¢-{error_type}", force_switch=True)
                else:
                    self._handle_failure(f"APIå¼‚å¸¸-{self._get_error_type(error_str)}")
        
        print("âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥")
        return None
    
    def _call_multimodal_api(self, prompt: str, image_data: str) -> Optional[str]:
        """è°ƒç”¨å¤šæ¨¡æ€APIè¿›è¡Œå›¾ç‰‡æè¿°ï¼ˆä¿ç•™åŸæ–¹æ³•ä½œä¸ºå¤‡ç”¨ï¼‰"""
        
        if not self.client:
                return None
                
        try:
            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            system_message = "You are a helpful assistant specialized in image description. Please provide detailed, accurate descriptions in Chinese."
            
            messages = [
                {
                    "role": "system", 
                    "content": system_message
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image_data
                            }
                        }
                    ]
                }
            ]
            
            response = self.client.chat.completions.create(
                model=get_current_model(),
                messages=messages,
                max_tokens=1000,
                temperature=0.7,
                top_p=0.9,
                stream=False
            )
            
            if response.choices and len(response.choices) > 0:
                choice = response.choices[0]
                
                if hasattr(choice, 'message'):
                    message = choice.message
                    content = getattr(message, 'content', None)
                    
                    if content:
                        return content
                
                return None
            
        except Exception as e:
            print(f"âŒ å¤šæ¨¡æ€APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return None
    
    def _format_description_prediction(self, api_response: str, task: Dict) -> Dict:
        """æ ¼å¼åŒ–å›¾ç‰‡æè¿°é¢„æµ‹ç»“æœä¸ºLabel Studioæ ¼å¼"""
        
        # æ„å»ºåŸºç¡€é¢„æµ‹ç»“æ„
        model_version = self.get("model_version")
        
        prediction = {
            "model_version": model_version,
            "score": 0.95,
            "result": []
        }
        
        # åŠ¨æ€è·å–å­—æ®µå
        from_name, to_name = self._get_field_names()
        
        # å¤„ç†APIå“åº”
        if api_response and api_response.strip():
            cleaned_response = self._clean_response_format(api_response.strip())
            
            # æ„å»ºLabel Studioç»“æœæ ¼å¼
            result_item = {
                "from_name": from_name,
                "to_name": to_name,
                "type": "textarea", 
                "value": {
                    "text": [cleaned_response]
                }
            }
            
            prediction["result"].append(result_item)
            
        else:
            default_message = "æ— æ³•ç”Ÿæˆå›¾ç‰‡æè¿°"
            result_item = {
                "from_name": from_name,
                "to_name": to_name, 
                "type": "textarea",
                "value": {
                    "text": [default_message]
                }
            }
            
            prediction["result"].append(result_item)
        
        return prediction
    
    def _clean_response_format(self, response: str) -> str:
        """æ¸…ç†APIå“åº”ä¸­çš„æ ¼å¼æ ‡è®°å¹¶éªŒè¯JSONå®Œæ•´æ€§"""
        import re
        
        # ç§»é™¤```jsonå’Œ```æ ‡è®°
        cleaned = re.sub(r'```json\s*', '', response)
        cleaned = re.sub(r'\s*```', '', cleaned)
        
        # ç§»é™¤å…¶ä»–ä»£ç å—æ ‡è®°
        cleaned = re.sub(r'```[\w]*\s*', '', cleaned)
        
        # ç§»é™¤markdownæ ¼å¼æ ‡è®°
        cleaned = re.sub(r'^\s*```\s*$', '', cleaned, flags=re.MULTILINE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½è¡Œ
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned)
        
        # éªŒè¯å’Œä¿®å¤JSONç»“æ„
        cleaned = self._validate_and_fix_json(cleaned.strip())
        
        return cleaned
    
    def _validate_and_fix_json(self, text: str) -> str:
        """éªŒè¯JSONç»“æ„å®Œæ•´æ€§å¹¶å°è¯•ä¿®å¤"""
        
        # é¦–å…ˆå°è¯•è§£æJSON
        try:
            json.loads(text)
            print("âœ… JSONç»“æ„éªŒè¯é€šè¿‡")
            return text
        except json.JSONDecodeError as e:
            print(f"âš ï¸ æ£€æµ‹åˆ°JSONç»“æ„é—®é¢˜: {str(e)}")
            
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
            fixed_text = self._fix_common_json_issues(text)
            
            # å†æ¬¡éªŒè¯ä¿®å¤åçš„JSON
            try:
                json.loads(fixed_text)
                print("âœ… JSONç»“æ„ä¿®å¤æˆåŠŸ")
                return fixed_text
            except json.JSONDecodeError as e2:
                print(f"âŒ JSONä¿®å¤å¤±è´¥: {str(e2)}")
                # å¦‚æœä¿®å¤å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªæ ‡å‡†çš„é”™è¯¯JSONç»“æ„
                return self._create_fallback_json_response(text)
    
    def _fix_common_json_issues(self, text: str) -> str:
        """ä¿®å¤å¸¸è§çš„JSONé—®é¢˜"""
        import re
        
        print("ğŸ”§ å°è¯•ä¿®å¤JSONç»“æ„...")
        
        # 1. ç§»é™¤å¯èƒ½çš„éJSONå‰ç¼€å’Œåç¼€æ–‡æœ¬
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª{å’Œæœ€åä¸€ä¸ª}
        first_brace = text.find('{')
        last_brace = text.rfind('}')
        
        if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
            text = text[first_brace:last_brace + 1]
            print("   ğŸ“ æå–JSONä¸»ä½“éƒ¨åˆ†")
        
        # 2. ä¿®å¤æœªé—­åˆçš„å­—ç¬¦ä¸²å¼•å·
        # ç®€å•çš„å¼•å·ä¿®å¤ï¼šç¡®ä¿æ¯è¡Œçš„å¼•å·æ˜¯æˆå¯¹çš„
        lines = text.split('\n')
        fixed_lines = []
        
        for line in lines:
            # è®¡ç®—å¼•å·æ•°é‡
            quote_count = line.count('"') - line.count('\\"')  # æ’é™¤è½¬ä¹‰å¼•å·
            
            # å¦‚æœå¼•å·æ•°é‡ä¸ºå¥‡æ•°ï¼Œå¯èƒ½ç¼ºå°‘é—­åˆå¼•å·
            if quote_count % 2 == 1:
                # åœ¨è¡Œæœ«æ·»åŠ å¼•å·ï¼ˆå¦‚æœè¯¥è¡Œçœ‹èµ·æ¥åƒæ˜¯å€¼ï¼‰
                stripped = line.rstrip()
                if stripped and not stripped.endswith(('"', ',', '}', ']')):
                    line = stripped + '"'
                    if not line.endswith(',') and not line.endswith('}'):
                        line += ','
            
            fixed_lines.append(line)
        
        text = '\n'.join(fixed_lines)
        
        # 3. ä¿®å¤ç¼ºå°‘çš„é€—å·
        # åœ¨}å‰é¢çš„è¡Œå¦‚æœæ²¡æœ‰é€—å·ï¼Œæ·»åŠ é€—å·
        text = re.sub(r'(["\]}])\s*\n\s*(["\[{])', r'\1,\n\2', text)
        
        # 4. ä¿®å¤å¤šä½™çš„é€—å·
        # ç§»é™¤}å’Œ]å‰é¢çš„å¤šä½™é€—å·
        text = re.sub(r',(\s*[}\]])', r'\1', text)
        
        # 5. ä¿®å¤æœªé—­åˆçš„æ•°ç»„å’Œå¯¹è±¡
        open_braces = text.count('{') - text.count('}')
        open_brackets = text.count('[') - text.count(']')
        
        # æ·»åŠ ç¼ºå¤±çš„é—­åˆæ‹¬å·
        text += '}' * open_braces
        text += ']' * open_brackets
        
        print(f"   ğŸ”§ ä¿®å¤å®Œæˆ: æ·»åŠ äº†{open_braces}ä¸ª{{}}å’Œ{open_brackets}ä¸ª[]")
        
        return text
    
    def _create_fallback_json_response(self, original_text: str) -> str:
        """åˆ›å»ºå¤‡ç”¨çš„JSONå“åº”ç»“æ„"""
        print("ğŸ†˜ åˆ›å»ºå¤‡ç”¨JSONå“åº”")
        
        # å°è¯•ä»åŸå§‹æ–‡æœ¬ä¸­æå–ä¸€äº›ä¿¡æ¯
        extracted_info = self._extract_basic_info_from_text(original_text)
        
        fallback_response = {
            "image_id": "unknown",
            "visual_cot": [
                {
                    "step": 1,
                    "reasoning_level": "perception",
                    "reasoning": "ç”±äºJSONè§£æé”™è¯¯ï¼Œè¿›è¡ŒåŸºç¡€åˆ†æ",
                    "observation": extracted_info.get("observation", "æ— æ³•å®Œæ•´è§£æAPIå“åº”"),
                    "expected_outcome": "è·å–åŸºç¡€å›¾ç‰‡ä¿¡æ¯",
                    "inference": extracted_info.get("inference", "å“åº”æ ¼å¼å­˜åœ¨é—®é¢˜ï¼Œå·²è¿›è¡ŒåŸºç¡€å¤„ç†"),
                    "step_type": "error_handling",
                    "confidence": "ä½"
                }
            ],
            "structured_description": {
                "disaster_type": extracted_info.get("disaster_type", "æœªèƒ½è¯†åˆ«"),
                "affected_area": extracted_info.get("affected_area", "æœªè§‚å¯Ÿåˆ°"),
                "disaster_level": "æœªè§‚å¯Ÿåˆ°",
                "parsing_status": "JSONæ ¼å¼é”™è¯¯ï¼Œå·²ä½¿ç”¨å¤‡ç”¨ç»“æ„"
            },
            "overall_text_description": extracted_info.get("description", f"ç”±äºå“åº”æ ¼å¼é—®é¢˜ï¼Œæ— æ³•å®Œæ•´è§£æå›¾ç‰‡æè¿°ã€‚åŸå§‹å“åº”ç‰‡æ®µï¼š{original_text[:200]}...")
        }
        
        return json.dumps(fallback_response, ensure_ascii=False, indent=2)
    
    def _extract_basic_info_from_text(self, text: str) -> Dict[str, str]:
        """ä»æŸåçš„æ–‡æœ¬ä¸­æå–åŸºç¡€ä¿¡æ¯"""
        import re
        
        info = {}
        
        # å°è¯•æå–ç¾å®³ç±»å‹
        disaster_patterns = [
            r'["\'](æ´ªæ°´|æ´ªæ¶|æ°´ç¾|flooding)["\']',
            r'disaster_type["\']?\s*:\s*["\']([^"\']+)["\']',
            r'ç¾å®³ç±»å‹["\']?\s*:\s*["\']([^"\']+)["\']'
        ]
        
        for pattern in disaster_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                info["disaster_type"] = match.group(1)
                break
        
        # å°è¯•æå–æè¿°ä¿¡æ¯
        desc_patterns = [
            r'overall_text_description["\']?\s*:\s*["\']([^"\']{20,})["\']',
            r'æ€»ä½“.*?æè¿°["\']?\s*:\s*["\']([^"\']{20,})["\']',
            r'æè¿°["\']?\s*:\s*["\']([^"\']{20,})["\']'
        ]
        
        for pattern in desc_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                info["description"] = match.group(1)
                break
        
        # å°è¯•æå–è§‚å¯Ÿä¿¡æ¯
        obs_patterns = [
            r'observation["\']?\s*:\s*["\']([^"\']{10,})["\']',
            r'è§‚å¯Ÿ["\']?\s*:\s*["\']([^"\']{10,})["\']'
        ]
        
        for pattern in obs_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                info["observation"] = match.group(1)
                break
        
        return info
    
    def _format_prediction(self, api_response: str, task: Dict) -> Dict:
        """æ ¼å¼åŒ–é¢„æµ‹ç»“æœä¸ºLabel Studioæ ¼å¼(å¤‡ç”¨æ–¹æ³•)"""
        
        prediction = {
            "model_version": self.get("model_version"),
            "score": 0.85,
            "result": []
        }
        
        # è¿”å›æ¸…ç†åçš„æ–‡æœ¬ç»“æœ
        if api_response and api_response.strip():
            cleaned_response = self._clean_response_format(api_response.strip())
            prediction["result"].append({
                "from_name": "prediction",
                "to_name": "text",
                "type": "textarea",
                "value": {
                    "text": [cleaned_response]
                }
            })
        
        return prediction
    
    def _print_status(self):
        """ğŸ“Š ç®€åŒ–çš„çŠ¶æ€æ˜¾ç¤º"""
        current_model = get_current_model()
        current_api_key = get_current_api_key()
        
        print(f"\nğŸ¤– å½“å‰çŠ¶æ€:")
        print(f"   ğŸ¯ æ¨¡å‹: {current_model.split('/')[-1]} (å¤±è´¥: {self.consecutive_failures}/{self.max_failures_before_switch})")
        print(f"   ğŸ”‘ API Key: ***{current_api_key[-8:]} ({GLOBAL_API_KEY_INDEX + 1}/{len(api_key_list)})")
        print(f"   ğŸ“‹ å¯ç”¨æ¨¡å‹: {len(available_models_global)} ä¸ª")
        print(f"   ğŸ”„ å…¨å±€çŠ¶æ€ç®¡ç†: ç®€åŒ–åˆ‡æ¢é€»è¾‘")
    
    def get_status(self) -> Dict:
        """ğŸ” è·å–ç®€åŒ–çŠ¶æ€ä¿¡æ¯"""
        return {
            "current_model": get_current_model(),
            "current_api_key": f"***{get_current_api_key()[-8:]}",
            "consecutive_failures": self.consecutive_failures,
            "max_failures_before_switch": self.max_failures_before_switch,
            "available_models": available_models_global.copy(),
            "total_api_keys": len(api_key_list),
            "global_model_index": GLOBAL_MODEL_INDEX,
            "global_api_key_index": GLOBAL_API_KEY_INDEX,
            "management_type": "simplified_global_state"
        }
    
    def _get_field_names(self) -> tuple:
        """åŠ¨æ€è·å–Label Studioé…ç½®ä¸­çš„å­—æ®µå"""
        try:
            # å°è¯•ä»Label Studioé…ç½®ä¸­è·å–å­—æ®µå
            if hasattr(self, 'label_interface') and self.label_interface:
                # æŸ¥æ‰¾TextAreaæ ‡ç­¾
                textarea_from_name, textarea_to_name, _ = self.label_interface.get_first_tag_occurence(
                    'TextArea', ['Image', 'Text', 'HyperText']
                )
                if textarea_from_name and textarea_to_name:
                    return textarea_from_name, textarea_to_name
            
            # æŸ¥æ‰¾Imageæ ‡ç­¾
            if hasattr(self, 'label_interface') and self.label_interface:
                image_from_name, image_to_name, _ = self.label_interface.get_first_tag_occurence(
                    'Image', []
                )
                if image_from_name:
                    return "caption", image_from_name
            
        except Exception as e:
            pass
        
        # æ ¹æ®æ‚¨çš„æ¨¡æ¿ï¼Œä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
        return "caption", "image"
    
    def _extract_choice(self, response: str, choices: List[str]) -> Optional[str]:
        """ä»å“åº”ä¸­æå–æœ€åŒ¹é…çš„é€‰æ‹©"""
        response_lower = response.lower()
        for choice in choices:
            if choice.lower() in response_lower:
                return choice
        return choices[0] if choices else None
    
    def fit(self, event, data, **kwargs):
        """
        è®­ç»ƒ/æ›´æ–°å›¾ç‰‡æè¿°æ¨¡å‹
        :param event: äº‹ä»¶ç±»å‹ ('ANNOTATION_CREATED', 'ANNOTATION_UPDATED', 'START_TRAINING')
        :param data: äº‹ä»¶æ•°æ®(åŒ…å«å›¾ç‰‡å’Œæè¿°æ ‡æ³¨)
        """
        # è®°å½•æ ‡æ³¨æ•°æ®ç”¨äºæ¨¡å‹ä¼˜åŒ–
        old_data = self.get('annotation_data')
        self.set('annotation_data', 'updated_description_data')
        self.set('model_version', 'updated_version')
        print(f"âœ… å›¾ç‰‡æè¿°æ¨¡å‹å·²æ›´æ–° (äº‹ä»¶: {event})")
        print(f"ğŸ“¸ å·²è®°å½•æ–°çš„å›¾ç‰‡æè¿°æ ‡æ³¨æ•°æ®ï¼Œç”¨äºåç»­æ¨¡å‹ä¼˜åŒ–")

