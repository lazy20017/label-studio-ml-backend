"""
ğŸ”¥ æ£®æ—ç«ç¾æ³•å¾‹æ³•è§„ä¸åº”æ€¥é¢„æ¡ˆçŸ¥è¯†æå–æ¨¡å‹

ä¸“é—¨é’ˆå¯¹æ£®æ—ç«ç¾é¢†åŸŸçš„æ™ºèƒ½çŸ¥è¯†æå–ç³»ç»Ÿï¼Œç”¨äºä»æ³•å¾‹æ³•è§„ã€åº”æ€¥é¢„æ¡ˆã€
æŠ€æœ¯è§„èŒƒç­‰æ–‡æ¡£ä¸­ç²¾ç¡®æå–å…³é”®ä¿¡æ¯å®ä½“å’Œè¯­ä¹‰å…³ç³»ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æ£®æ—é˜²ç«æ³•å¾‹æ¡æ¬¾è¯†åˆ«
- åº”æ€¥å“åº”æµç¨‹æå–  
- ç»„ç»‡æœºæ„èŒè´£åˆ†æ
- æŠ€æœ¯æ ‡å‡†è§„èŒƒè§£æ
- å…³ç³»è¡¨è¾¾è¯­ä¹‰æ ‡æ³¨

é€‚ç”¨æ–‡æ¡£ç±»å‹ï¼š
- ã€Šæ£®æ—é˜²ç«æ¡ä¾‹ã€‹ç­‰æ³•å¾‹æ³•è§„
- æ£®æ—ç«ç¾åº”æ€¥é¢„æ¡ˆ
- æ£®æ—é˜²ç«æŠ€æœ¯æ ‡å‡†
- ç«é™©é¢„è­¦åˆ¶åº¦æ–‡ä»¶
- æ‰‘ç«ä½œæˆ˜æ–¹æ¡ˆ

ä½œè€…ï¼šForest Fire Knowledge Extraction Team
ç‰ˆæœ¬ï¼šv2.0 (æ£®æ—ç«ç¾ä¸“ç”¨ç‰ˆ)
å¯åŠ¨å‘½ä»¤ï¼šlabel-studio-ml start my_ml_backend
"""

from typing import List, Dict, Optional
import json
import os
import time
from openai import OpenAI
from label_studio_ml.model import LabelStudioMLBase
from label_studio_ml.response import ModelResponse

# ==================== æ£®æ—ç«ç¾ä¸“ç”¨å®ä½“é…ç½® ====================
# ğŸ”¥ ä»æ£®æ—ç«ç¾ä¸“ç”¨é…ç½®æ–‡ä»¶å¯¼å…¥å®ä½“é…ç½®
try:
    from .entity_config_forest_fire import get_entity_config, get_entity_labels, get_all_categories, get_entities_by_category
    NER_ENTITY_CONFIG = get_entity_config()
    ENTITY_LABELS = get_entity_labels()
    print(f"ğŸ”¥ åŠ è½½äº† {len(ENTITY_LABELS)} ç§æ£®æ—ç«ç¾ä¸“ç”¨å®ä½“ç±»å‹")
except ImportError as e:
    print(f"âŒ æ£®æ—ç«ç¾é…ç½®æ–‡ä»¶å¯¼å…¥å¤±è´¥: {e}")
    # å¦‚æœæ£®æ—ç«ç¾é…ç½®å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é€šç”¨é…ç½®
    try:
        from .entity_config import get_entity_config, get_entity_labels, get_all_categories, get_entities_by_category
        NER_ENTITY_CONFIG = get_entity_config()
        ENTITY_LABELS = get_entity_labels()
        print(f"âš ï¸ ä½¿ç”¨é€šç”¨é…ç½®æ–‡ä»¶: {len(ENTITY_LABELS)} ç§å®ä½“ç±»å‹")
    except ImportError as e2:
        print(f"âŒ æ‰€æœ‰é…ç½®æ–‡ä»¶éƒ½æ— æ³•å¯¼å…¥: {e2}")
        # æœ€åå°è¯•ç»å¯¹å¯¼å…¥
        try:
            import sys
            import os
            current_dir = os.path.dirname(os.path.abspath(__file__))
            sys.path.insert(0, current_dir)
            from entity_config_forest_fire import get_entity_config, get_entity_labels, get_all_categories, get_entities_by_category
            NER_ENTITY_CONFIG = get_entity_config()
            ENTITY_LABELS = get_entity_labels()
            print(f"ğŸ”¥ ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½æ£®æ—ç«ç¾é…ç½®: {len(ENTITY_LABELS)} ç§å®ä½“ç±»å‹")
        except ImportError as e3:
            print(f"âŒ æœ€ç»ˆå¯¼å…¥å¤±è´¥: {e3}")
            exit()


# ç”ŸæˆJSONæ ¼å¼ç¤ºä¾‹
def get_json_format_example():
    """ç”ŸæˆJSONæ ¼å¼ç¤ºä¾‹"""
    return """{{
  "entities": [
    {{
      "text": "å®ä½“æ–‡æœ¬",
      "start": èµ·å§‹ä½ç½®,
      "end": ç»“æŸä½ç½®,
      "label": "å®ä½“ç±»å‹"
    }}
  ]
}}"""

def validate_label(label: str) -> str:
    """éªŒè¯æ ‡ç­¾æ˜¯å¦åœ¨æœ‰æ•ˆæ ‡ç­¾åˆ—è¡¨ä¸­"""
    if not label:
        return None
    
    clean_label = label.strip()
    
    # ç›´æ¥åŒ¹é…æ ‡ç­¾åç§°
    if clean_label in ENTITY_LABELS:
        return clean_label
    
    return None



class NewModel(LabelStudioMLBase):
    """ğŸ”¥ æ£®æ—ç«ç¾ä¸“ç”¨çŸ¥è¯†æå–æ¨¡å‹
    
    ä¸“é—¨é’ˆå¯¹æ£®æ—ç«ç¾é¢†åŸŸä¼˜åŒ–çš„å‘½åå®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–æ¨¡å‹ã€‚
    èƒ½å¤Ÿä»æ£®æ—é˜²ç«æ³•å¾‹æ³•è§„ã€åº”æ€¥é¢„æ¡ˆã€æŠ€æœ¯æ ‡å‡†ç­‰æ–‡æ¡£ä¸­
    ç²¾ç¡®æå–å…³é”®ä¿¡æ¯å®ä½“å’Œè¯­ä¹‰å…³ç³»ã€‚
    
    æ ¸å¿ƒèƒ½åŠ›ï¼š
    - ğŸ›ï¸ æ³•å¾‹æ¡æ¬¾ç²¾ç¡®è¯†åˆ«
    - ğŸš¨ åº”æ€¥æµç¨‹å…³ç³»æŠ½å–  
    - ğŸ¢ ç»„ç»‡æœºæ„èŒè´£åˆ†æ
    - âš™ï¸ æŠ€æœ¯æ ‡å‡†è§„èŒƒè§£æ
    - ğŸ”— å¤šå…ƒå…³ç³»è¯­ä¹‰æ ‡æ³¨
    - ğŸ¯ æ£®æ—é˜²ç«ä¸“ä¸šæœ¯è¯­è¯†åˆ«
    """
    
    def setup(self):
        """ğŸ”¥ æ£®æ—ç«ç¾ä¸“ç”¨æ¨¡å‹åˆå§‹åŒ–é…ç½®
        """
        self.set("model_version", "2.0.0-æ£®æ—ç«ç¾ä¸“ç”¨ç‰ˆ")
        
        # é­”å¡”ç¤¾åŒºAPIé…ç½®
        self.api_key = os.getenv('MODELSCOPE_API_KEY', 'ms-2c045fb7-f463-45bf-b0f9-a36d50b0400e')
        self.api_base_url = os.getenv('MODELSCOPE_API_URL', 'https://api-inference.modelscope.cn/v1')
        
        # ğŸ”„ å¤šæ¨¡å‹é…ç½® - æŒ‰ä¼˜å…ˆçº§æ’åº
        self.available_models = [
                'Qwen/Qwen3-Coder-480B-A35B-Instruct', # ä¸»åŠ›æ¨¡å‹ - æœ€æ–°Qwen3
                'ZhipuAI/GLM-4.5', # å¤‡ç”¨æ¨¡å‹1 - å¤§å‚æ•°é‡
                'deepseek-ai/DeepSeek-V3.1', # å¤‡ç”¨æ¨¡å‹2 - ä¸­ç­‰å‚æ•°é‡
                'Qwen/Qwen3-235B-A22B-Instruct-2507', # å¤‡ç”¨æ¨¡å‹2 - ä¸­ç­‰å‚æ•°é‡
                'deepseek-ai/DeepSeek-R1-0528', # å¤‡ç”¨æ¨¡å‹5 - å¹³è¡¡æ€§èƒ½
                'deepseek-ai/DeepSeek-R1-0528' # å¤‡ç”¨æ¨¡å‹6 - å¹³è¡¡æ€§èƒ½
        ]
        
        # ğŸ¯ æ¨¡å‹åˆ‡æ¢æ§åˆ¶
        self.current_model_index = 0  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹ç´¢å¼•
        self.model_consecutive_failures = 0  # å½“å‰æ¨¡å‹è¿ç»­å¤±è´¥æ¬¡æ•°
        self.max_model_failures = 3  # æ¨¡å‹è¿ç»­å¤±è´¥é˜ˆå€¼
        self.model_failure_history = {}  # è®°å½•æ¯ä¸ªæ¨¡å‹çš„å¤±è´¥å†å²
        
        # åŠ¨æ€è·å–å½“å‰æ¨¡å‹åç§°
        self.model_name = self.available_models[self.current_model_index]
        
        # å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œåªåœ¨éœ€è¦æ—¶è¿æ¥
        self.client = None
        self._api_initialized = False
        
        print("ğŸ”¥ğŸ”¥ğŸ”¥ æ£®æ—ç«ç¾ä¸“ç”¨çŸ¥è¯†æå–æ¨¡å‹åˆå§‹åŒ–å®Œæˆ ğŸ”¥ğŸ”¥ğŸ”¥")
        print(f"ğŸ“Š æ¨¡å‹ç‰ˆæœ¬: {self.get('model_version')}")
        print(f"ğŸ¯ ä¸»åŠ›æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“‹ å¤‡ç”¨æ¨¡å‹: {len(self.available_models)-1} ä¸ª")
        print(f"ğŸ”„ æ™ºèƒ½åˆ‡æ¢: 429é”™è¯¯ç«‹å³åˆ‡æ¢ï¼Œè¿ç»­å¤±è´¥{self.max_model_failures}æ¬¡åˆ‡æ¢")
        print(f"ğŸ† ä¸“ä¸šé¢†åŸŸ: æ£®æ—é˜²ç«æ³•å¾‹æ³•è§„ä¸åº”æ€¥é¢„æ¡ˆ")
        print(f"ğŸ’ª æ ¸å¿ƒèƒ½åŠ›: æ³•è§„æ¡æ¬¾ã€åº”æ€¥æµç¨‹ã€ç»„ç»‡èŒè´£ã€æŠ€æœ¯æ ‡å‡†ã€å…³ç³»æŠ½å–")
        
    def _switch_to_next_model(self):
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹"""
        # è®°å½•å½“å‰æ¨¡å‹çš„å¤±è´¥
        current_model = self.available_models[self.current_model_index]
        if current_model not in self.model_failure_history:
            self.model_failure_history[current_model] = 0
        self.model_failure_history[current_model] += self.model_consecutive_failures
        
        # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
        old_model = self.model_name
        old_index = self.current_model_index
        
        self.current_model_index = (self.current_model_index + 1) % len(self.available_models)
        self.model_name = self.available_models[self.current_model_index]
        self.model_consecutive_failures = 0  # é‡ç½®æ–°æ¨¡å‹çš„å¤±è´¥è®¡æ•°
        
        # é‡ç½®APIè¿æ¥ï¼ˆå¼ºåˆ¶é‡æ–°è¿æ¥æ–°æ¨¡å‹ï¼‰
        self._api_initialized = False
        self.client = None
        
        print(f"ğŸ”„ æ¨¡å‹åˆ‡æ¢: {old_model} â†’ {self.model_name}")
        print(f"ğŸ“Š åˆ‡æ¢åŸå› : è¿ç»­å¤±è´¥ {self.max_model_failures} æ¬¡")
        
        # å¦‚æœå›åˆ°äº†ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œè¯´æ˜æ‰€æœ‰æ¨¡å‹éƒ½è¯•è¿‡äº†
        if self.current_model_index == 0 and old_index != 0:
            print("âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½å·²å°è¯•ï¼Œå›åˆ°ä¸»åŠ›æ¨¡å‹")
            
        return True
    
    def _handle_model_failure(self, reason: str = "æœªçŸ¥é”™è¯¯", force_switch: bool = False):
        """ğŸ”¥ æ£®æ—ç«ç¾ä¸“ç”¨æ™ºèƒ½æ¨¡å‹å¤±è´¥å¤„ç†"""
        self.model_consecutive_failures += 1
        current_model = self.available_models[self.current_model_index]
        
        print(f"âŒ æ£®æ—ç«ç¾æ¨¡å‹ {current_model} å¤±è´¥: {reason} (è¿ç»­å¤±è´¥: {self.model_consecutive_failures}/{self.max_model_failures})")
        
        # ğŸš¨ å¼ºåˆ¶åˆ‡æ¢æˆ–è¾¾åˆ°å¤±è´¥é˜ˆå€¼æ—¶åˆ‡æ¢æ¨¡å‹
        should_switch = force_switch or (self.model_consecutive_failures >= self.max_model_failures)
        
        if should_switch:
            if len(self.available_models) > 1:  # åªæœ‰åœ¨æœ‰å¤šä¸ªæ¨¡å‹æ—¶æ‰åˆ‡æ¢
                if force_switch:
                    print(f"ğŸ”¥ æ£®æ—ç«ç¾æ¨¡å‹å¼ºåˆ¶åˆ‡æ¢: {reason}")
                else:
                    print(f"ğŸ“Š è¾¾åˆ°å¤±è´¥é˜ˆå€¼ï¼Œåˆ‡æ¢æ£®æ—ç«ç¾æ¨¡å‹")
                self._switch_to_next_model()
                return True  # è¡¨ç¤ºå·²åˆ‡æ¢æ¨¡å‹
            else:
                print("âš ï¸ åªæœ‰ä¸€ä¸ªæ£®æ—ç«ç¾æ¨¡å‹å¯ç”¨ï¼Œæ— æ³•åˆ‡æ¢")
                return False
        
        return False  # æ²¡æœ‰åˆ‡æ¢æ¨¡å‹
    
    def _handle_model_success(self):
        """å¤„ç†æ¨¡å‹æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°"""
        if self.model_consecutive_failures > 0:
            print(f"âœ… æ£®æ—ç«ç¾æ¨¡å‹ {self.model_name} æ¢å¤æ­£å¸¸")
            self.model_consecutive_failures = 0
    
    def _should_switch_immediately(self, error_str: str) -> bool:
        """ğŸ”¥ æ£®æ—ç«ç¾ä¸“ç”¨ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ç«‹å³åˆ‡æ¢æ¨¡å‹ï¼ˆä¸é‡è¯•ï¼‰"""
        immediate_switch_patterns = [
            # APIé™æµé”™è¯¯ - ç«‹å³åˆ‡æ¢
            "429",
            "Too Many Requests", 
            "Request limit exceeded",
            "Rate limit exceeded",
            "Quota exceeded",
            
            # è®¤è¯/æƒé™é”™è¯¯ - ç«‹å³åˆ‡æ¢
            "401", 
            "403",
            "Unauthorized",
            "Forbidden",
            "Invalid API key",
            "API key expired",
            
            # æ¨¡å‹ä¸å¯ç”¨é”™è¯¯ - ç«‹å³åˆ‡æ¢
            "404",
            "Model not found",
            "Model unavailable",
            "Service unavailable",
            "Model is overloaded",
            
            # æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ - ç«‹å³åˆ‡æ¢
            "500",
            "502", 
            "503",
            "504",
            "Internal server error",
            "Bad gateway",
            "Gateway timeout",
            
            # å‚æ•°é”™è¯¯ - ç«‹å³åˆ‡æ¢ï¼ˆæ¨¡å‹ä¸æ”¯æŒå½“å‰å‚æ•°ï¼‰
            "Invalid model",
            "Unsupported model",
            "Model does not exist"
        ]
        
        error_lower = error_str.lower()
        for pattern in immediate_switch_patterns:
            if pattern.lower() in error_lower:
                return True
        
        return False
    
    def _get_error_type(self, error_str: str) -> str:
        """ğŸ”¥ æ£®æ—ç«ç¾ä¸“ç”¨ï¼šè·å–é”™è¯¯ç±»å‹æè¿°"""
        error_lower = error_str.lower()
        
        # ğŸš¨ é«˜ä¼˜å…ˆçº§é”™è¯¯ï¼ˆç«‹å³åˆ‡æ¢ï¼‰
        if any(x in error_lower for x in ["429", "too many requests", "rate limit", "quota exceeded"]):
            return "APIé™æµ"
        elif any(x in error_lower for x in ["401", "403", "unauthorized", "forbidden", "api key"]):
            return "è®¤è¯å¤±è´¥"
        elif any(x in error_lower for x in ["404", "model not found", "model unavailable"]):
            return "æ¨¡å‹ä¸å­˜åœ¨"
        elif any(x in error_lower for x in ["500", "502", "503", "504", "internal server"]):
            return "æœåŠ¡å™¨é”™è¯¯"
        elif any(x in error_lower for x in ["invalid model", "unsupported model"]):
            return "æ¨¡å‹ä¸æ”¯æŒ"
        
        # ğŸ”„ ä¸€èˆ¬é”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰
        elif any(x in error_lower for x in ["timeout", "connection", "network"]):
            return "ç½‘ç»œè¶…æ—¶"
        elif any(x in error_lower for x in ["json", "parse", "format"]):
            return "æ ¼å¼é”™è¯¯"
        elif "empty" in error_lower or "ç©º" in error_str:
            return "ç©ºå“åº”"
        else:
            return "æœªçŸ¥é”™è¯¯"
        
    def _ensure_api_connection(self):
        """ç¡®ä¿APIè¿æ¥å·²åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._api_initialized and self.client:
            return True
        
        if not self.api_key:
            print("âš ï¸ è¯·è®¾ç½®MODELSCOPE_API_KEYç¯å¢ƒå˜é‡")
            return False
        
        try:
            print("ğŸ”„ æ­£åœ¨è¿æ¥å¤§æ¨¡å‹API...")
            self.client = OpenAI(
                base_url=self.api_base_url,
                api_key=self.api_key,
                max_retries=0,  # ğŸš¨ ç¦ç”¨OpenAIå†…ç½®é‡è¯•ï¼Œè®©æ™ºèƒ½åˆ‡æ¢æ¥ç®¡
                timeout=30.0    # ğŸš¨ è®¾ç½®è¶…æ—¶é¿å…é•¿æ—¶é—´ç­‰å¾…
            )
            
            # æµ‹è¯•è¿æ¥
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10,
                temperature=0.1
            )
            
            self._api_initialized = True
            print("âœ… å¤§æ¨¡å‹APIè¿æ¥æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ APIè¿æ¥å¤±è´¥: {str(e)[:100]}")
            self.client = None
            self._api_initialized = False
            return False
    


    def predict(self, tasks: List[Dict], context: Optional[Dict] = None, **kwargs) -> ModelResponse:
        """ å‘½åå®ä½“è¯†åˆ«é¢„æµ‹
            :param tasks: Label Studio tasks in JSON format
            :param context: Label Studio context in JSON format
            :return: ModelResponse with predictions
        """
        total_tasks = len(tasks)
        predictions = []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå®ä½“æ ‡æ³¨ä»»åŠ¡
        if not self._is_annotation_task(tasks):
            print("â„¹ï¸ éæ ‡æ³¨ä»»åŠ¡ï¼Œè·³è¿‡å¤§æ¨¡å‹è¿æ¥")
            # è¿”å›ç©ºé¢„æµ‹ç»“æœ
            empty_predictions = []
            for task in tasks:
                empty_prediction = {
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": []
                }
                empty_predictions.append(empty_prediction)
            return ModelResponse(predictions=empty_predictions)
        
        # åªåœ¨éœ€è¦è¿›è¡Œæ ‡æ³¨é¢„æµ‹æ—¶æ‰è¿æ¥å¤§æ¨¡å‹
        if not self._ensure_api_connection():
            print("âŒ æ— æ³•è¿æ¥å¤§æ¨¡å‹APIï¼Œè¿”å›ç©ºé¢„æµ‹ç»“æœ")
            empty_predictions = []
            for task in tasks:
                empty_prediction = {
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": [],
                    "error": "APIè¿æ¥å¤±è´¥"
                }
                empty_predictions.append(empty_prediction)
            return ModelResponse(predictions=empty_predictions)
        
        if total_tasks > 1:
            print(f"ğŸš€ å¼€å§‹å¤„ç† {total_tasks} ä¸ªæ ‡æ³¨ä»»åŠ¡")
        
        start_time = time.time()
        
        for i, task in enumerate(tasks):
            if total_tasks > 1:  # å¤šä»»åŠ¡æ—¶æ˜¾ç¤ºè¿›åº¦
                print(f"\nğŸ”„ å¤„ç†ä»»åŠ¡ {i+1}/{total_tasks}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            task_start_time = time.time()
            
            try:
                prediction = self._process_single_task(task)
                task_end_time = time.time()
                task_duration = task_end_time - task_start_time
                
                if prediction and prediction.get('result') and len(prediction.get('result', [])) > 0:
                    # æˆåŠŸè¯†åˆ«åˆ°å®ä½“
                    predictions.append(prediction)
                    entities_count = len(prediction.get('result', []))
                    if total_tasks > 1:
                        print(f"âœ… ä»»åŠ¡ {i+1} æˆåŠŸ (è€—æ—¶: {task_duration:.1f}s, å®ä½“: {entities_count})")
                else:
                    # æœªè¯†åˆ«åˆ°å®ä½“æˆ–å¤„ç†å¤±è´¥
                    failed_prediction = {
                        "model_version": self.get("model_version"),
                        "score": 0.0,
                        "result": [],
                        "error": "æœªè¯†åˆ«åˆ°ä»»ä½•å®ä½“",
                        "status": "failed"
                    }
                    predictions.append(failed_prediction)
                    if total_tasks > 1:
                        print(f"âŒ ä»»åŠ¡ {i+1} å¤±è´¥ - æ— å®ä½“ (è€—æ—¶: {task_duration:.1f}s)")
                    
            except Exception as e:
                task_end_time = time.time()
                task_duration = task_end_time - task_start_time
                if total_tasks > 1:
                    print(f"âŒ ä»»åŠ¡ {i+1} å¼‚å¸¸ (è€—æ—¶: {task_duration:.1f}s): {str(e)[:50]}")
                failed_prediction = {
                    "model_version": self.get("model_version"),
                    "score": 0.0,
                    "result": [],
                    "error": f"å¤„ç†å¼‚å¸¸: {str(e)}",
                    "status": "failed"
                }
                predictions.append(failed_prediction)
            
        
        # å¤„ç†å®Œæˆåçš„æ€»ç»“
        end_time = time.time()
        total_duration = end_time - start_time
        processed_count = len(predictions)
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡
        successful_tasks = sum(1 for p in predictions if p.get('result') and len(p.get('result', [])) > 0)
        failed_tasks = processed_count - successful_tasks
        total_entities = sum(len(p.get('result', [])) for p in predictions)
        
        if total_tasks > 1:
            print(f"\nğŸ“Š å¤„ç†å®Œæˆ: {successful_tasks}/{processed_count} æˆåŠŸ, æ€»å®ä½“: {total_entities}, è€—æ—¶: {total_duration:.1f}s")
            if failed_tasks > 0:
                print(f"âš ï¸ {failed_tasks} ä¸ªä»»åŠ¡å¤„ç†å¤±è´¥")
            
            # æ˜¾ç¤ºæ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
            self._print_model_statistics()
        
        return ModelResponse(predictions=predictions)
    
    def _is_annotation_task(self, tasks: List[Dict]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦è¿›è¡Œå®ä½“æ ‡æ³¨çš„ä»»åŠ¡"""
        if not tasks:
            return False
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åŒ…å«éœ€è¦æ ‡æ³¨çš„æ–‡æœ¬å†…å®¹
        for task in tasks:
            task_data = task.get('data', {})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹éœ€è¦æ ‡æ³¨
            text_keys = ['text', 'content', 'prompt', 'question', 'description', 'query']
            has_text_content = False
            
            for key, value in task_data.items():
                if isinstance(value, str) and key in text_keys and value.strip():
                    has_text_content = True
                    break
            
            if has_text_content:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ ‡æ³¨ç»“æœï¼ˆå¦‚æœæœ‰å®Œæ•´æ ‡æ³¨åˆ™å¯èƒ½æ˜¯æŸ¥çœ‹ä»»åŠ¡ï¼‰
                annotations = task.get('annotations', [])
                if annotations:
                    # å¦‚æœæœ‰æ ‡æ³¨ä½†æ˜¯ç©ºçš„ï¼Œè¯´æ˜éœ€è¦é¢„æµ‹
                    for annotation in annotations:
                        result = annotation.get('result', [])
                        if not result:  # ç©ºæ ‡æ³¨ï¼Œéœ€è¦é¢„æµ‹
                            return True
                else:
                    # æ²¡æœ‰æ ‡æ³¨ï¼Œéœ€è¦é¢„æµ‹
                    return True
        
        return False
    
    def _process_single_task(self, task: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task_data = task.get('data', {})
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = ""
        text_keys = ['text', 'content', 'prompt', 'question', 'description', 'query']
        
        for key, value in task_data.items():
            if isinstance(value, str) and key in text_keys:
                text_content = value
                break
        
        if not text_content:
            return None
        
        # ğŸ”¥ æ„å»ºæ£®æ—ç«ç¾ä¸“ç”¨NERæç¤ºè¯
        json_format = get_json_format_example()
        
        # æŒ‰ç±»åˆ«ç»„ç»‡å®ä½“ç±»å‹è¯´æ˜
        try:
            from entity_config import get_all_categories, get_entities_by_category
            categories = get_all_categories()
            categorized_examples = ""
            
            for category in categories:
                entities = get_entities_by_category(category)
                if entities:
                    # ç‰¹æ®Šå¤„ç†å…³ç³»æ ‡ç­¾ç±»åˆ«
                    if category == "å…³ç³»æ ‡ç­¾":
                        categorized_examples += f"\nğŸ”— {category}ç±»ï¼ˆè¯­ä¹‰å…³ç³»å®ä½“ï¼‰:\n"
                        for label_key, config in list(entities.items())[:8]:  # å…³ç³»æ ‡ç­¾æ˜¾ç¤ºæ›´å¤š
                            examples = "ã€".join(config['examples'][:3])  # å…³ç³»æ ‡ç­¾æ˜¾ç¤º3ä¸ªç¤ºä¾‹
                            description = config['description']
                            categorized_examples += f"   â€¢ {description}: {examples}\n"
                    else:
                        categorized_examples += f"\nğŸ“‚ {category}ç±»:\n"
                        for label_key, config in list(entities.items())[:5]:  # æ¯ç±»æœ€å¤šæ˜¾ç¤º5ä¸ªå®ä½“ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
                            examples = "ã€".join(config['examples'][:2])  # æ¯ä¸ªå®ä½“æ˜¾ç¤º2ä¸ªç¤ºä¾‹
                            description = config['description']
                            categorized_examples += f"   â€¢ {description}: {examples}\n"
            
            # ç”Ÿæˆç®€åŒ–çš„å®ä½“åˆ—è¡¨ï¼ˆä½¿ç”¨descriptionï¼‰
            entity_descriptions = []
            for label_key in ENTITY_LABELS[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªï¼Œé¿å…è¿‡é•¿
                if label_key in NER_ENTITY_CONFIG:
                    entity_descriptions.append(NER_ENTITY_CONFIG[label_key]['description'])
                else:
                    entity_descriptions.append(label_key)
            
            entity_labels_list = "ã€".join(entity_descriptions)
            if len(ENTITY_LABELS) > 20:
                entity_labels_list += f"ç­‰{len(ENTITY_LABELS)}ç§å®ä½“ç±»å‹"
                
        except ImportError:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸæ¥çš„æ ¼å¼
            categorized_examples = ""
            for label, config in NER_ENTITY_CONFIG.items():
                examples = "ã€".join(config['examples'][:2])
                categorized_examples += f"   {label}({config['description']}): {examples}\n"
            entity_labels_list = "ã€".join(ENTITY_LABELS)
        
        # ç”Ÿæˆä¸¥æ ¼çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆåªæ˜¾ç¤ºæ ‡ç­¾åç§°ï¼Œä¸æ˜¾ç¤ºæè¿°ï¼‰
        valid_labels_only = list(ENTITY_LABELS)
        labels_display = "ã€".join(valid_labels_only)
        
        prompt = f"""ğŸ”¥ æ£®æ—ç«ç¾æ³•å¾‹æ³•è§„ä¸åº”æ€¥é¢„æ¡ˆçŸ¥è¯†æå–ä»»åŠ¡

æ‚¨æ˜¯æ£®æ—é˜²ç«é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ£®æ—ç«ç¾ç›¸å…³çš„æ³•å¾‹æ³•è§„ã€åº”æ€¥é¢„æ¡ˆã€è§„èŒƒæ ‡å‡†æ–‡æœ¬ä¸­ï¼Œç²¾ç¡®æå–å…³é”®ä¿¡æ¯å®ä½“å’Œè¯­ä¹‰å…³ç³»ã€‚

ğŸ“‹ å¾…åˆ†ææ–‡æœ¬ï¼š
{text_content}

ğŸ¯ æ£®æ—ç«ç¾é¢†åŸŸå®ä½“ç±»å‹åŠç¤ºä¾‹ï¼š{categorized_examples}

ğŸ”¥ æ£®æ—ç«ç¾ä¸“ä¸šé¢†åŸŸå…³æ³¨é‡ç‚¹ï¼š

ğŸ“œ **æ³•å¾‹æ³•è§„å±‚é¢**ï¼š
- æ£®æ—é˜²ç«æ³•å¾‹æ¡æ¬¾ï¼šã€Šæ£®æ—é˜²ç«æ¡ä¾‹ã€‹ç¬¬Xæ¡ã€ã€Šåˆ‘æ³•ã€‹å±å®³å…¬å…±å®‰å…¨ç½ªç­‰
- è´£ä»»ä¸»ä½“åˆ’åˆ†ï¼šå„çº§æ”¿åºœã€æ—ä¸šéƒ¨é—¨ã€æ£®æ—ç»è¥å•ä½ç­‰èŒè´£
- æ³•å¾‹è´£ä»»è®¤å®šï¼šåˆ‘äº‹è´£ä»»ã€è¡Œæ”¿è´£ä»»ã€æ°‘äº‹è´£ä»»ç­‰

ğŸš¨ **åº”æ€¥é¢„æ¡ˆå±‚é¢**ï¼š
- é¢„è­¦ç­‰çº§åˆ’åˆ†ï¼šè“è‰²ã€é»„è‰²ã€æ©™è‰²ã€çº¢è‰²é¢„è­¦çº§åˆ«
- åº”æ€¥å“åº”æµç¨‹ï¼šå‘ç°æŠ¥å‘Šâ†’è¯„ä¼°åˆ†æâ†’å¯åŠ¨å“åº”â†’å¤„ç½®æ•‘æ´â†’æ¢å¤é‡å»º
- ç»„ç»‡æŒ‡æŒ¥ä½“ç³»ï¼šåº”æ€¥æŒ‡æŒ¥éƒ¨ã€ç°åœºæŒ‡æŒ¥éƒ¨ã€å„å·¥ä½œç»„è®¾ç½®

ğŸ› ï¸ **æŠ€æœ¯è§„èŒƒå±‚é¢**ï¼š
- é˜²ç«è®¾æ–½æ ‡å‡†ï¼šé˜²ç«é€šé“å®½åº¦ã€é˜²ç«çº¿è§„æ ¼ã€æ¶ˆé˜²è®¾å¤‡é…ç½®ç­‰
- ç›‘æµ‹é¢„è­¦æŠ€æœ¯ï¼šç­æœ›å¡”ã€è§†é¢‘ç›‘æ§ã€å«æ˜Ÿé¥æ„Ÿã€æ— äººæœºå·¡æŠ¤ç­‰
- æ‰‘ç«æˆ˜æœ¯æ–¹æ³•ï¼šç›´æ¥æ‰‘æ‰“ã€é—´æ¥ç­ç«ã€ä»¥ç«æ”»ç«ã€åŒ–å­¦ç­ç«ç­‰

ğŸ”— **å…³ç³»è¡¨è¾¾è¯†åˆ«**ï¼š
æ£®æ—ç«ç¾é¢†åŸŸçš„å…³ç³»æ ‡ç­¾ç”¨äºæ ‡æ³¨å®ä½“é—´çš„ä¸“ä¸šè¯­ä¹‰å…³ç³»ï¼š

ğŸ’¡ **æ£®æ—ç«ç¾å…³ç³»æ ‡æ³¨åŸåˆ™**ï¼š
1. **æ³•å¾‹ä¾æ®å…³ç³»**ï¼šå¦‚"æ ¹æ®ã€Šæ£®æ—é˜²ç«æ¡ä¾‹ã€‹è§„å®š"ã€"ä¾ç…§å›½å®¶æ ‡å‡†æ‰§è¡Œ"
2. **è´£ä»»ç®¡è¾–å…³ç³»**ï¼šå¦‚"å¿æ”¿åºœè´Ÿè´£æœ¬è¡Œæ”¿åŒºåŸŸæ£®æ—é˜²ç«"ã€"æ—ä¸šå±€ä¸»ç®¡æ£®æ—é˜²ç«å·¥ä½œ"  
3. **å› æœå½±å“å…³ç³»**ï¼šå¦‚"å¹²æ—±å¤©æ°”å¯¼è‡´ç«é™©ç­‰çº§å‡é«˜"ã€"è¿è§„ç”¨ç«å¼•å‘æ£®æ—ç«ç¾"
4. **æ—¶åºæµç¨‹å…³ç³»**ï¼šå¦‚"å‘ç°ç«æƒ…åç«‹å³æŠ¥å‘Š"ã€"æ‰‘æ•‘ç»“æŸåå¼€å±•è°ƒæŸ¥"
5. **åè°ƒé…åˆå…³ç³»**ï¼šå¦‚"å¤šéƒ¨é—¨è”åˆè¡ŒåŠ¨"ã€"å†›åœ°ååŒä½œæˆ˜"

ğŸ” **æ£®æ—ç«ç¾ä¸“ä¸šæœ¯è¯­ç‰¹åˆ«å…³æ³¨**ï¼š
- **ç«é™©ç­‰çº§**ï¼šä¸€çº§ï¼ˆä½ï¼‰ã€äºŒçº§ï¼ˆè¾ƒä½ï¼‰ã€ä¸‰çº§ï¼ˆä¸­ç­‰ï¼‰ã€å››çº§ï¼ˆé«˜ï¼‰ã€äº”çº§ï¼ˆæé«˜ï¼‰
- **ç«ç¾ç±»å‹**ï¼šåœ°è¡¨ç«ã€æ ‘å† ç«ã€åœ°ä¸‹ç«ï¼ˆæ³¥ç‚­ç«ï¼‰
- **æ‰‘ç«ç»„ç»‡**ï¼šä¸“ä¸šæ‰‘ç«é˜Ÿã€åŠä¸“ä¸šæ‰‘ç«é˜Ÿã€ç¾¤ä¼—æ‰‘ç«é˜Ÿã€æ£®æ—æ¶ˆé˜²é˜Ÿä¼
- **ç«æºç®¡æ§**ï¼šè®¡åˆ’çƒ§é™¤ã€ç”Ÿäº§æ€§ç”¨ç«ã€ç”Ÿæ´»ç”¨ç«ã€ç¥­ç¥€ç”¨ç«
- **æ°”è±¡è¦ç´ **ï¼šé£å‘é£é€Ÿã€æ¹¿åº¦ã€æ¸©åº¦ã€é™æ°´é‡ã€å¯ç‡ƒç‰©å«æ°´ç‡

âš ï¸ **æå–è¦æ±‚**ï¼š
1. ç²¾ç¡®è¯†åˆ«æ£®æ—é˜²ç«ä¸“ä¸šå®ä½“ï¼ŒåŒ…æ‹¬æ³•è§„æ¡æ–‡ã€æŠ€æœ¯æ ‡å‡†ã€ç»„ç»‡æœºæ„ã€è®¾å¤‡è£…å¤‡ç­‰
2. å‡†ç¡®æ ‡æ³¨å®ä½“è¾¹ç•Œä½ç½®ï¼ˆå­—ç¬¦çº§ç²¾ç¡®å®šä½ï¼‰  
3. ä¸¥æ ¼ä½¿ç”¨é…ç½®çš„æ ‡ç­¾åç§°ï¼Œç¦æ­¢è‡ªåˆ›æˆ–å˜æ›´æ ‡ç­¾
4. å…³ç³»æ ‡ç­¾å¿…é¡»åŒ…å«å®Œæ•´çš„è¯­ä¹‰è¡¨è¾¾ï¼Œä½“ç°æ£®æ—é˜²ç«ä¸šåŠ¡é€»è¾‘
5. é‡ç‚¹å…³æ³¨åº”æ€¥é¢„æ¡ˆä¸­çš„æµç¨‹æ­¥éª¤ã€èŒè´£åˆ†å·¥ã€æŠ€æœ¯æ ‡å‡†

ğŸ“‹ è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{json_format}

ğŸ·ï¸ ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹æ ‡ç­¾åç§°ï¼ˆä¸€å­—ä¸å·®ï¼‰ï¼š
{labels_display}

âŒ **æ£®æ—ç«ç¾é¢†åŸŸç¦æ­¢äº‹é¡¹**ï¼š
- ç¦æ­¢å°†"æ¶ˆé˜²é˜Ÿ"æ ‡æ³¨ä¸º"åº”æ€¥é˜Ÿä¼"ï¼ˆåº”ä½¿ç”¨"åº”æ€¥æ•‘æ´é˜Ÿä¼"ï¼‰
- ç¦æ­¢å°†"é˜²ç«æœŸ"æ ‡æ³¨ä¸º"æ—¶é—´"ï¼ˆåº”ä½¿ç”¨"æ—¶é—´æœŸé™"ï¼‰
- ç¦æ­¢å°†"ç«é™©é¢„è­¦"æ ‡æ³¨ä¸º"é¢„è­¦"ï¼ˆåº”ä½¿ç”¨"é¢„è­¦ä¿¡æ¯"ï¼‰
- ç¦æ­¢ç®€åŒ–å…³ç³»è¡¨è¾¾ï¼Œå¦‚"è´Ÿè´£"åº”å®Œæ•´æ ‡æ³¨ä¸º"XXXè´Ÿè´£XXX"

âœ… **æ£®æ—ç«ç¾é¢†åŸŸæ­£ç¡®ç¤ºä¾‹**ï¼š
å®ä½“æ ‡ç­¾ï¼š
- "ã€Šæ£®æ—é˜²ç«æ¡ä¾‹ã€‹ç¬¬äºŒåä¸‰æ¡" â†’ "æ³•å¾‹æ¡æ¬¾"
- "å›½å®¶æ£®æ—è‰åŸé˜²ç­ç«æŒ‡æŒ¥éƒ¨" â†’ "æ”¿åºœæœºæ„"  
- "æ£®æ—ç«é™©é»„è‰²é¢„è­¦" â†’ "é¢„è­¦ä¿¡æ¯"
- "ä¸“ä¸šæ£®æ—æ¶ˆé˜²é˜Ÿä¼" â†’ "åº”æ€¥æ•‘æ´é˜Ÿä¼"

å…³ç³»æ ‡ç­¾ï¼š
- "æ ¹æ®ã€Šæ£®æ—é˜²ç«æ¡ä¾‹ã€‹ç¬¬åæ¡è§„å®š" â†’ "ä¾æ®å…³ç³»"
- "å¿æ”¿åºœè´Ÿè´£æœ¬è¡Œæ”¿åŒºåŸŸæ£®æ—é˜²ç«å·¥ä½œ" â†’ "è´£ä»»å…³ç³»"
- "å¤§é£å¤©æ°”å¯¼è‡´ç«åŠ¿è”“å»¶" â†’ "å› æœå…³ç³»"  
- "æ—ä¸šéƒ¨é—¨ä¸æ¶ˆé˜²éƒ¨é—¨ååŒä½œæˆ˜" â†’ "åè°ƒå…³ç³»"
- "ç«æƒ…å‘ç”Ÿåç«‹å³å¯åŠ¨åº”æ€¥é¢„æ¡ˆ" â†’ "æ—¶åºå…³ç³»"

ğŸ¯ **æå–ç›®æ ‡**ï¼šæ„å»ºæ£®æ—ç«ç¾é¢†åŸŸçš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼Œæ”¯æ’‘æ™ºèƒ½å†³ç­–å’Œåº”æ€¥å“åº”ï¼"""
        
        # è°ƒç”¨API
        api_response = self._call_modelscope_api(prompt)
        
        if api_response and api_response.strip():
            return self._format_prediction(api_response, task)
        
        # APIè°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºå“åº”
        print("âŒ APIè°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºå“åº”")
        return None
    
    def _call_modelscope_api(self, prompt: str) -> Optional[str]:
        """ğŸ”¥ æ£®æ—ç«ç¾ä¸“ç”¨ï¼šè°ƒç”¨é­”å¡”ç¤¾åŒºAPIï¼ˆæ”¯æŒæ™ºèƒ½æ¨¡å‹åˆ‡æ¢ï¼‰"""
        max_retries_per_model = 2  # æ¯ä¸ªæ¨¡å‹æœ€å¤šé‡è¯•2æ¬¡å†åˆ‡æ¢
        
        for attempt in range(max_retries_per_model):
            # ç¡®ä¿APIè¿æ¥å¯ç”¨
            if not self._ensure_api_connection():
                self._handle_model_failure("è¿æ¥å¤±è´¥")
                if self._has_more_models_to_try():
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                return None
            
            try:
                print(f"ğŸ”¥ è°ƒç”¨æ£®æ—ç«ç¾æ¨¡å‹: {self.model_name} (å°è¯• {attempt + 1}/{max_retries_per_model})")
                
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ğŸ”¥ You are a specialized Knowledge Extraction Expert for Forest Fire Management domain. MISSION: Extract entities and relationships from forest fire laws, emergency plans, and technical standards. CRITICAL REQUIREMENTS: 1) Extract ALL forest fire domain entities (legal clauses, emergency procedures, fire prevention equipment, organizational structures, etc.) 2) Extract complete relational expressions showing legal dependencies, responsibilities, causal relationships, and procedural flows 3) Use EXACT label names - no variations, abbreviations, or descriptions 4) Focus on forest fire prevention, suppression, emergency response, and legal compliance 5) Always output valid JSON with precise character-level positioning. DOMAIN EXPERTISE: Forest fire risk levels, fire suppression tactics, emergency command systems, legal responsibilities, and technical standards."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.1,
                    top_p=0.9,
                    stream=False
                )
                
                if response.choices and len(response.choices) > 0:
                    content = response.choices[0].message.content
                    if content and content.strip():
                        # APIè°ƒç”¨æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                        self._handle_model_success()
                        return content
                    else:
                        print(f"âš ï¸ æ£®æ—ç«ç¾æ¨¡å‹ {self.model_name} è¿”å›ç©ºå†…å®¹")
                        # ç©ºå†…å®¹ä¹Ÿç®—å¤±è´¥
                        self._handle_model_failure("ç©ºå“åº”")
                else:
                    print(f"âš ï¸ æ£®æ—ç«ç¾æ¨¡å‹ {self.model_name} å“åº”æ ¼å¼å¼‚å¸¸")
                    self._handle_model_failure("æ ¼å¼å¼‚å¸¸")
                    
            except Exception as e:
                error_str = str(e)
                print(f"âŒ æ£®æ—ç«ç¾æ¨¡å‹ {self.model_name} APIè°ƒç”¨å¼‚å¸¸: {error_str[:100]}")
                
                # ğŸš¨ æ£€æŸ¥ç‰¹æ®Šé”™è¯¯ç±»å‹ï¼Œç«‹å³åˆ‡æ¢æ¨¡å‹
                should_switch_immediately = self._should_switch_immediately(error_str)
                
                if should_switch_immediately:
                    print(f"ğŸ”¥ æ£€æµ‹åˆ°éœ€è¦ç«‹å³åˆ‡æ¢çš„æ£®æ—ç«ç¾æ¨¡å‹é”™è¯¯: {self._get_error_type(error_str)}")
                    self._handle_model_failure("ç«‹å³åˆ‡æ¢", force_switch=True)
                    if self._has_more_models_to_try():
                        break  # ç«‹å³è·³å‡ºé‡è¯•å¾ªç¯ï¼Œåˆ‡æ¢æ¨¡å‹
                    else:
                        print("âŒ æ‰€æœ‰æ£®æ—ç«ç¾æ¨¡å‹éƒ½å·²å°è¯•å¤±è´¥")
                        return None
                else:
                    self._handle_model_failure(f"APIå¼‚å¸¸: {self._get_error_type(error_str)}")
            
            # å¦‚æœå½“å‰æ¨¡å‹å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢
            if self.model_consecutive_failures >= self.max_model_failures:
                if self._has_more_models_to_try():
                    print(f"ğŸ”¥ è¾¾åˆ°å¤±è´¥é˜ˆå€¼ï¼Œåˆ‡æ¢æ£®æ—ç«ç¾æ¨¡å‹...")
                    break  # è·³å‡ºé‡è¯•å¾ªç¯ï¼Œåˆ‡æ¢æ¨¡å‹
                else:
                    print("âŒ æ‰€æœ‰æ£®æ—ç«ç¾æ¨¡å‹éƒ½å·²å°è¯•å¤±è´¥")
                    return None
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›None
        return None
    
    def _has_more_models_to_try(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–æ¨¡å‹å¯ä»¥å°è¯•"""
        return len(self.available_models) > 1
        
    def _print_model_statistics(self):
        """æ‰“å°æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ¤– æ¨¡å‹ä½¿ç”¨æƒ…å†µ:")
        print(f"   å½“å‰æ¨¡å‹: {self.model_name}")
        print(f"   å½“å‰å¤±è´¥æ¬¡æ•°: {self.model_consecutive_failures}/{self.max_model_failures}")
        
        if self.model_failure_history:
            print(f"   æ¨¡å‹å¤±è´¥å†å²:")
            for model, failures in self.model_failure_history.items():
                if failures > 0:
                    model_short = model.split('/')[-1] if '/' in model else model
                    print(f"     â€¢ {model_short}: {failures} æ¬¡å¤±è´¥")
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
        print(f"   å¯ç”¨æ¨¡å‹: {len(self.available_models)} ä¸ª")
        for i, model in enumerate(self.available_models):
            status = "ğŸ¯" if i == self.current_model_index else "ğŸ’¤"
            model_short = model.split('/')[-1] if '/' in model else model
            print(f"     {status} {model_short}")
    
    def get_model_status(self) -> Dict:
        """è·å–æ¨¡å‹çŠ¶æ€ä¿¡æ¯ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
        return {
            "current_model": self.model_name,
            "current_model_index": self.current_model_index,
            "consecutive_failures": self.model_consecutive_failures,
            "max_failures": self.max_model_failures,
            "available_models": self.available_models,
            "failure_history": self.model_failure_history.copy()
        }
    
    def _format_prediction(self, api_response: str, task: Dict) -> Dict:
        """æ ¼å¼åŒ–é¢„æµ‹ç»“æœä¸ºLabel Studioæ ¼å¼"""
        
        prediction = {
            "model_version": self.get("model_version"),
            "score": 0.95,
            "result": []
        }
        
        # å°è¯•è§£æNERç»“æœ
        ner_results = self._parse_ner_response(api_response, task)
        if ner_results and len(ner_results) > 0:
            prediction["result"] = ner_results
            prediction["score"] = 0.95
            return prediction
        
        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½•å®ä½“ï¼Œè¿”å›å¤±è´¥ä¿¡æ¯
        prediction["score"] = 0.0
        prediction["result"] = []
        return None
    
    def _parse_ner_response(self, api_response: str, task: Dict) -> Optional[List[Dict]]:
        """è§£æAIè¿”å›çš„å‘½åå®ä½“è¯†åˆ«JSONç»“æœï¼Œå¹¶ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¡¥å……"""
        
        # è·å–åŸå§‹æ–‡æœ¬
        task_data = task.get('data', {})
        original_text = ""
        for key in ['text', 'content', 'prompt', 'question', 'description', 'query']:
            if key in task_data and isinstance(task_data[key], str):
                original_text = task_data[key]
                break
        
        if not original_text:
            return None
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        results = []
        ai_entities = []
        
        # ç¬¬ä¸€æ­¥ï¼šè§£æAIæ¨¡å‹çš„è¯†åˆ«ç»“æœ
        if api_response and api_response.strip():
            ai_entities = self._parse_ai_entities(api_response, original_text)
            if ai_entities:
                results.extend(ai_entities)
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¡¥å……è¯†åˆ«
        regex_entities = self._extract_regex_entities(original_text, ai_entities)
        if regex_entities:
            results.extend(regex_entities)
        
        # ç¬¬ä¸‰æ­¥ï¼šå»é‡å’Œæ’åº
        final_results = self._deduplicate_entities(results)
        
        return final_results if final_results else None
    
    def _parse_ai_entities(self, api_response: str, original_text: str) -> List[Dict]:
        """è§£æAIæ¨¡å‹è¿”å›çš„å®ä½“"""
        ai_results = []
        
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                ner_data = json.loads(api_response.strip())
            except json.JSONDecodeError as e:
                # å°è¯•æå–JSONéƒ¨åˆ†
                import re
                
                # å¤šç§JSONæå–ç­–ç•¥
                patterns = [
                    r'\{[^{}]*"entities"[^{}]*:.*?\}',  # æœ€ä¸¥æ ¼çš„entitiesåŒ¹é…
                    r'\{.*?"entities".*?\}',            # å®½æ¾çš„entitiesåŒ¹é…
                    r'\{.*\}',                          # æœ€å®½æ¾çš„JSONåŒ¹é…
                ]
                
                ner_data = None
                for pattern in patterns:
                    json_match = re.search(pattern, api_response, re.DOTALL)
                    if json_match:
                        try:
                            ner_data = json.loads(json_match.group())
                            break
                        except json.JSONDecodeError:
                            continue
                
                if not ner_data:
                    return ai_results
            
            # æ£€æŸ¥entitieså­—æ®µ
            if 'entities' not in ner_data or not isinstance(ner_data['entities'], list):
                return ai_results
            
            entities = ner_data['entities']
            
            # è½¬æ¢ä¸ºLabel Studioæ ¼å¼
            for entity in entities:
                # éªŒè¯å¿…éœ€å­—æ®µ
                if not all(key in entity for key in ['text', 'start', 'end', 'label']):
                    continue
                
                start = entity['start']
                end = entity['end']
                text = entity['text']
                original_label = entity['label']
                
                # ä¸¥æ ¼éªŒè¯æ ‡ç­¾
                validated_label = validate_label(original_label)
                if not validated_label:
                    continue
                
                # ä½¿ç”¨éªŒè¯é€šè¿‡çš„æ ‡ç­¾
                label = validated_label
                
                # éªŒè¯ä½ç½®ä¿¡æ¯åŸºæœ¬åˆç†æ€§
                if not isinstance(start, int) or not isinstance(end, int) or start < 0:
                    continue
                
                # å…ˆå°è¯•ä¿®æ­£ä½ç½®ï¼Œå†è¿›è¡ŒèŒƒå›´æ£€æŸ¥
                corrected_start, corrected_end, corrected_text = self._correct_entity_position(
                    original_text, text, start, end
                )
                
                # æ£€æŸ¥ä¿®æ­£åçš„ä½ç½®æ˜¯å¦åˆç†
                if corrected_start is None or corrected_end is None or corrected_text is None:
                    continue
                
                # éªŒè¯ä¿®æ­£åçš„ä½ç½®ä¸è¶…å‡ºæ–‡æœ¬é•¿åº¦
                if corrected_end > len(original_text) or corrected_start < 0:
                    continue
                
                if corrected_text:
                    # éªŒè¯ä¿®æ­£åçš„å®ä½“æ˜¯å¦åˆç†
                    if self._is_valid_entity(corrected_text, validated_label):
                        result = {
                            "from_name": "label",
                            "to_name": "text",
                            "type": "labels",
                            "value": {
                                "start": corrected_start,
                                "end": corrected_end,
                                "text": corrected_text,
                                "labels": [label]
                            },
                            "source": "ai"  # æ ‡è®°æ¥æºä¸ºAI
                        }
                        
                        ai_results.append(result)
            
            return ai_results
            
        except Exception:
            return ai_results
    
    def _extract_regex_entities(self, original_text: str, existing_entities: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¡¥å……è¯†åˆ«å®ä½“"""
        regex_results = []
        
        try:
            # è·å–å·²è¯†åˆ«å®ä½“çš„ä½ç½®èŒƒå›´ï¼Œé¿å…é‡å¤
            existing_ranges = set()
            for entity in existing_entities:
                value = entity.get('value', {})
                start = value.get('start', -1)
                end = value.get('end', -1)
                if start >= 0 and end > start:
                    # æ‰©å±•èŒƒå›´ä»¥é¿å…é‡å 
                    for pos in range(max(0, start-1), min(len(original_text), end+1)):
                        existing_ranges.add(pos)
            
            # ä»entity_configè·å–æ­£åˆ™æ¨¡å¼
            from entity_config import get_entity_config
            entity_config = get_entity_config()
            
            # éå†æ‰€æœ‰é…ç½®çš„å®ä½“ç±»å‹
            for label_key, config in entity_config.items():
                if 'patterns' not in config or not config['patterns']:
                    continue
                
                patterns = config['patterns']
                description = config['description']
                
                # å¯¹æ¯ä¸ªæ­£åˆ™æ¨¡å¼è¿›è¡ŒåŒ¹é…
                for pattern in patterns:
                    try:
                        import re
                        matches = re.finditer(pattern, original_text)
                        
                        for match in matches:
                            start = match.start()
                            end = match.end()
                            text = match.group()
                            
                            # æ£€æŸ¥æ˜¯å¦ä¸å·²è¯†åˆ«çš„å®ä½“é‡å 
                            overlapping = any(pos in existing_ranges for pos in range(start, end))
                            if overlapping:
                                continue
                            
                            # éªŒè¯å®ä½“æ˜¯å¦åˆç†
                            if self._is_valid_entity(text, label_key):
                                result = {
                                    "from_name": "label",
                                    "to_name": "text",
                                    "type": "labels",
                                    "value": {
                                        "start": start,
                                        "end": end,
                                        "text": text,
                                        "labels": [label_key]  # ä½¿ç”¨æ ‡ç­¾é”®åè€Œä¸æ˜¯description
                                    },
                                    "source": "regex"  # æ ‡è®°æ¥æºä¸ºæ­£åˆ™
                                }
                                
                                regex_results.append(result)
                                
                                # æ›´æ–°å·²è¯†åˆ«èŒƒå›´
                                for pos in range(start, end):
                                    existing_ranges.add(pos)
                                
                    except re.error:
                        continue
            
            return regex_results
            
        except Exception:
            return regex_results
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """å»é‡å’Œæ’åºå®ä½“"""
        
        # æŒ‰èµ·å§‹ä½ç½®æ’åº
        sorted_entities = sorted(entities, key=lambda x: x.get('value', {}).get('start', 0))
        
        # å»é‡é€»è¾‘ï¼šå¦‚æœä¸¤ä¸ªå®ä½“ä½ç½®é‡å è¶…è¿‡50%ï¼Œä¿ç•™ç½®ä¿¡åº¦é«˜çš„
        deduplicated = []
        
        for current in sorted_entities:
            current_value = current.get('value', {})
            current_start = current_value.get('start', 0)
            current_end = current_value.get('end', 0)
            current_text = current_value.get('text', '')
            current_source = current.get('source', 'unknown')
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æ·»åŠ çš„å®ä½“é‡å 
            should_add = True
            for i, existing in enumerate(deduplicated):
                existing_value = existing.get('value', {})
                existing_start = existing_value.get('start', 0)
                existing_end = existing_value.get('end', 0)
                existing_text = existing_value.get('text', '')
                existing_source = existing.get('source', 'unknown')
                
                # è®¡ç®—é‡å åº¦
                overlap_start = max(current_start, existing_start)
                overlap_end = min(current_end, existing_end)
                
                if overlap_start < overlap_end:  # æœ‰é‡å 
                    overlap_length = overlap_end - overlap_start
                    current_length = current_end - current_start
                    existing_length = existing_end - existing_start
                    
                    # è®¡ç®—é‡å æ¯”ä¾‹ï¼ˆç›¸å¯¹äºè¾ƒçŸ­çš„å®ä½“ï¼‰
                    min_length = min(current_length, existing_length)
                    overlap_ratio = overlap_length / min_length if min_length > 0 else 0
                    
                    if overlap_ratio > 0.5:  # é‡å è¶…è¿‡50%
                        
                        # ä¼˜å…ˆçº§ï¼šAI > æ­£åˆ™ï¼Œé•¿å®ä½“ > çŸ­å®ä½“
                        should_replace = False
                        if current_source == 'ai' and existing_source == 'regex':
                            should_replace = True
                        elif current_source == existing_source and current_length > existing_length:
                            should_replace = True
                        
                        if should_replace:
                            deduplicated[i] = current
                        
                        should_add = False
                        break
            
            if should_add:
                deduplicated.append(current)
        
        # æœ€ç»ˆæŒ‰ä½ç½®æ’åº
        final_results = sorted(deduplicated, key=lambda x: x.get('value', {}).get('start', 0))
        
        # ç§»é™¤sourceæ ‡è®°ï¼ˆLabel Studioä¸éœ€è¦ï¼‰
        for result in final_results:
            result.pop('source', None)
        
        return final_results
    
    def _correct_entity_position(self, original_text: str, entity_text: str, start: int, end: int) -> tuple:
        """ä¿®æ­£å®ä½“ä½ç½®"""
        # é¦–å…ˆæ£€æŸ¥åŸå§‹ä½ç½®æ˜¯å¦æ­£ç¡®
        if start < len(original_text) and end <= len(original_text):
            extracted = original_text[start:end]
            if extracted == entity_text:
                return start, end, entity_text
        
        # æ¸…ç†å®ä½“æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        clean_entity = entity_text.strip()
        if not clean_entity:
            return None, None, None
        
        # åœ¨åŸæ–‡ä¸­æœç´¢å®ä½“æ–‡æœ¬
        try:
            # å°è¯•ç²¾ç¡®åŒ¹é…
            exact_start = original_text.find(clean_entity)
            if exact_start != -1:
                exact_end = exact_start + len(clean_entity)
                return exact_start, exact_end, clean_entity
            
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
            import re
            clean_text_for_search = re.sub(r'[^\w\u4e00-\u9fff]', '', clean_entity)
            if len(clean_text_for_search) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦æ‰è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                for i in range(len(original_text) - len(clean_text_for_search) + 1):
                    slice_text = original_text[i:i + len(clean_text_for_search)]
                    clean_slice = re.sub(r'[^\w\u4e00-\u9fff]', '', slice_text)
                    if clean_slice == clean_text_for_search:
                        return i, i + len(clean_text_for_search), slice_text
            
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
            if len(clean_entity) >= 3:
                core_part = clean_entity[:min(len(clean_entity), 5)]  # å–å‰å‡ ä¸ªå­—ç¬¦ä½œä¸ºæ ¸å¿ƒ
                core_start = original_text.find(core_part)
                if core_start != -1:
                    # å°è¯•æ‰©å±•åŒ¹é…
                    extended_end = min(core_start + len(clean_entity) + 2, len(original_text))
                    extended_text = original_text[core_start:extended_end]
                    return core_start, extended_end, extended_text
            
        except Exception:
            pass
        
        return None, None, None
    
    def _is_valid_entity(self, text: str, label: str) -> bool:
        """ğŸ”¥ æ£®æ—ç«ç¾é¢†åŸŸä¸“ç”¨å®ä½“éªŒè¯"""
        if not text or len(text.strip()) < 1:
            return False
        
        # å»é™¤é¦–å°¾æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        clean_text = text.strip()
        
        # ä¸èƒ½åªæ˜¯æ ‡ç‚¹ç¬¦å·
        import re
        if re.match(r'^[^\w\u4e00-\u9fff]+$', clean_text):
            return False
        
        # åŸºç¡€é•¿åº¦éªŒè¯
        if len(clean_text) < 1:
            return False
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦æœ‰æ•ˆ
        if label not in ENTITY_LABELS:
            return False
        
        # ğŸ”¥ æ£®æ—ç«ç¾é¢†åŸŸç‰¹æ®ŠéªŒè¯
        
        # 1. å…³ç³»æ ‡ç­¾éªŒè¯ - æ£®æ—ç«ç¾ä¸“ä¸šå…³ç³»è¯æ±‡
        if label.endswith("å…³ç³»"):
            # æ£®æ—é˜²ç«å…³ç³»å…³é”®è¯
            forest_fire_relations = [
                # æ³•å¾‹ä¾æ®å…³ç³»
                'æ ¹æ®', 'ä¾æ®', 'æŒ‰ç…§', 'éµå¾ª', 'æ‰§è¡Œ', 'å®æ–½', 'è½å®',
                # è´£ä»»ç®¡è¾–å…³ç³»  
                'è´Ÿè´£', 'ä¸»ç®¡', 'ç®¡è¾–', 'æ‰¿æ‹…', 'å±¥è¡Œ', 'ç›‘ç£', 'æŒ‡å¯¼',
                # å› æœå½±å“å…³ç³»
                'å¯¼è‡´', 'é€ æˆ', 'å¼•èµ·', 'å¼•å‘', 'äº§ç”Ÿ', 'å½¢æˆ', 'å½±å“', 'æ³¢åŠ',
                # æ—¶åºæµç¨‹å…³ç³»
                'ä¹‹å‰', 'ä¹‹å', 'åŒæ—¶', 'éšå³', 'ç«‹å³', 'åŠæ—¶', 'è¿…é€Ÿ', 'å¯åŠ¨',
                # åè°ƒé…åˆå…³ç³»
                'åè°ƒ', 'é…åˆ', 'è”åˆ', 'ååŒ', 'é…åˆ', 'ä¼šåŒ', 'ç»Ÿä¸€',
                # åŒ…å«å½’å±å…³ç³»
                'åŒ…æ‹¬', 'åŒ…å«', 'å±äº', 'éš¶å±', 'ä¸‹è®¾', 'è®¾ç½®', 'å»ºç«‹',
                # æ£®æ—é˜²ç«ä¸“ä¸šå…³ç³»
                'é¢„è­¦', 'ç›‘æµ‹', 'å·¡æŠ¤', 'æ‰‘æ•‘', 'é˜²æ§', 'ç®¡æ§', 'ç¦æ­¢'
            ]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ£®æ—é˜²ç«å…³ç³»è¯æ±‡
            if not any(keyword in clean_text for keyword in forest_fire_relations):
                # å¯¹äºå…³ç³»æ ‡ç­¾ï¼Œå¦‚æœä¸åŒ…å«ä¸“ä¸šå…³ç³»è¯ï¼Œä½†æ–‡æœ¬åˆç†ï¼Œä»ç„¶æ¥å—
                if len(clean_text) >= 2 and not re.match(r'^[^\w\u4e00-\u9fff]+$', clean_text):
                    pass  # æ¥å—
                else:
                    return False
        
        # 2. æ£®æ—ç«ç¾ä¸“ä¸šå®ä½“éªŒè¯
        elif label in ['æ³•å¾‹æ¡æ¬¾', 'æ³•å¾‹æ³•è§„']:
            # æ³•å¾‹æ¡æ–‡ç‰¹å¾éªŒè¯
            legal_patterns = ['ç¬¬.*æ¡', 'ç¬¬.*ç« ', 'ç¬¬.*èŠ‚', 'æ¡ä¾‹', 'æ³•', 'è§„å®š', 'åŠæ³•', 'æ ‡å‡†']
            if any(pattern in clean_text for pattern in legal_patterns):
                return True
            # å³ä½¿ä¸åŒ¹é…æ¨¡å¼ï¼Œå¦‚æœæ–‡æœ¬åˆç†ä¹Ÿæ¥å—
            
        elif label in ['æ”¿åºœæœºæ„', 'åº”æ€¥æ•‘æ´é˜Ÿä¼']:
            # ç»„ç»‡æœºæ„ç‰¹å¾éªŒè¯
            org_keywords = ['éƒ¨', 'å±€', 'å§”', 'ç½²', 'å…', 'å¤„', 'ä¸­å¿ƒ', 'é˜Ÿ', 'ç»„', 'æŒ‡æŒ¥éƒ¨', 'æ”¿åºœ', 'æ¶ˆé˜²']
            if any(keyword in clean_text for keyword in org_keywords):
                return True
                
        elif label in ['é¢„è­¦ä¿¡æ¯', 'ç¾å®³ç­‰çº§']:
            # é¢„è­¦ç­‰çº§ç‰¹å¾éªŒè¯
            warning_keywords = ['é¢„è­¦', 'è­¦æŠ¥', 'ç­‰çº§', 'çº§åˆ«', 'è“è‰²', 'é»„è‰²', 'æ©™è‰²', 'çº¢è‰²', 'ä¸€çº§', 'äºŒçº§', 'ä¸‰çº§', 'å››çº§', 'äº”çº§']
            if any(keyword in clean_text for keyword in warning_keywords):
                return True
                
        elif label in ['æ—¶é—´æœŸé™', 'åœ°ç†åŒºåŸŸ']:
            # æ—¶é—´åœ°ç‚¹ç‰¹å¾éªŒè¯
            time_geo_keywords = ['å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'æœŸ', 'çœ', 'å¸‚', 'å¿', 'åŒº', 'é•‡', 'æ‘', 'æ—åŒº', 'å±±åŒº']
            if any(keyword in clean_text for keyword in time_geo_keywords):
                return True
        
        # 3. åŸºç¡€æ–‡æœ¬è´¨é‡éªŒè¯
        # è¿‡æ»¤æ‰è¿‡çŸ­æˆ–æ— æ„ä¹‰çš„æ–‡æœ¬
        if len(clean_text) < 2 and label not in ['æ•°å€¼', 'é‡‘é¢']:
            return False
            
        # æ£®æ—ç«ç¾ä¸“ä¸šæœ¯è¯­ç™½åå•ï¼ˆå³ä½¿å¾ˆçŸ­ä¹Ÿæ¥å—ï¼‰
        forest_fire_terms = [
            'ç«æƒ…', 'ç«æº', 'ç«ç‚¹', 'ç«åœº', 'ç«çº¿', 'ç«å¤´', 'ç«å°¾', 'ç«åŠ¿',
            'æ‰‘ç«', 'ç­ç«', 'é˜²ç«', 'ç”¨ç«', 'çƒ§é™¤', 'æ¸…ç†', 'å·¡æŠ¤', 'ç­æœ›',
            'é£å‘', 'é£é€Ÿ', 'æ¹¿åº¦', 'æ¸©åº¦', 'é™æ°´', 'å¹²æ—±', 'é«˜æ¸©', 'å¤§é£'
        ]
        
        if clean_text in forest_fire_terms:
            return True
        
        return True  # é»˜è®¤æ¥å—ï¼Œç”±ä¸Šå±‚é€»è¾‘è¿›ä¸€æ­¥è¿‡æ»¤
    
    
    def fit(self, event, data, **kwargs):
        """è®­ç»ƒ/æ›´æ–°æ¨¡å‹"""
        self.set('my_data', 'updated_data')
        self.set('model_version', 'updated_version')
        print(f"âœ… æ¨¡å‹å·²æ›´æ–° ({event})")

