from typing import List, Dict, Optional, Tuple
import json
import os
import base64
import cv2
import numpy as np
import hashlib
import time
from openai import OpenAI
from label_studio_ml.model import LabelStudioMLBase
from label_studio_ml.response import ModelResponse


# ==================== é…ç½® ====================
SUPPORTED_VIDEO_FORMATS = ['mp4', 'avi', 'mov', 'wmv', 'flv', 'webm', 'mkv']
LABEL_STUDIO_MEDIA_DIR = os.getenv('LABEL_STUDIO_MEDIA_DIR', r'C:\Users\Administrator\AppData\Local\label-studio\label-studio\media')

# ç®€åŒ–çš„è§†é¢‘å¤„ç†é…ç½®

# è§†é¢‘æŠ½å¸§é…ç½®
FRAME_EXTRACTION_CONFIG = {
    "strategy": "time_based",
    "max_frames": 3,
    "min_frames": 3,
    "time_interval": 5.0,
    "assumed_fps": 30.0,
    "quality_factor": 0.85,
    "serial_processing": True,
    "frame_processing_delay": 0.5,
}

# ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®
TARGET_LABELS = {
    "Floods": "æ´ªæ°´åŒºåŸŸ - è¢«æ°´æ·¹æ²¡çš„åœ°é¢ã€è¡—é“æˆ–åŒºåŸŸ",
    "Affected objects": "å—å½±å“ç‰©ä½“ - è¢«æ´ªæ°´æŸåã€ç§»ä½æˆ–å½±å“çš„ç‰©å“ï¼Œå¦‚è½¦è¾†ã€æ ‘æœ¨ã€ç”µçº¿æ†ç­‰",
    "personnel": "äººå‘˜ - å‡ºç°åœ¨åœºæ™¯ä¸­çš„äºº",
    "building": "å»ºç­‘ç‰© - æˆ¿å±‹ã€å»ºç­‘ç»“æ„"
}


class NewModel(LabelStudioMLBase):
    """ç²¾ç®€ç‰ˆè§†é¢‘ç›®æ ‡è·Ÿè¸ªML Backendæ¨¡å‹"""
    
    def setup(self):
        """åˆå§‹åŒ–é…ç½®"""
        print(f"\nğŸš€ è§†é¢‘ç›®æ ‡è·Ÿè¸ªML Backendå¯åŠ¨ä¸­...")
        
        self.set("model_version", "2.0.0-simplified")
        
        # APIé…ç½®
        self.api_key = os.getenv('MODELSCOPE_API_KEY', 'ms-d200fd06-f07f-4be8-a6a8-9ebf76dd103a')
        self.api_base_url = os.getenv('MODELSCOPE_API_URL', 'https://api-inference.modelscope.cn/v1')
        self.model_name = "Qwen/Qwen2.5-VL-72B-Instruct"
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client()
        self._show_config()
        
    def _init_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        if self.api_key:
            try:
                self.client = OpenAI(
                    base_url=self.api_base_url,
                    api_key=self.api_key
                )
                print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.model_name}")
            except Exception as e:
                print(f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
        else:
            print(f"âš ï¸ è¯·è®¾ç½®MODELSCOPE_API_KEYç¯å¢ƒå˜é‡")
            self.client = None
        
    def _show_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        config = FRAME_EXTRACTION_CONFIG
        serial_mode = config.get("serial_processing", True)
        delay = config.get("frame_processing_delay", 0.5)
        
        print(f"\nğŸ¬ è§†é¢‘æŠ½å¸§å¤„ç†é…ç½®:")
        print(f"ğŸ“Š æŠ½å¸§ç­–ç•¥: {config['strategy']}")
        print(f"ğŸ”¢ æœ€å¤§å¸§æ•°: {config['max_frames']}")
        print(f"ğŸ”„ å¤„ç†æ¨¡å¼: {'ğŸ”— ä¸²è¡Œå¤„ç†' if serial_mode else 'ğŸ“¦ æ‰¹é‡å¤„ç†'}")
        if serial_mode:
            print(f"â±ï¸ å¸§é—´å»¶è¿Ÿ: {delay}ç§’")
    
    def _extract_video_frames(self, video_path: str) -> Tuple[List[Dict], Dict]:
        """ç²¾ç®€ç‰ˆè§†é¢‘æŠ½å¸§æ–¹æ³•ã€‚è¿”å›(frames_data, video_info)"""
        frames_data = []
        video_info = {}
        
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
                return frames_data
            
            # è·å–è§†é¢‘ä¿¡æ¯
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS) or FRAME_EXTRACTION_CONFIG["assumed_fps"]
            duration = total_frames / fps if fps > 0 else 0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # ä¿å­˜è§†é¢‘ä¿¡æ¯
            video_info = {
                "total_frames": total_frames,
                "fps": fps,
                "duration": duration,
                "width": width,
                "height": height
            }
            
            print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}FPS, {duration:.1f}ç§’, å°ºå¯¸:{width}x{height}")
            
            # è®¡ç®—æŠ½å¸§è®¡åˆ’
            frame_indices = self._calculate_frame_indices(total_frames, fps, duration)
            
            if not frame_indices:
                print("âŒ æ— æ³•ç”ŸæˆæŠ½å¸§è®¡åˆ’")
                cap.release()
                return frames_data, video_info
            
            print(f"ğŸ” æŠ½å¸§è®¡åˆ’: æŠ½å– {len(frame_indices)} å¸§")
            
            # æŠ½å–å¸§
            quality = int(FRAME_EXTRACTION_CONFIG["quality_factor"] * 100)
            
            for i, target_frame in enumerate(frame_indices):
                # è·³è½¬åˆ°ç›®æ ‡å¸§
                success = self._seek_to_frame(cap, target_frame, fps)
                
                if success:
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        # ç”Ÿæˆæ›´å¼ºçš„å“ˆå¸Œå€¼
                        frame_hash = self._calculate_frame_hash(frame)
                        
                        # è½¬æ¢ä¸ºbase64
                        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, quality])
                        frame_base64 = base64.b64encode(buffer).decode('utf-8')
                        data_url = f"data:image/jpeg;base64,{frame_base64}"
                        
                        # è·å–æ—¶é—´æˆ³
                        actual_frame_pos = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
                        timestamp = actual_frame_pos / fps if fps > 0 else 0
                        
                        frame_data = {
                            "frame_number": target_frame,
                            "actual_frame": actual_frame_pos,
                            "timestamp": timestamp,
                            "data_url": data_url,
                            "frame_hash": frame_hash
                        }
                        
                        frames_data.append(frame_data)
                        print(f"âœ… æŠ½å–å¸§{target_frame} (å®é™…:{actual_frame_pos}) æ—¶é—´:{timestamp:.2f}s å“ˆå¸Œ:{frame_hash[:16]}...")
                    else:
                        print(f"âŒ æ— æ³•è¯»å–ç¬¬ {target_frame} å¸§")
                else:
                    print(f"âŒ æ— æ³•è·³è½¬åˆ°ç¬¬ {target_frame} å¸§")
            
            cap.release()
            
            # æ£€æŸ¥å¸§çš„å¤šæ ·æ€§
            self._check_frame_diversity(frames_data)
            
        except Exception as e:
            print(f"âŒ è§†é¢‘æŠ½å¸§å¤±è´¥: {e}")
        
        return frames_data, video_info
    
    def _calculate_frame_indices(self, total_frames: int, fps: float, duration: float) -> List[int]:
        """è®¡ç®—æŠ½å¸§ç´¢å¼•"""
        config = FRAME_EXTRACTION_CONFIG
        max_frames = config["max_frames"]
        min_frames = config["min_frames"]
        time_interval = config["time_interval"]
        assumed_fps = config["assumed_fps"]
        
        # åŸºäºæ—¶é—´é—´éš”è®¡ç®—
        theoretical_frames = int(duration / time_interval) + 1
        target_frames = min(max_frames, max(min_frames, theoretical_frames))
        
        frame_interval = int(time_interval * assumed_fps)
        
        frame_indices = []
        for i in range(target_frames):
            frame_idx = i * frame_interval
            if frame_idx < total_frames:
                frame_indices.append(frame_idx)
            else:
                break
        
        # å¦‚æœå¸§æ•°ä¸å¤Ÿï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
        if len(frame_indices) < min_frames and total_frames >= min_frames:
            target_frames = min(max_frames, max(min_frames, theoretical_frames))
            if target_frames >= total_frames:
                frame_indices = list(range(total_frames))
            else:
                interval = total_frames / target_frames
                frame_indices = [int(i * interval) for i in range(target_frames)]
        
        return sorted(list(set(frame_indices)))
    
    def _seek_to_frame(self, cap, target_frame: int, fps: float) -> bool:
        """è·³è½¬åˆ°æŒ‡å®šå¸§"""
        try:
            # æ–¹æ³•1: æŒ‰æ—¶é—´æˆ³è·³è½¬
            target_time_ms = (target_frame / fps) * 1000
            cap.set(cv2.CAP_PROP_POS_MSEC, target_time_ms)
            return True
        except:
            try:
                # æ–¹æ³•2: æŒ‰å¸§è·³è½¬
                cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                return True
            except:
                return False
    
    def _calculate_frame_hash(self, frame) -> str:
        """è®¡ç®—å¸§çš„å¼ºå“ˆå¸Œå€¼"""
        try:
            # ä½¿ç”¨å¤šç§ç‰¹å¾è®¡ç®—æ›´å¼ºçš„å“ˆå¸Œ
            # 1. å›¾åƒå†…å®¹å“ˆå¸Œ
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # 2. è®¡ç®—å›¾åƒçš„ç›´æ–¹å›¾
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            
            # 3. è®¡ç®—è¾¹ç¼˜ç‰¹å¾
            edges = cv2.Canny(gray, 50, 150)
            edge_sum = np.sum(edges)
            
            # 4. è®¡ç®—å›¾åƒç»Ÿè®¡ä¿¡æ¯
            mean_val = np.mean(gray)
            std_val = np.std(gray)
            
            # 5. ç»„åˆç‰¹å¾åˆ›å»ºå”¯ä¸€å“ˆå¸Œ
            feature_string = f"{hist.flatten().tobytes()}{edge_sum}{mean_val:.2f}{std_val:.2f}"
            
            # ä½¿ç”¨SHA256ç”Ÿæˆå“ˆå¸Œ
            hash_obj = hashlib.sha256(feature_string.encode())
            return hash_obj.hexdigest()
            
        except Exception as e:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹å¸§æ•°æ®
            print(f"âš ï¸ é«˜çº§å“ˆå¸Œè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
            return hashlib.md5(frame.tobytes()).hexdigest()
    
    def _check_frame_diversity(self, frames_data: List[Dict]):
        """æ£€æŸ¥æŠ½å–å¸§çš„å¤šæ ·æ€§"""
        if len(frames_data) < 2:
            return
        
        frame_hashes = [f["frame_hash"] for f in frames_data]
        unique_hashes = set(frame_hashes)
        diversity_rate = len(unique_hashes) / len(frames_data)
        
        print(f"ğŸ” æŠ½å¸§å¤šæ ·æ€§åˆ†æ:")
        print(f"  æ€»å¸§æ•°: {len(frames_data)}")
        print(f"  å”¯ä¸€å¸§: {len(unique_hashes)}")
        print(f"  å¤šæ ·æ€§: {diversity_rate:.1%}")
        
        if len(unique_hashes) == 1:
            print("âš ï¸ æ‰€æœ‰å¸§å†…å®¹ç›¸åŒï¼")
        elif diversity_rate < 0.5:
            print("âš ï¸ å¸§å¤šæ ·æ€§è¾ƒä½")
        else:
            print("âœ… å¸§å¤šæ ·æ€§è‰¯å¥½")
    
    def _find_video_file(self, video_url: str) -> Optional[str]:
        """æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶çš„å®é™…è·¯å¾„"""
        print(f"ğŸ” æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶: {video_url}")
        
        # ç”Ÿæˆå¯èƒ½çš„è·¯å¾„
        paths_to_check = []
        
        # 1. æ ‡å‡†Label Studioè·¯å¾„æ ¼å¼: /data/upload/X/filename
        if video_url.startswith('/data/upload/'):
            # ç§»é™¤/data/å‰ç¼€ï¼Œç›´æ¥åœ¨mediaç›®å½•ä¸‹æŸ¥æ‰¾
            relative_path = video_url.replace('/data/', '')
            full_path = os.path.join(LABEL_STUDIO_MEDIA_DIR, relative_path)
            paths_to_check.append(full_path)
            print(f"   å°è¯•è·¯å¾„1: {full_path}")
        
        # 2. ç›´æ¥åœ¨mediaç›®å½•ä¸‹æŸ¥æ‰¾
        paths_to_check.append(os.path.join(LABEL_STUDIO_MEDIA_DIR, video_url.lstrip('/')))
        
        # 3. ç§»é™¤/data/å‰ç¼€çš„ç®€åŒ–è·¯å¾„
        simplified_path = video_url.replace('/data/', '')
        paths_to_check.append(simplified_path)
        
        # 4. åŸå§‹è·¯å¾„
        paths_to_check.append(video_url)
        
        # è¿”å›ç¬¬ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„
        for i, path in enumerate(paths_to_check):
            print(f"   æ£€æŸ¥è·¯å¾„{i+1}: {path}")
            if os.path.exists(path):
                print(f"âœ… æ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {path}")
                return path
        else:
                print(f"   âŒ è·¯å¾„ä¸å­˜åœ¨")
        
        print(f"âŒ æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {video_url}")
        return None
    
    def _call_serial_api(self, prompt: str, video_frames: List[Dict]) -> Optional[str]:
        """ä¸²è¡Œè°ƒç”¨APIåˆ†ææ¯å¸§"""
        if not self.client or not video_frames:
                return None
            
        try:
            print(f"ğŸ”„ å¼€å§‹ä¸²è¡Œå¤„ç† {len(video_frames)} å¸§")
            
            all_frame_results = []
            
            for i, frame_info in enumerate(video_frames):
                frame_number = frame_info['frame_number']
                timestamp = frame_info['timestamp']
                
                print(f"ğŸ“± å¤„ç†ç¬¬{i+1}/{len(video_frames)}å¸§ (å¸§å·:{frame_number}, æ—¶é—´:{timestamp:.2f}s)")
                
                # å•å¸§åˆ†æ
                frame_result = self._analyze_single_frame(frame_info, frame_number, timestamp)
                
                if frame_result:
                    all_frame_results.append(frame_result)
                    print(f"âœ… ç¬¬{i+1}å¸§åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(frame_result.get('objects', []))} ä¸ªå¯¹è±¡")
                
                # æ·»åŠ å»¶è¿Ÿ
                delay = FRAME_EXTRACTION_CONFIG.get("frame_processing_delay", 0.5)
                if delay > 0:
                    time.sleep(delay)
            
            if all_frame_results:
                combined_result = {"video_objects": all_frame_results}
                return json.dumps(combined_result, ensure_ascii=False, indent=2)
            
            return None
    
        except Exception as e:
            print(f"âŒ ä¸²è¡ŒAPIè°ƒç”¨å¤±è´¥: {e}")
            return None
        
    def _generate_labels_description(self) -> str:
        """ç”Ÿæˆå¸¦è¯´æ˜çš„æ ‡ç­¾æè¿°"""
        descriptions = []
        for label, desc in TARGET_LABELS.items():
            descriptions.append(f"â€¢ {label}: {desc}")
        return "\n".join(descriptions)
    
    def _convert_coordinates_to_percentage(self, bbox: List[float], video_info: Dict) -> List[float]:
        """å°†åæ ‡è½¬æ¢ä¸ºç™¾åˆ†æ¯”æ ¼å¼"""
        x1, y1, x2, y2 = bbox
        
        # è·å–è§†é¢‘å°ºå¯¸ç”¨äºåˆ¤æ–­
        width = video_info.get("width", 640)
        height = video_info.get("height", 364)
        
        # æ›´æ™ºèƒ½çš„åæ ‡æ ¼å¼æ£€æµ‹ï¼š
        # 1. å¦‚æœåæ ‡å€¼éƒ½è¶…è¿‡100ï¼Œæ˜æ˜¾æ˜¯åƒç´ åæ ‡
        # 2. å¦‚æœåæ ‡å€¼è¶…è¿‡è§†é¢‘å°ºå¯¸ï¼Œä¹Ÿæ˜¯åƒç´ åæ ‡
        # 3. å¦‚æœxåæ ‡æ¥è¿‘è§†é¢‘å®½åº¦ï¼Œyåæ ‡æ¥è¿‘è§†é¢‘é«˜åº¦ï¼Œæ˜¯åƒç´ åæ ‡
        max_coord = max(bbox)
        is_pixel = (
            max_coord > 100 or  # è¶…è¿‡100%æ˜æ˜¾æ˜¯åƒç´ 
            (width > 0 and max_coord > width * 0.8) or  # æ¥è¿‘è§†é¢‘å®½åº¦
            (height > 0 and max_coord > height * 0.8) or  # æ¥è¿‘è§†é¢‘é«˜åº¦
            any(coord > min(width, height) for coord in bbox if width > 0 and height > 0)  # è¶…è¿‡è¾ƒå°ç»´åº¦
        )
        
        if is_pixel:
            print(f"ğŸ”„ æ£€æµ‹åˆ°åƒç´ åæ ‡ï¼Œè½¬æ¢ä¸ºç™¾åˆ†æ¯”: {bbox}")
            
            if width > 0 and height > 0:
                # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                x1_pct = (x1 / width) * 100
                y1_pct = (y1 / height) * 100
                x2_pct = (x2 / width) * 100
                y2_pct = (y2 / height) * 100
                
                result = [x1_pct, y1_pct, x2_pct, y2_pct]
                print(f"ğŸ“ è½¬æ¢ç»“æœ: {result}")
                return result
            else:
                print(f"âš ï¸ è§†é¢‘å°ºå¯¸ä¿¡æ¯æ— æ•ˆ ({width}x{height})ï¼Œä½¿ç”¨åŸå§‹åæ ‡")
                return bbox
        else:
            # å·²ç»æ˜¯ç™¾åˆ†æ¯”æ ¼å¼
            print(f"âœ… åæ ‡å·²ä¸ºç™¾åˆ†æ¯”æ ¼å¼: {bbox}")
            return bbox
    
    def _analyze_single_frame(self, frame_info: Dict, frame_number: int, timestamp: float) -> Optional[Dict]:
        """åˆ†æå•ä¸ªå¸§"""
        try:
            # åŠ¨æ€ç”ŸæˆåŒ…å«æ ‡ç­¾è¯´æ˜çš„æç¤ºè¯
            labels_desc = self._generate_labels_description()
            prompt = f"""åˆ†æè¿™ä¸ªè§†é¢‘å¸§ï¼Œæ£€æµ‹ä»¥ä¸‹å¯¹è±¡ç±»å‹ï¼š

{labels_desc}

è¦æ±‚ï¼š
1. ä»”ç»†åˆ†æå›¾åƒä¸­çš„æ¯ä¸ªåŒºåŸŸ
2. è¯†åˆ«ä¸Šè¿°ç±»å‹çš„å¯¹è±¡
3. ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„å¯¹è±¡æä¾›å‡†ç¡®çš„è¾¹ç•Œæ¡†åæ ‡

è¿”å›JSONæ ¼å¼ï¼š
{{"frame_objects": [{{"label": "å¯¹è±¡ç±»å‹", "bbox": [x1, y1, x2, y2], "confidence": 0.95}}]}}

æ³¨æ„ï¼š
- åæ ‡å¿…é¡»ä¸ºç™¾åˆ†æ¯”æ ¼å¼ï¼š[å·¦ä¸Šx%, å·¦ä¸Šy%, å³ä¸‹x%, å³ä¸‹y%]
- åæ ‡å€¼èŒƒå›´0-100ï¼Œä¾‹å¦‚ [10.5, 15.2, 45.8, 50.3]
- labelå¿…é¡»å®Œå…¨åŒ¹é…ä¸Šè¿°å®šä¹‰çš„å¯¹è±¡ç±»å‹ä¹‹ä¸€
- confidenceèŒƒå›´0-1ï¼Œè¡¨ç¤ºæ£€æµ‹ç½®ä¿¡åº¦"""
            
            messages = [
                {"role": "system", "content": "You are a helpful assistant specialized in video frame analysis."},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": frame_info["data_url"]}}
                    ]
                }
            ]
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_tokens=1000,
                temperature=0.7,
                stream=False
            )
            
            if response.choices and len(response.choices) > 0:
                content = response.choices[0].message.content
                    
                if content:
                    # è§£æJSONå“åº”
                    import re
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        frame_data = json.loads(json_str)
                        
                        return {
                            "frame_number": frame_number,
                            "timestamp": timestamp,
                            "objects": frame_data.get("frame_objects", [])
                        }
            
            # å¤±è´¥æ—¶è¿”å›ç©ºç»“æœ
            return {
                "frame_number": frame_number,
                "timestamp": timestamp,
                "objects": []
            }
                
        except Exception as e:
            print(f"âŒ å¸§{frame_number}åˆ†æå¤±è´¥: {e}")
            return None
    
    def _format_video_prediction(self, api_response: str, task: Dict, video_info: Dict = None) -> Dict:
        """æ ¼å¼åŒ–è§†é¢‘é¢„æµ‹ç»“æœ"""
        prediction = {
            "model_version": self.get("model_version"),
            "score": 0.95,
            "result": []
        }
        
        try:
            # è§£æAPIå“åº”
            import re
            json_match = re.search(r'\{.*\}', api_response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                tracking_data = json.loads(json_str)
                
                if 'video_objects' in tracking_data:
                    # æ”¶é›†å¯¹è±¡è·Ÿè¸ª
                    object_tracks = {}
                    
                    for frame_data in tracking_data['video_objects']:
                        frame_number = frame_data.get('frame_number', 0)
                        timestamp = frame_data.get('timestamp', 0.0)
                        objects = frame_data.get('objects', [])
                        
                        for obj in objects:
                            if 'label' in obj and 'bbox' in obj:
                                label = obj['label']
                                bbox = obj['bbox']
                                confidence = obj.get('confidence', 0.8)
                                
                                print(f"ğŸ” å¤„ç†å¯¹è±¡: {label}, åæ ‡: {bbox}, ç½®ä¿¡åº¦: {confidence}")
                                
                                # è½¬æ¢åæ ‡ä¸ºç™¾åˆ†æ¯”æ ¼å¼
                                converted_bbox = self._convert_coordinates_to_percentage(bbox, video_info or {})
                                x1, y1, x2, y2 = converted_bbox
                                
                                print(f"ğŸ“ è½¬æ¢ååæ ‡: ({x1:.2f}, {y1:.2f}, {x2:.2f}, {y2:.2f})")
                                
                                # ç¡®ä¿åæ ‡é¡ºåºæ­£ç¡®ï¼ˆå·¦ä¸Šè§’ -> å³ä¸‹è§’ï¼‰
                                if x1 > x2:
                                    x1, x2 = x2, x1
                                if y1 > y2:
                                    y1, y2 = y2, y1
                                
                                # è®¡ç®—Label Studioéœ€è¦çš„æ ¼å¼ï¼šx, y, width, height (ç™¾åˆ†æ¯”)
                                x = x1  # å·¦ä¸Šè§’Xåæ ‡ç™¾åˆ†æ¯”
                                y = y1  # å·¦ä¸Šè§’Yåæ ‡ç™¾åˆ†æ¯”
                                width = x2 - x1  # å®½åº¦ç™¾åˆ†æ¯”
                                height = y2 - y1  # é«˜åº¦ç™¾åˆ†æ¯”
                                
                                print(f"ğŸ¯ è½¬æ¢ååæ ‡: x={x:.1f}%, y={y:.1f}%, width={width:.1f}%, height={height:.1f}%")
                                
                                # ç¡®ä¿åæ ‡åœ¨åˆç†èŒƒå›´å†…
                                x = max(0, min(100, x))
                                y = max(0, min(100, y))
                                width = max(0.1, min(100-x, width))
                                height = max(0.1, min(100-y, height))
                                
                                if label not in object_tracks:
                                    object_tracks[label] = []
                                
                                object_tracks[label].append({
                                    "frame": frame_number,
                                    "enabled": True,
                                    "rotation": 0,
                                    "x": float(x),  # ç¡®ä¿æ˜¯ç™¾åˆ†æ¯” (0-100)
                                    "y": float(y),  # ç¡®ä¿æ˜¯ç™¾åˆ†æ¯” (0-100)
                                    "width": float(width),  # ç¡®ä¿æ˜¯ç™¾åˆ†æ¯” (0-100)
                                    "height": float(height),  # ç¡®ä¿æ˜¯ç™¾åˆ†æ¯” (0-100)
                                    "time": timestamp,
                                    "score": confidence  # æ¯ä¸ªç‚¹çš„ç½®ä¿¡åº¦
                                })
                    
                    # åˆ›å»ºVideoRectangleç»“æœ
                    print(f"ğŸ¯ æ£€æµ‹åˆ°çš„å¯¹è±¡è½¨è¿¹: {list(object_tracks.keys())}")
                    for label, track_sequence in object_tracks.items():
                        track_sequence.sort(key=lambda x: x["frame"])
                        avg_confidence = sum(item["score"] for item in track_sequence) / len(track_sequence)
                        print(f"ğŸ“Š å¯¹è±¡ '{label}' æœ‰ {len(track_sequence)} ä¸ªè½¨è¿¹ç‚¹ï¼Œå¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                        
                        sequence_data = []
                        for item in track_sequence:
                            sequence_data.append({
                                "frame": item["frame"],
                                "enabled": item["enabled"],
                                "rotation": item["rotation"],
                                "x": item["x"],
                                "y": item["y"],
                                "width": item["width"],
                                "height": item["height"],
                                "time": item["time"],
                                "score": item["score"]  # æ·»åŠ æ¯ä¸ªç‚¹çš„ç½®ä¿¡åº¦
                            })
                        
                        # æ·»åŠ è§†é¢‘ä¿¡æ¯
                        value_data = {
                            "sequence": sequence_data,
                            "labels": [label]
                        }
                        
                        # å¦‚æœæœ‰è§†é¢‘ä¿¡æ¯ï¼Œæ·»åŠ durationå’ŒframesCount
                        if video_info:
                            value_data["duration"] = video_info.get("duration", 0)
                            value_data["framesCount"] = video_info.get("total_frames", 0)
                        
                        result_item = {
                            "from_name": "box",
                            "to_name": "video",
                            "type": "videorectangle",
                            "value": value_data,
                            "score": avg_confidence
                        }
                        
                        prediction["result"].append(result_item)
                    
        except Exception as e:
            print(f"âŒ è§£æé¢„æµ‹ç»“æœå¤±è´¥: {e}")
        
        return prediction
    
    def _create_mock_prediction(self, video_frames: List[Dict], task: Dict, video_info: Dict = None) -> Dict:
        """åˆ›å»ºæ¨¡æ‹Ÿçš„é¢„æµ‹ç»“æœï¼ˆç”¨äºAPIä¸å¯ç”¨æ—¶ï¼‰"""
        print("ğŸ­ ç”Ÿæˆæ¨¡æ‹Ÿæ ‡æ³¨ç»“æœ...")
        
        prediction = {
            "model_version": self.get("model_version"),
            "score": 0.85,
            "result": []
        }
        
        try:
            # ä¸ºæ¯ä¸ªæ ‡ç­¾ç±»å‹åˆ›å»ºä¸€ä¸ªè·Ÿè¸ªåºåˆ—
            labels = list(TARGET_LABELS.keys())
            
            for label_idx, label in enumerate(labels):
                if label_idx >= len(video_frames):
                    break  # é™åˆ¶æ ‡ç­¾æ•°é‡ä¸è¶…è¿‡å¸§æ•°
                
                # åˆ›å»ºè·Ÿè¸ªåºåˆ—
                sequence_data = []
                
                for i, frame_info in enumerate(video_frames):
                    frame_number = frame_info['frame_number']
                    timestamp = frame_info['timestamp']
                    
                    # ä¸ºä¸åŒæ ‡ç­¾ç”Ÿæˆä¸åŒä½ç½®çš„æ¨¡æ‹Ÿè¾¹ç•Œæ¡† (ä½¿ç”¨ç™¾åˆ†æ¯” 0-100)
                    base_x = 10.0 + (label_idx * 25.0) % 60.0
                    base_y = 10.0 + (label_idx * 20.0) % 50.0
                    
                    # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–æ¨¡æ‹Ÿç›®æ ‡ç§»åŠ¨
                    variation = i * 2.0
                    x = base_x + variation % 15.0
                    y = base_y + variation % 10.0
                    width = 15.0 + variation % 10.0
                    height = 12.0 + variation % 8.0
                    
                    sequence_data.append({
                        "frame": frame_number,
                        "enabled": True,
                        "rotation": 0,
                        "x": float(x),  # ç™¾åˆ†æ¯”åæ ‡ (0-100)
                        "y": float(y),  # ç™¾åˆ†æ¯”åæ ‡ (0-100)
                        "width": float(width),  # ç™¾åˆ†æ¯”å®½åº¦ (0-100)
                        "height": float(height),  # ç™¾åˆ†æ¯”é«˜åº¦ (0-100)
                        "time": timestamp,
                        "score": 0.85 - (label_idx * 0.05) + (i * 0.01)  # æ¨¡æ‹Ÿç½®ä¿¡åº¦å˜åŒ–
                    })
                
                # åˆ›å»ºVideoRectangleç»“æœ
                value_data = {
                    "sequence": sequence_data,
                    "labels": [label]
                }
                
                # å¦‚æœæœ‰è§†é¢‘ä¿¡æ¯ï¼Œæ·»åŠ durationå’ŒframesCount
                if video_info:
                    value_data["duration"] = video_info.get("duration", 0)
                    value_data["framesCount"] = video_info.get("total_frames", 0)
                
                result_item = {
                    "from_name": "box",
                    "to_name": "video",
                    "type": "videorectangle",
                    "value": value_data,
                    "score": 0.85 - (label_idx * 0.1)  # é€’å‡çš„ç½®ä¿¡åº¦
                }
                
                prediction["result"].append(result_item)
                print(f"ğŸ¯ ç”Ÿæˆæ¨¡æ‹Ÿæ ‡ç­¾: {label} (åŒ…å«{len(sequence_data)}ä¸ªè·Ÿè¸ªç‚¹)")
            
            print(f"âœ… æ¨¡æ‹Ÿé¢„æµ‹å®Œæˆï¼Œç”Ÿæˆäº†{len(prediction['result'])}ä¸ªæ ‡æ³¨å¯¹è±¡")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹å¤±è´¥: {e}")
        
        return prediction
    
    def predict(self, tasks: List[Dict], context: Optional[Dict] = None, **kwargs) -> ModelResponse:
        """ä¸»é¢„æµ‹æ–¹æ³•"""
        print(f"\nğŸš€ æ”¶åˆ°é¢„æµ‹è¯·æ±‚ï¼Œä»»åŠ¡æ•°é‡: {len(tasks)}")
        predictions = []
        
        for i, task in enumerate(tasks):
            print(f"\nğŸ“ å¤„ç†ä»»åŠ¡ {i+1}/{len(tasks)}")
            try:
                prediction = self._process_task(task)
                if prediction:
                    predictions.append(prediction)
                    print(f"âœ… ä»»åŠ¡{i+1}å¤„ç†æˆåŠŸï¼Œç”Ÿæˆäº†{len(prediction.get('result', []))}ä¸ªæ ‡æ³¨")
                else:
                    print(f"âš ï¸ ä»»åŠ¡{i+1}è¿”å›ç©ºç»“æœ")
                    predictions.append({"model_version": self.get("model_version"), "score": 0.0, "result": []})
            except Exception as e:
                print(f"âŒ ä»»åŠ¡{i+1}å¤„ç†å¤±è´¥: {e}")
                import traceback
                print(f"ğŸ” é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                predictions.append({"model_version": self.get("model_version"), "score": 0.0, "result": []})
        
        print(f"\nğŸ¯ é¢„æµ‹å®Œæˆï¼Œæ€»å…±è¿”å›{len(predictions)}ä¸ªé¢„æµ‹ç»“æœ")
        for i, pred in enumerate(predictions):
            result_count = len(pred.get('result', []))
            print(f"  ä»»åŠ¡{i+1}: {result_count}ä¸ªæ ‡æ³¨å¯¹è±¡")
        
        return ModelResponse(predictions=predictions)
    
    def _process_task(self, task: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        print(f"\nğŸ¯ å¼€å§‹å¤„ç†ä»»åŠ¡...")
        print(f"ğŸ“‹ ä»»åŠ¡æ•°æ®: {json.dumps(task, ensure_ascii=False, indent=2)}")
        
        # æŸ¥æ‰¾è§†é¢‘URL
        video_url = self._extract_video_url(task.get('data', {}))
        if not video_url:
            print("âŒ æœªæ‰¾åˆ°è§†é¢‘URL")
            return None
        
        print(f"ğŸ¬ æ‰¾åˆ°è§†é¢‘URL: {video_url}")
        
        # æŸ¥æ‰¾å¹¶å¤„ç†è§†é¢‘æ–‡ä»¶
        video_path = self._find_video_file(video_url)
        if not video_path:
            print(f"âŒ æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {video_url}")
            return None
        
        print(f"âœ… è§†é¢‘æ–‡ä»¶è·¯å¾„: {video_path}")
        
        # æŠ½å¸§å¹¶åˆ†æ
        video_frames, video_info = self._extract_video_frames(video_path)
        if not video_frames:
            print("âŒ è§†é¢‘æŠ½å¸§å¤±è´¥")
            return None
        
        print(f"âœ… æˆåŠŸæŠ½å– {len(video_frames)} å¸§")
        
        # æ£€æŸ¥APIå®¢æˆ·ç«¯çŠ¶æ€
        if not self.client:
            print("âš ï¸ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¿”å›æ¨¡æ‹Ÿæ ‡æ³¨ç»“æœ")
            return self._create_mock_prediction(video_frames, task, video_info)
        
        # è°ƒç”¨APIåˆ†æ
        print("ğŸ¤– å¼€å§‹è°ƒç”¨APIåˆ†æ...")
        api_response = self._call_serial_api("", video_frames)
        
        if api_response:
            print("âœ… APIåˆ†æå®Œæˆï¼Œå¼€å§‹æ ¼å¼åŒ–ç»“æœ...")
            print(f"ğŸ“Š APIå“åº”: {api_response[:200]}...")
            result = self._format_video_prediction(api_response, task, video_info)
            print(f"ğŸ¯ æœ€ç»ˆé¢„æµ‹ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
            return result
        else:
            print("âŒ APIåˆ†æå¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
            return self._create_mock_prediction(video_frames, task, video_info)
    
    def _extract_video_url(self, task_data: Dict) -> Optional[str]:
        """ä»ä»»åŠ¡æ•°æ®ä¸­æå–è§†é¢‘URL"""
        for key, value in task_data.items():
            if isinstance(value, str):
                # æ£€æŸ¥å…³é”®å­—æ®µ
                if key.lower() in ['video', 'vid', 'movie', 'media', 'url']:
                    return value
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                if any(value.lower().endswith(f'.{ext}') for ext in SUPPORTED_VIDEO_FORMATS):
                    return value
        return None
    
    def fit(self, event, data, **kwargs):
        """è®­ç»ƒæ–¹æ³•"""
        self.set('annotation_data', 'updated_video_tracking_data')
        self.set('model_version', 'updated_video_version')
        print(f"âœ… æ¨¡å‹å·²æ›´æ–° (äº‹ä»¶: {event})")
